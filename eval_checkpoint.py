#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹çš„checkpointè¯„ä¼°è„šæœ¬
å¯ä»¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹checkpointï¼Œå¤ç”¨ç°æœ‰çš„evaluatorã€beam searchç­‰ç»„ä»¶è¿›è¡Œæ¨ç†

ä½¿ç”¨æ–¹æ³•:
python eval_checkpoint.py \
    --model=DIFF_GRM \
    --dataset=AmazonReviews2014 \
    --category=Toys_and_Games \
    --checkpoint="saved/AmazonReviews2014_Aug-29-2025_02-52-21/pytorch_model.bin" \
    --beam_search_modes='["confidence"]' \
    --vectorized_beam_search='{"test":{"beam_act":256,"beam_max":256}}'
"""

import argparse
import os
import torch
import json
from datetime import datetime
from torch.utils.data import DataLoader
from accelerate import Accelerator
from logging import getLogger

from genrec.utils import (
    get_config, init_seed, init_logger, init_device,
    get_dataset, get_tokenizer, get_model, get_trainer,
    parse_command_line_args, log
)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Eval a saved checkpoint on test split (no training).")
    parser.add_argument('--model', type=str, default='DIFF_GRM', help='Model name')
    parser.add_argument('--dataset', type=str, default='AmazonReviews2014', help='Dataset name')
    parser.add_argument('--checkpoint', type=str, required=True, help='Path to pytorch_model.bin checkpoint')
    parser.add_argument('--output_file', type=str, default=None, help='Output results to JSON file')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')
    
    # è§£æå·²çŸ¥å‚æ•°å’ŒæœªçŸ¥å‚æ•°
    args, unparsed = parser.parse_known_args()
    return args, unparsed


def validate_sid_mapping_consistency(tokenizer, dataset):
    """éªŒè¯SIDæ˜ å°„çš„ä¸€è‡´æ€§"""
    print("ğŸ” Validating SID mapping consistency...")
    
    try:
        # æ£€æŸ¥item2tokensæ˜ å°„æ˜¯å¦å­˜åœ¨
        if not hasattr(tokenizer, 'item2tokens') or not tokenizer.item2tokens:
            print("âš ï¸  Warning: item2tokens mapping not found")
            return
        
        # æ£€æŸ¥tokens2itemæ˜ å°„æ˜¯å¦å­˜åœ¨
        if not hasattr(tokenizer, 'tokens2item') or not tokenizer.tokens2item:
            print("âš ï¸  Warning: tokens2item mapping not found")
            return
        
        # æ£€æŸ¥æ˜ å°„æ•°é‡æ˜¯å¦ä¸€è‡´
        n_items = dataset.n_items
        n_mapped_items = len(tokenizer.item2tokens)
        print(f"ğŸ“Š Items in dataset: {n_items}, Items in mapping: {n_mapped_items}")
        
        if n_mapped_items != n_items - 1:  # å‡å»PAD token
            print(f"âš ï¸  Warning: Mapping count mismatch (expected {n_items-1}, got {n_mapped_items})")
        
        # éšæœºæµ‹è¯•å‡ ä¸ªitemçš„æ˜ å°„ä¸€è‡´æ€§
        import random
        test_items = list(tokenizer.item2tokens.keys())[:min(10, len(tokenizer.item2tokens))]
        
        for item in test_items:
            if item in tokenizer.item2tokens:
                tokens = tokenizer.item2tokens[item]
                # æ£€æŸ¥åå‘æ˜ å°„
                if hasattr(tokenizer, 'codebooks_to_item_id'):
                    item_id = tokenizer.codebooks_to_item_id(list(tokens))
                    if item_id is not None:
                        original_item = dataset.id_mapping['id2item'][item_id]
                        if original_item != item:
                            print(f"âš ï¸  Warning: Mapping inconsistency for item {item}")
                            print(f"    Original: {item}, Mapped back: {original_item}")
                        else:
                            print(f"âœ… Item {item} mapping verified")
                    else:
                        print(f"âš ï¸  Warning: Item {item} tokens are invalid")
        
        print("âœ… SID mapping consistency check completed")
        
    except Exception as e:
        print(f"âš ï¸  Warning: SID mapping consistency check failed: {e}")


def validate_checkpoint_path(checkpoint_path):
    """éªŒè¯checkpointè·¯å¾„æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶
    if not os.path.isfile(checkpoint_path):
        raise ValueError(f"Checkpoint path is not a file: {checkpoint_path}")
    
    print(f"âœ… Found checkpoint: {checkpoint_path}")


def setup_environment(args, cli_config):
    """è®¾ç½®ç¯å¢ƒå’Œé…ç½®"""
    # å¼ºåˆ¶è®¾ç½®å…³é”®é…ç½®ï¼Œç¡®ä¿SIDæ˜ å°„ä¸€è‡´æ€§
    cli_config.setdefault('force_regenerate_opq', False)  # å…³é”®ï¼šä¸è¦é‡æ–°ç”Ÿæˆé‡åŒ–ç»“æœ
    
    # è·å–é…ç½®
    config = get_config(args.model, args.dataset, config_file=None, config_dict=cli_config)
    
    # è®¾ç½®è®¾å¤‡å’ŒåŠ é€Ÿå™¨
    config['device'], config['use_ddp'] = init_device()
    project_dir = os.path.join(
        config['tensorboard_log_dir'], 
        config["dataset"], 
        f"{config['model']}_EVAL"
    )
    accelerator = Accelerator(log_with='tensorboard', project_dir=project_dir)
    config['accelerator'] = accelerator
    
    # åˆå§‹åŒ–éšæœºç§å­å’Œæ—¥å¿—
    init_seed(config['rand_seed'], config['reproducibility'])
    init_logger(config)
    logger = getLogger()
    
    return config, accelerator, logger


def load_dataset_and_tokenizer(config, args):
    """åŠ è½½æ•°æ®é›†å’Œtokenizer"""
    print(f"ğŸ“Š Loading dataset: {args.dataset}")
    
    # åŠ è½½æ•°æ®é›†
    dataset_class = get_dataset(args.dataset)
    dataset = dataset_class(config)
    split_data = dataset.split()
    
    print(f"ğŸ“Š Dataset loaded: {dataset}")
    print(f"ğŸ“Š Split sizes: train={len(split_data['train'])}, val={len(split_data['val'])}, test={len(split_data['test'])}")
    
    # åŠ è½½tokenizerï¼ˆå…³é”®ï¼šä¼šå¤ç”¨è®­ç»ƒæ—¶çš„SIDæ˜ å°„ï¼‰
    print(f"ğŸ”§ Loading tokenizer: {args.model}")
    tokenizer_class = get_tokenizer(args.model)
    tokenizer = tokenizer_class(config, dataset)
    
    vocab_size = getattr(tokenizer, "vocab_size", None)
    max_len = getattr(tokenizer, "max_token_seq_len", None)
    print(f"ğŸ”§ Tokenizer loaded: vocab_size={vocab_size}, max_seq_len={max_len}")
    
    # éªŒè¯SIDæ˜ å°„ä¸€è‡´æ€§
    validate_sid_mapping_consistency(tokenizer, dataset)
    
    return dataset, split_data, tokenizer


def create_dataloaders(split_data, tokenizer, config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ Creating dataloaders...")
    
    # åªtokenizeæµ‹è¯•é›†ï¼ŒèŠ‚çœæ—¶é—´
    test_split = {'test': split_data['test']}
    test_dataset = tokenizer.tokenize(test_split)['test']
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['eval_batch_size'],
        shuffle=False,
        collate_fn=tokenizer.collate_fn['test']
    )
    
    print(f"ğŸ”„ Test dataloader created: {len(test_loader)} batches, batch_size={config['eval_batch_size']}")
    
    return test_loader


def load_model_and_checkpoint(config, dataset, tokenizer, args):
    """åŠ è½½æ¨¡å‹å’Œcheckpoint"""
    print(f"ğŸ¤– Loading model: {args.model}")
    
    # åˆ›å»ºæ¨¡å‹
    model_class = get_model(args.model)
    model = model_class(config, dataset, tokenizer)
    
    try:
        n_params = getattr(model, "n_parameters", None)
        if n_params is None:
            n_params = sum(p.numel() for p in model.parameters())
    except Exception:
        n_params = "unknown"
    print(f"ğŸ¤– Model created: {n_params} parameters")
    
    # åŠ è½½checkpoint
    print(f"ğŸ’¾ Loading checkpoint from: {args.checkpoint}")
    ckpt = torch.load(args.checkpoint, map_location='cpu')
    
    # å…¼å®¹å‡ ç§ä¿å­˜æ–¹å¼
    if isinstance(ckpt, dict) and "state_dict" in ckpt:
        state_dict = ckpt["state_dict"]
    elif isinstance(ckpt, dict) and "model" in ckpt:
        state_dict = ckpt["model"]
    else:
        state_dict = ckpt
    
    # å…¼å®¹ DDP å‰ç¼€
    if not any(k.startswith("module.") for k in model.state_dict().keys()) and \
       any(k.startswith("module.") for k in state_dict.keys()):
        state_dict = {k.replace("module.", "", 1): v for k, v in state_dict.items()}
    
    model.load_state_dict(state_dict, strict=True)
    print("âœ… Checkpoint loaded successfully")
    
    return model


def run_evaluation(model, test_loader, config, accelerator, logger, args, tokenizer):
    """è¿è¡Œè¯„ä¼°"""
    print("ğŸš€ Starting evaluation...")
    
    # å‡†å¤‡æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
    model, test_loader = accelerator.prepare(model, test_loader)
    
    # åˆ›å»ºtrainerï¼ˆåªç”¨äºevaluateæ–¹æ³•ï¼‰
    trainer_class = get_trainer(args.model)
    trainer = trainer_class(config, model, tokenizer)
    
    # è·å–beam searchæ¨¡å¼
    modes = config.get("beam_search_modes", ["confidence"])
    print(f"ğŸ¯ Beam search modes: {modes}")
    
    # è¿è¡Œè¯„ä¼°
    results = trainer.evaluate(test_loader, split='test')
    
    # æ‰“å°ç»“æœ
    if accelerator.is_main_process:
        print("\n" + "="*60)
        print("ğŸ“ˆ EVALUATION RESULTS")
        print("="*60)
        
        # æŒ‰æ¨¡å¼åˆ†ç»„æ˜¾ç¤ºç»“æœ
        for mode in modes:
            mode_suffix = "" if mode == "confidence" else f"_{mode}"
            print(f"\nğŸ¯ Mode: {mode}")
            print("-" * 40)
            
            # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
            for metric in config.get('metrics', ['ndcg', 'recall']):
                for k in config.get('topk', [5, 10]):
                    key = f"{metric}@{k}{mode_suffix}"
                    if key in results:
                        print(f"  {key}: {results[key]:.4f}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            for stat_key in ['legal_ratio', 'duplicate_ratio', 'dup@10']:
                key = f"{stat_key}{mode_suffix}"
                if key in results:
                    print(f"  {key}: {results[key]:.4f}")
            
            # æ˜¾ç¤ºåŠ æƒåˆ†æ•°ï¼ˆä»…confidenceæ¨¡å¼ï¼‰
            if mode == "confidence" and "weighted_score" in results:
                print(f"  weighted_score: {results['weighted_score']:.4f}")
        
        print("\n" + "="*60)
    
    return results


def save_results(results, args, config):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    if args.output_file is None:
        return
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    output_data = {
        'checkpoint': args.checkpoint,
        'model': args.model,
        'dataset': args.dataset,
        'config': {
            'eval_batch_size': config['eval_batch_size'],
            'beam_search_modes': config.get('beam_search_modes', ["confidence"]),
            'vectorized_beam_search': config.get('vectorized_beam_search', {}),
            'topk': config.get('topk', [5, 10]),
            'metrics': config.get('metrics', ['ndcg', 'recall'])
        },
        'results': results,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    out_dir = os.path.dirname(args.output_file)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ Results saved to: {args.output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting checkpoint evaluation...")
    
    # è§£æå‚æ•°
    args, unparsed = parse_args()
    cli_config = parse_command_line_args(unparsed)
    
    # éªŒè¯checkpointè·¯å¾„
    validate_checkpoint_path(args.checkpoint)
    
    # è®¾ç½®ç¯å¢ƒ
    config, accelerator, logger = setup_environment(args, cli_config)
    
    # è®°å½•å…³é”®é…ç½®
    log(f'[EVAL] Device: {config["device"]}', accelerator, logger)
    log(f'[EVAL] Model: {args.model}, Dataset: {args.dataset}', accelerator, logger)
    log(f'[EVAL] Checkpoint: {args.checkpoint}', accelerator, logger)
    
    # åŠ è½½æ•°æ®é›†å’Œtokenizer
    dataset, split_data, tokenizer = load_dataset_and_tokenizer(config, args)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = create_dataloaders(split_data, tokenizer, config)
    
    # åŠ è½½æ¨¡å‹å’Œcheckpoint
    model = load_model_and_checkpoint(config, dataset, tokenizer, args)
    
    # è¿è¡Œè¯„ä¼°
    results = run_evaluation(model, test_loader, config, accelerator, logger, args, tokenizer)
    
    # æ¸…ç†èµ„æº
    try:
        trainer = get_trainer(args.model)(config, model, tokenizer)
        if hasattr(trainer, 'end'):
            trainer.end()
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to cleanup trainer: {e}")
    
    # ä¿å­˜ç»“æœ
    save_results(results, args, config)
    
    print("âœ… Evaluation completed successfully!")


if __name__ == "__main__":
    main() 