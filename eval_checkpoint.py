#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹çš„checkpointè¯„ä¼°è„šæœ¬
å¯ä»¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹checkpointï¼Œå¤ç”¨ç°æœ‰çš„evaluatorã€beam searchç­‰ç»„ä»¶è¿›è¡Œæ¨ç†

ä½¿ç”¨æ–¹æ³•:
python eval_checkpoint.py \
    --model=DIFF_GRM \
    --dataset=AmazonReviews2014 \
    --category=Toys_and_Games \
    --checkpoint="saved/AmazonReviews2014_Aug-29-2025_02-52-21/pytorch_model.bin" \
    --beam_search_modes='["confidence"]' \
    --vectorized_beam_search='{"test":{"beam_act":256,"beam_max":256}}'
"""

import argparse
import os
import torch
import json
from datetime import datetime
from torch.utils.data import DataLoader
from accelerate import Accelerator
from logging import getLogger

from genrec.utils import (
    get_config, init_seed, init_logger, init_device,
    get_dataset, get_tokenizer, get_model, get_trainer,
    parse_command_line_args, log
)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Eval a saved checkpoint on test split (no training).")
    parser.add_argument('--model', type=str, default='DIFF_GRM', help='Model name')
    parser.add_argument('--dataset', type=str, default='AmazonReviews2014', help='Dataset name')
    parser.add_argument('--checkpoint', type=str, required=True, help='Path to pytorch_model.bin checkpoint')
    parser.add_argument('--output_file', type=str, default=None, help='Output results to JSON file')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')
    parser.add_argument('--prefer_ckpt_arch', action='store_true',
                        help='Try to construct model architecture from checkpoint/config next to it.')
    parser.add_argument('--config_file', type=str, default=None,
                        help='Optional path to the training config (json/yaml) if not embedded in ckpt.')
    
    # è§£æå·²çŸ¥å‚æ•°å’ŒæœªçŸ¥å‚æ•°
    args, unparsed = parser.parse_known_args()
    return args, unparsed


def validate_sid_mapping_consistency(tokenizer, dataset):
    """éªŒè¯SIDæ˜ å°„çš„ä¸€è‡´æ€§"""
    print("ğŸ” Validating SID mapping consistency...")
    
    try:
        # æ£€æŸ¥item2tokensæ˜ å°„æ˜¯å¦å­˜åœ¨
        if not hasattr(tokenizer, 'item2tokens') or not tokenizer.item2tokens:
            print("âš ï¸  Warning: item2tokens mapping not found")
            return
        
        # æ£€æŸ¥tokens2itemæ˜ å°„æ˜¯å¦å­˜åœ¨
        if not hasattr(tokenizer, 'tokens2item') or not tokenizer.tokens2item:
            print("âš ï¸  Warning: tokens2item mapping not found")
            return
        
        # æ£€æŸ¥æ˜ å°„æ•°é‡æ˜¯å¦ä¸€è‡´
        n_items = dataset.n_items
        n_mapped_items = len(tokenizer.item2tokens)
        print(f"ğŸ“Š Items in dataset: {n_items}, Items in mapping: {n_mapped_items}")
        
        if n_mapped_items != n_items - 1:  # å‡å»PAD token
            print(f"âš ï¸  Warning: Mapping count mismatch (expected {n_items-1}, got {n_mapped_items})")
        
        # éšæœºæµ‹è¯•å‡ ä¸ªitemçš„æ˜ å°„ä¸€è‡´æ€§
        import random
        test_items = list(tokenizer.item2tokens.keys())[:min(10, len(tokenizer.item2tokens))]
        
        for item in test_items:
            if item in tokenizer.item2tokens:
                tokens = tokenizer.item2tokens[item]
                # æ£€æŸ¥åå‘æ˜ å°„
                if hasattr(tokenizer, 'codebooks_to_item_id'):
                    item_id = tokenizer.codebooks_to_item_id(list(tokens))
                    if item_id is not None:
                        original_item = dataset.id_mapping['id2item'][item_id]
                        if original_item != item:
                            print(f"âš ï¸  Warning: Mapping inconsistency for item {item}")
                            print(f"    Original: {item}, Mapped back: {original_item}")
                        else:
                            print(f"âœ… Item {item} mapping verified")
                    else:
                        print(f"âš ï¸  Warning: Item {item} tokens are invalid")
        
        print("âœ… SID mapping consistency check completed")
        
    except Exception as e:
        print(f"âš ï¸  Warning: SID mapping consistency check failed: {e}")


def _maybe_fill_arch_from_state_dict(config, sd):
    """
    å°½å¯èƒ½ä» state_dict çš„å½¢çŠ¶æ¨æ–­å…³é”®æ¶æ„å‚æ•°ï¼š
      - hidden_size / d_model / n_embd
      - n_inner / ffn_hidden
      - encoder_n_layer / decoder_n_layer
      - use_cross_attn
      - n_headï¼ˆå°½åŠ›æ¨ï¼Œéå”¯ä¸€ï¼‰
    ä»…åœ¨ config æœªè®¾ç½®ç›¸åº”å­—æ®µæ—¶å¡«å……ã€‚
    """
    keys = list(sd.keys())
    if not keys:
        return

    # hidden_size: é€šå¸¸å¯ä»¥ä» self_attn.qkv.weight å½¢çŠ¶å¾—åˆ° [3*H, H] æˆ–è€… from ln/emb
    def _first_shape_of(prefixes):
        for k in keys:
            for p in prefixes:
                if k.endswith(p) or p in k:
                    t = sd[k]
                    return tuple(t.shape)
        return None

    # 1) hidden_size / d_model / n_embd
    shp = _first_shape_of(["self_attn.qkv.weight", "attn.qkv.weight", "encoder_blocks.0.self_attn.qkv.weight",
                           "decoder_blocks.0.self_attn.qkv.weight", "embedding.weight"])
    if shp is not None and len(shp) == 2:
        # [3*H, H] æˆ– [vocab_size, H]
        H = shp[1]
        old_val = config.get('n_embd', 'not set')
        config['n_embd'] = H
        config.setdefault('hidden_size', H)
        config.setdefault('d_model', H)
        print(f"  ğŸ§© Inferred n_embd: {old_val} -> {H}")

    # 2) n_inner / ffn_hiddenï¼ˆmlp çš„ä¸­é—´ç»´åº¦ï¼‰ï¼Œå…¸å‹ [ffn, hidden]
    shp = _first_shape_of(["mlp.c_fc.weight", "feed_forward.c_fc.weight"])
    if shp is not None and len(shp) == 2:
        ffn, hid = shp
        # å½“prefer_ckpt_archæ—¶ï¼Œå¼ºåˆ¶è¦†ç›–n_innerï¼Œé¿å…è¢«é»˜è®¤å€¼å¡ä½
        if 'n_inner' not in config or config.get('n_inner') != ffn:
            old_val = config.get('n_inner', 'not set')
            config['n_inner'] = ffn
            print(f"  ğŸ§© Inferred n_inner: {old_val} -> {ffn}")
        
        # åŒæ—¶è®¾ç½®mlp_ratioï¼ˆå¦‚æœd_modelå·²çŸ¥ï¼‰
        if 'n_embd' in config and config['n_embd'] > 0:
            new_ratio = ffn // config['n_embd']
            if new_ratio * config['n_embd'] == ffn:
                old_ratio = config.get('mlp_ratio', 'not set')
                config['mlp_ratio'] = new_ratio
                print(f"  ğŸ§© Inferred mlp_ratio: {old_ratio} -> {new_ratio}")

    # 3) å±‚æ•°
    def _max_block_idx(prefix):
        max_idx = -1
        for k in keys:
            if k.startswith(prefix):
                # e.g. encoder_blocks.3.ln_2.weight
                parts = k.split('.')
                if len(parts) > 2 and parts[1].isdigit():
                    max_idx = max(max_idx, int(parts[1]))
        return max_idx + 1 if max_idx >= 0 else None

    enc_layers = _max_block_idx("encoder_blocks.")
    dec_layers = _max_block_idx("decoder_blocks.")
    if enc_layers:
        old_val = config.get('encoder_n_layer', 'not set')
        config['encoder_n_layer'] = enc_layers
        config.setdefault('n_layer_encoder', enc_layers)  # åŒå†™å…¼å®¹
        print(f"  ğŸ§© Inferred encoder_n_layer: {old_val} -> {enc_layers}")
    if dec_layers:
        old_val = config.get('decoder_n_layer', 'not set')
        config['decoder_n_layer'] = dec_layers
        config.setdefault('n_layer_decoder', dec_layers)  # åŒå†™å…¼å®¹
        print(f"  ğŸ§© Inferred decoder_n_layer: {old_val} -> {dec_layers}")

    # 4) æ˜¯å¦å­˜åœ¨ cross-attn
    use_xattn = any("cross_attn.qkv.weight" in k for k in keys)
    old_val = config.get('use_cross_attn', 'not set')
    config['use_cross_attn'] = bool(use_xattn)
    print(f"  ğŸ§© Inferred use_cross_attn: {old_val} -> {use_xattn}")

    # 5) ä¼°è®¡ n_headï¼ˆéå”¯ä¸€ï¼‰ï¼šå°è¯• gcd æ‹†åˆ†
    # qkv.weight: [3*H, H] -> å…ˆæ‹¿ Hï¼Œå°è¯•æŠŠ H æ‹†æˆ n_head * head_dim
    if 'n_embd' in config:
        H = config['n_embd']
        # å¸¸è§ head_dim
        for hd in (128, 96, 64, 48, 32, 16):
            if H % hd == 0:
                old_val = config.get('n_head', 'not set')
                config['n_head'] = H // hd
                print(f"  ğŸ§© Inferred n_head: {old_val} -> {H // hd} (head_dim={hd})")
                break


def validate_checkpoint_path(checkpoint_path):
    """éªŒè¯checkpointè·¯å¾„æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶
    if not os.path.isfile(checkpoint_path):
        raise ValueError(f"Checkpoint path is not a file: {checkpoint_path}")
    
    print(f"âœ… Found checkpoint: {checkpoint_path}")


def setup_environment(args, cli_config):
    """è®¾ç½®ç¯å¢ƒå’Œé…ç½®"""
    # å¼ºåˆ¶è®¾ç½®å…³é”®é…ç½®ï¼Œç¡®ä¿SIDæ˜ å°„ä¸€è‡´æ€§
    cli_config.setdefault('force_regenerate_opq', False)  # å…³é”®ï¼šä¸è¦é‡æ–°ç”Ÿæˆé‡åŒ–ç»“æœ
    
    # ä¼˜å…ˆä» ckpt/æ˜¾å¼ä¼ å…¥çš„ config æ¢å¤è®­ç»ƒæœŸæ¶æ„
    found_cfg = args.config_file
    run_dir = os.path.dirname(args.checkpoint)
    if args.prefer_ckpt_arch and not found_cfg:
        for name in ("config.json", "args.json", "hparams.yaml"):
            p = os.path.join(run_dir, name)
            if os.path.exists(p):
                found_cfg = p
                print(f"ğŸ§© Found run config next to ckpt: {p}")
                break
    
    # è·å–é…ç½®
    config = get_config(args.model, args.dataset, config_file=found_cfg, config_dict=cli_config)
    
    # è®¾ç½®è®¾å¤‡å’ŒåŠ é€Ÿå™¨
    config['device'], config['use_ddp'] = init_device()
    project_dir = os.path.join(
        config['tensorboard_log_dir'], 
        config["dataset"], 
        f"{config['model']}_EVAL"
    )
    accelerator = Accelerator(log_with='tensorboard', project_dir=project_dir)
    config['accelerator'] = accelerator
    
    # åˆå§‹åŒ–éšæœºç§å­å’Œæ—¥å¿—
    init_seed(config['rand_seed'], config['reproducibility'])
    init_logger(config)
    logger = getLogger()
    
    return config, accelerator, logger


def load_dataset_and_tokenizer(config, args):
    """åŠ è½½æ•°æ®é›†å’Œtokenizer"""
    print(f"ğŸ“Š Loading dataset: {args.dataset}")
    
    # åŠ è½½æ•°æ®é›†
    dataset_class = get_dataset(args.dataset)
    dataset = dataset_class(config)
    split_data = dataset.split()
    
    print(f"ğŸ“Š Dataset loaded: {dataset}")
    print(f"ğŸ“Š Split sizes: train={len(split_data['train'])}, val={len(split_data['val'])}, test={len(split_data['test'])}")
    
    # åŠ è½½tokenizerï¼ˆå…³é”®ï¼šä¼šå¤ç”¨è®­ç»ƒæ—¶çš„SIDæ˜ å°„ï¼‰
    print(f"ğŸ”§ Loading tokenizer: {args.model}")
    tokenizer_class = get_tokenizer(args.model)
    tokenizer = tokenizer_class(config, dataset)
    
    vocab_size = getattr(tokenizer, "vocab_size", None)
    max_len = getattr(tokenizer, "max_token_seq_len", None)
    print(f"ğŸ”§ Tokenizer loaded: vocab_size={vocab_size}, max_seq_len={max_len}")
    
    # éªŒè¯SIDæ˜ å°„ä¸€è‡´æ€§
    validate_sid_mapping_consistency(tokenizer, dataset)
    
    return dataset, split_data, tokenizer


def create_dataloaders(split_data, tokenizer, config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ”„ Creating dataloaders...")
    
    # åªtokenizeæµ‹è¯•é›†ï¼ŒèŠ‚çœæ—¶é—´
    test_split = {'test': split_data['test']}
    test_dataset = tokenizer.tokenize(test_split)['test']
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['eval_batch_size'],
        shuffle=False,
        collate_fn=tokenizer.collate_fn['test']
    )
    
    print(f"ğŸ”„ Test dataloader created: {len(test_loader)} batches, batch_size={config['eval_batch_size']}")
    
    return test_loader


def load_model_and_checkpoint(config, dataset, tokenizer, args):
    """åŠ è½½æ¨¡å‹å’Œcheckpoint"""
    print(f"ğŸ¤– Loading model: {args.model}")
    
    # å…ˆæŠŠ ckpt è¯»å‡ºæ¥ï¼Œçœ‹æ˜¯å¦å¸¦æœ‰è®­ç»ƒæœŸ config/hparamsï¼ˆä¸€äº›è®­ç»ƒè„šæœ¬ä¼šè¿™ä¹ˆå­˜ï¼‰
    print(f"ğŸ’¾ Loading checkpoint from: {args.checkpoint}")
    ckpt = torch.load(args.checkpoint, map_location='cpu')
    state_dict = None
    
    # å¯èƒ½çš„å®¹å™¨å½¢å¼ï¼š{'state_dict':..., 'model':..., 'config':...}
    if isinstance(ckpt, dict):
        if 'config' in ckpt and args.prefer_ckpt_arch:
            print("ğŸ§© Found training config inside checkpoint; merging arch keys.")
            train_cfg = ckpt['config']
            # åªåˆå¹¶"æ¶æ„ç›¸å…³"çš„å…³é”®å­—ï¼Œé¿å…è¦†ç›–æ¨ç†å‚æ•°
            ARCH_KEYS = {
                'n_embd', 'hidden_size', 'd_model', 'n_inner', 'ffn_hidden', 'mlp_ratio',
                'n_head', 'encoder_n_layer', 'decoder_n_layer', 'n_layer',
                'dropout', 'attn_pdrop', 'resid_pdrop', 'embd_pdrop',
                'norm_type', 'norm_eps', 'act', 'use_cross_attn', 'tie_embeddings'
            }
            for k in ARCH_KEYS:
                if k in train_cfg:
                    config[k] = train_cfg[k]
                    print(f"  ğŸ§© Merged arch key: {k} = {train_cfg[k]}")
        
        if 'state_dict' in ckpt:
            state_dict = ckpt['state_dict']
        elif 'model' in ckpt:   # æœ‰äº›äººæŠŠæƒé‡æ”¾åœ¨ 'model'
            state_dict = ckpt['model']
    
    if state_dict is None:
        state_dict = ckpt  # çº¯ state_dict
    
    # å¦‚æœä»ç„¶æ²¡æœ‰æ¶æ„æ–‡ä»¶ï¼Œåˆå¼€å¯ prefer_ckpt_archï¼Œåˆ™å°è¯•ä»æƒé‡å½¢çŠ¶åšæœ€å°æ¨æ–­ï¼ˆå…œåº•ï¼‰
    if args.prefer_ckpt_arch:
        try:
            _maybe_fill_arch_from_state_dict(config, state_dict)
        except Exception as e:
            print(f"âš ï¸  Fallback arch inference failed (safe to ignore if you have a config): {e}")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆæ­¤æ—¶ config å·²å°½åŠ›å¯¹é½è®­ç»ƒæœŸæ¶æ„ï¼‰
    if args.prefer_ckpt_arch:
        print("ğŸ§¾ Final architecture config:")
        print(f"  d_model/n_embd: {config.get('d_model') or config.get('n_embd')}")
        print(f"  n_inner: {config.get('n_inner')}")
        print(f"  mlp_ratio: {config.get('mlp_ratio')}")
        print(f"  n_head: {config.get('n_head')}")
        print(f"  encoder_layers: {config.get('encoder_n_layer') or config.get('n_layer_encoder')}")
        print(f"  decoder_layers: {config.get('decoder_n_layer') or config.get('n_layer_decoder') or config.get('n_layer')}")
    
    model_class = get_model(args.model)
    model = model_class(config, dataset, tokenizer)
    
    try:
        n_params = getattr(model, "n_parameters", None)
        if n_params is None:
            n_params = sum(p.numel() for p in model.parameters())
    except Exception:
        n_params = "unknown"
    print(f"ğŸ¤– Model created: {n_params} parameters")
    
    # å¯¹ DDP å‰ç¼€åšå…¼å®¹
    if not any(k.startswith("module.") for k in model.state_dict().keys()) and \
       any(k.startswith("module.") for k in state_dict.keys()):
        state_dict = {k.replace("module.", "", 1): v for k, v in state_dict.items()}
    
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    if missing or unexpected:
        print(f"âš ï¸  load_state_dict not strict. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
        for k in (missing[:10] if len(missing) > 10 else missing):
            print(f"  MISSING: {k}")
        for k in (unexpected[:10] if len(unexpected) > 10 else unexpected):
            print(f"  UNEXPECTED: {k}")
        # å¦‚æœä½ å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼Œæ”¹å› strict=Trueï¼›è¿™é‡Œåªæ˜¯ä¸ºäº†åœ¨æ¨æ–­æ¶æ„æ—¶å°½é‡è·‘èµ·æ¥
    else:
        print("âœ… Checkpoint loaded successfully")
    
    return model


def run_evaluation(model, test_loader, config, accelerator, logger, args, tokenizer):
    """è¿è¡Œè¯„ä¼°"""
    print("ğŸš€ Starting evaluation...")
    
    # å‡†å¤‡æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
    model, test_loader = accelerator.prepare(model, test_loader)
    
    # åˆ›å»ºtrainerï¼ˆåªç”¨äºevaluateæ–¹æ³•ï¼‰
    trainer_class = get_trainer(args.model)
    trainer = trainer_class(config, model, tokenizer)
    
    # è·å–beam searchæ¨¡å¼
    modes = config.get("beam_search_modes", ["confidence"])
    print(f"ğŸ¯ Beam search modes: {modes}")
    
    # è¿è¡Œè¯„ä¼°
    results = trainer.evaluate(test_loader, split='test')
    
    # æ‰“å°ç»“æœ
    if accelerator.is_main_process:
        print("\n" + "="*60)
        print("ğŸ“ˆ EVALUATION RESULTS")
        print("="*60)
        
        # æŒ‰æ¨¡å¼åˆ†ç»„æ˜¾ç¤ºç»“æœ
        for mode in modes:
            mode_suffix = "" if mode == "confidence" else f"_{mode}"
            print(f"\nğŸ¯ Mode: {mode}")
            print("-" * 40)
            
            # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
            for metric in config.get('metrics', ['ndcg', 'recall']):
                for k in config.get('topk', [5, 10]):
                    key = f"{metric}@{k}{mode_suffix}"
                    if key in results:
                        print(f"  {key}: {results[key]:.4f}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            for stat_key in ['legal_ratio', 'duplicate_ratio', 'dup@10']:
                key = f"{stat_key}{mode_suffix}"
                if key in results:
                    print(f"  {key}: {results[key]:.4f}")
            
            # æ˜¾ç¤ºåŠ æƒåˆ†æ•°ï¼ˆä»…confidenceæ¨¡å¼ï¼‰
            if mode == "confidence" and "weighted_score" in results:
                print(f"  weighted_score: {results['weighted_score']:.4f}")
        
        print("\n" + "="*60)
    
    return results


def save_results(results, args, config):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    if args.output_file is None:
        return
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    output_data = {
        'checkpoint': args.checkpoint,
        'model': args.model,
        'dataset': args.dataset,
        'config': {
            'eval_batch_size': config['eval_batch_size'],
            'beam_search_modes': config.get('beam_search_modes', ["confidence"]),
            'vectorized_beam_search': config.get('vectorized_beam_search', {}),
            'topk': config.get('topk', [5, 10]),
            'metrics': config.get('metrics', ['ndcg', 'recall'])
        },
        'results': results,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    out_dir = os.path.dirname(args.output_file)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ Results saved to: {args.output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting checkpoint evaluation...")
    
    # è§£æå‚æ•°
    args, unparsed = parse_args()
    cli_config = parse_command_line_args(unparsed)
    
    # éªŒè¯checkpointè·¯å¾„
    validate_checkpoint_path(args.checkpoint)
    
    # è®¾ç½®ç¯å¢ƒ
    config, accelerator, logger = setup_environment(args, cli_config)
    
    # è®°å½•å…³é”®é…ç½®
    log(f'[EVAL] Device: {config["device"]}', accelerator, logger)
    log(f'[EVAL] Model: {args.model}, Dataset: {args.dataset}', accelerator, logger)
    log(f'[EVAL] Checkpoint: {args.checkpoint}', accelerator, logger)
    
    # åŠ è½½æ•°æ®é›†å’Œtokenizer
    dataset, split_data, tokenizer = load_dataset_and_tokenizer(config, args)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = create_dataloaders(split_data, tokenizer, config)
    
    # åŠ è½½æ¨¡å‹å’Œcheckpoint
    model = load_model_and_checkpoint(config, dataset, tokenizer, args)
    
    # è¿è¡Œè¯„ä¼°
    results = run_evaluation(model, test_loader, config, accelerator, logger, args, tokenizer)
    
    # æ¸…ç†èµ„æº
    try:
        trainer = get_trainer(args.model)(config, model, tokenizer)
        if hasattr(trainer, 'end'):
            trainer.end()
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to cleanup trainer: {e}")
    
    # ä¿å­˜ç»“æœ
    save_results(results, args, config)
    
    print("âœ… Evaluation completed successfully!")


if __name__ == "__main__":
    main() 