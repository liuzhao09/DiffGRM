/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/accelerate/accelerator.py:399: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:root:Device: cuda
INFO:root:[DATASET] Amazon Reviews 2014 for category: CDs_and_Vinyl
INFO:root:[DATASET] Reviews have been processed...
INFO:root:[DATASET] Metadata has been processed...
INFO:root:[Dataset] AmazonReviews2014
	Number of users: 75259
	Number of items: 64444
	Number of interactions: 1097592
	Average item sequence length: 14.584195910123706
INFO:root:[TOKENIZER] Index factory: OPQ4,IVF1,PQ4x8
INFO:root:[TOKENIZER] Encoding sentence embeddings...
INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: Alibaba-NLP/gte-large-en-v1.5
Batches:   0%|          | 0/252 [00:00<?, ?it/s]Batches:   0%|          | 0/252 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/share/liuzhao09/diffgm/main.py", line 25, in <module>
    pipeline = Pipeline(
  File "/share/liuzhao09/diffgm/genrec/pipeline.py", line 67, in __init__
    self.tokenizer = get_tokenizer(model_name)(self.config, self.raw_dataset)
  File "/share/liuzhao09/diffgm/genrec/models/DIFF_GRM/tokenizer.py", line 57, in __init__
    self.item2tokens = self._init_tokenizer(dataset)
  File "/share/liuzhao09/diffgm/genrec/models/DIFF_GRM/tokenizer.py", line 283, in _init_tokenizer
    raw_embs = self._encode_sent_emb(dataset, raw_path)
  File "/share/liuzhao09/diffgm/genrec/models/DIFF_GRM/tokenizer.py", line 106, in _encode_sent_emb
    sent_embs = sent_emb_model.encode(
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py", line 623, in encode
    out_features = self.forward(features, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py", line 690, in forward
    input = module(input, **module_kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py", line 393, in forward
    output_states = self.auto_model(**trans_features, **kwargs, return_dict=False)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/Alibaba-NLP/new-impl/40ced75c3017eb27626c9d4ea981bde21a2662f4/modeling.py", line 901, in forward
    (embedding_output, attention_mask, rope_embeds, length) = self.embeddings(
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/Alibaba-NLP/new-impl/40ced75c3017eb27626c9d4ea981bde21a2662f4/modeling.py", line 415, in forward
    embeddings = self.LayerNorm(embeddings)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
  File "/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.15 GiB of which 4.24 GiB is free. Process 65531 has 47.74 GiB memory in use. Process 77019 has 27.15 GiB memory in use. Of the allocated memory 26.67 GiB is allocated by PyTorch, and 1.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
