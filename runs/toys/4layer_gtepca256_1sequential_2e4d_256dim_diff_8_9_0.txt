/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/accelerate/accelerator.py:399: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:root:Device: cuda
INFO:root:[DATASET] Amazon Reviews 2014 for category: Toys_and_Games
INFO:root:[DATASET] Reviews have been processed...
INFO:root:[DATASET] Metadata has been processed...
INFO:root:[Dataset] AmazonReviews2014
	Number of users: 19413
	Number of items: 11925
	Number of interactions: 167597
	Average item sequence length: 8.633235460773708
INFO:root:[TOKENIZER] Index factory: OPQ4,IVF1,PQ4x8
INFO:root:[TOKENIZER] Encoding sentence embeddings...
INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: Alibaba-NLP/gte-large-en-v1.5
Batches:   0%|          | 0/47 [00:00<?, ?it/s]Batches:   2%|▏         | 1/47 [00:31<24:12, 31.58s/it]Batches:   4%|▍         | 2/47 [00:38<12:43, 16.96s/it]Batches:   6%|▋         | 3/47 [00:44<08:40, 11.84s/it]Batches:   9%|▊         | 4/47 [00:48<06:31,  9.10s/it]Batches:  11%|█         | 5/47 [00:52<05:01,  7.17s/it]Batches:  13%|█▎        | 6/47 [00:56<04:02,  5.92s/it]Batches:  15%|█▍        | 7/47 [00:59<03:22,  5.06s/it]Batches:  17%|█▋        | 8/47 [01:02<02:53,  4.44s/it]Batches:  19%|█▉        | 9/47 [01:05<02:31,  3.99s/it]Batches:  21%|██▏       | 10/47 [01:08<02:18,  3.75s/it]Batches:  23%|██▎       | 11/47 [01:11<02:07,  3.54s/it]Batches:  26%|██▌       | 12/47 [01:14<01:56,  3.32s/it]Batches:  28%|██▊       | 13/47 [01:17<01:48,  3.19s/it]Batches:  30%|██▉       | 14/47 [01:19<01:37,  2.95s/it]Batches:  32%|███▏      | 15/47 [01:22<01:30,  2.81s/it]Batches:  34%|███▍      | 16/47 [01:24<01:23,  2.71s/it]Batches:  36%|███▌      | 17/47 [01:27<01:19,  2.66s/it]Batches:  38%|███▊      | 18/47 [01:29<01:15,  2.60s/it]Batches:  40%|████      | 19/47 [01:32<01:11,  2.57s/it]Batches:  43%|████▎     | 20/47 [01:34<01:05,  2.44s/it]Batches:  45%|████▍     | 21/47 [01:36<00:59,  2.30s/it]Batches:  47%|████▋     | 22/47 [01:38<00:55,  2.21s/it]Batches:  49%|████▉     | 23/47 [01:40<00:51,  2.13s/it]Batches:  51%|█████     | 24/47 [01:42<00:50,  2.20s/it]Batches:  53%|█████▎    | 25/47 [01:44<00:46,  2.12s/it]Batches:  55%|█████▌    | 26/47 [01:46<00:42,  2.02s/it]Batches:  57%|█████▋    | 27/47 [01:48<00:39,  1.96s/it]Batches:  60%|█████▉    | 28/47 [01:50<00:35,  1.87s/it]Batches:  62%|██████▏   | 29/47 [01:52<00:35,  1.96s/it]Batches:  64%|██████▍   | 30/47 [01:54<00:32,  1.90s/it]Batches:  66%|██████▌   | 31/47 [01:55<00:28,  1.81s/it]Batches:  68%|██████▊   | 32/47 [01:57<00:26,  1.80s/it]Batches:  70%|███████   | 33/47 [01:59<00:25,  1.80s/it]Batches:  72%|███████▏  | 34/47 [02:00<00:22,  1.75s/it]Batches:  74%|███████▍  | 35/47 [02:02<00:20,  1.70s/it]Batches:  77%|███████▋  | 36/47 [02:03<00:17,  1.63s/it]Batches:  79%|███████▊  | 37/47 [02:05<00:15,  1.60s/it]Batches:  81%|████████  | 38/47 [02:06<00:13,  1.54s/it]Batches:  83%|████████▎ | 39/47 [02:08<00:11,  1.44s/it]Batches:  85%|████████▌ | 40/47 [02:09<00:09,  1.36s/it]Batches:  87%|████████▋ | 41/47 [02:10<00:07,  1.29s/it]Batches:  89%|████████▉ | 42/47 [02:11<00:06,  1.20s/it]Batches:  91%|█████████▏| 43/47 [02:12<00:04,  1.12s/it]Batches:  94%|█████████▎| 44/47 [02:13<00:03,  1.03s/it]Batches:  96%|█████████▌| 45/47 [02:13<00:01,  1.07it/s]Batches:  98%|█████████▊| 46/47 [02:14<00:00,  1.17it/s]Batches: 100%|██████████| 47/47 [02:14<00:00,  1.44it/s]Batches: 100%|██████████| 47/47 [02:14<00:00,  2.87s/it]
INFO:root:[TOKENIZER] Applying PCA to sentence embeddings...
INFO:root:[TOKENIZER] Items for training: 11861 of 11924
INFO:root:[TOKENIZER] Training items sample: ['B001JTDL1Q', 'B009QUGMOS', 'B000N8LVJ8', 'B0037UP9MY', 'B000PXVQI2', 'B0007YDBLE', 'B0073RFP6C', 'B000BK7JTO', 'B00BD05PEW', 'B0039H7Y1Y']
INFO:root:[TOKENIZER] Mask shape: (11924,), True count: 11861
INFO:root:[TOKENIZER] Sentence embeddings shape: (11924, 256)
INFO:root:[TOKENIZER] Force regenerating OPQ quantization results...
INFO:root:[TOKENIZER] Items for training: 11861 of 11924
INFO:root:[TOKENIZER] Training items sample: ['B001JTDL1Q', 'B009QUGMOS', 'B000N8LVJ8', 'B0037UP9MY', 'B000PXVQI2', 'B0007YDBLE', 'B0073RFP6C', 'B000BK7JTO', 'B00BD05PEW', 'B0039H7Y1Y']
INFO:root:[TOKENIZER] Mask shape: (11924,), True count: 11861
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
INFO:root:[TOKENIZER] sent_embs shape: (11924, 256)
INFO:root:[TOKENIZER] train_mask shape: (11924,)
INFO:root:[TOKENIZER] train_mask True count: 11861
INFO:root:[TOKENIZER] Training index...
INFO:root:[TOKENIZER] Saving semantic IDs to cache/AmazonReviews2014/Toys_and_Games/processed/gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8.sem_ids...
INFO:root:[TOKENIZER] Loading semantic IDs from cache/AmazonReviews2014/Toys_and_Games/processed/gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8.sem_ids...
INFO:root:[TOKENIZER] Force regenerate OPQ enabled, ignoring existing mapping files
INFO:root:[TOKENIZER] Force regenerate OPQ enabled, generating new mappings
INFO:root:[TOKENIZER] Saved mappings with tag: gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8_4d to cache/AmazonReviews2014/Toys_and_Games/processed
INFO:root:[TOKENIZER] Files: item_id2tokens_gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8_4d.npy, tokens2item_gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8_4d.pkl
Tokenizing train set:   0%|          | 0/87180 [00:00<?, ? examples/s]Tokenizing train set:   1%|          | 1018/87180 [00:00<00:08, 10093.54 examples/s]Tokenizing train set:   3%|▎         | 2873/87180 [00:00<00:12, 6928.58 examples/s] Tokenizing train set:   5%|▍         | 3943/87180 [00:00<00:10, 7976.70 examples/s]Tokenizing train set:   6%|▌         | 5000/87180 [00:00<00:14, 5790.32 examples/s]Tokenizing train set:   7%|▋         | 6201/87180 [00:00<00:11, 7106.53 examples/s]Tokenizing train set:   8%|▊         | 7116/87180 [00:01<00:13, 5734.90 examples/s]Tokenizing train set:   9%|▉         | 8247/87180 [00:01<00:11, 6855.20 examples/s]Tokenizing train set:  11%|█         | 9584/87180 [00:01<00:12, 6098.14 examples/s]Tokenizing train set:  12%|█▏        | 10876/87180 [00:01<00:10, 7259.31 examples/s]Tokenizing train set:  14%|█▍        | 12000/87180 [00:01<00:09, 7561.92 examples/s]Tokenizing train set:  15%|█▍        | 12998/87180 [00:01<00:11, 6512.74 examples/s]Tokenizing train set:  16%|█▌        | 14000/87180 [00:02<00:10, 6804.19 examples/s]Tokenizing train set:  17%|█▋        | 15000/87180 [00:02<00:13, 5293.81 examples/s]Tokenizing train set:  18%|█▊        | 16111/87180 [00:02<00:11, 6323.97 examples/s]Tokenizing train set:  20%|█▉        | 17108/87180 [00:02<00:12, 5400.24 examples/s]Tokenizing train set:  21%|██        | 18137/87180 [00:02<00:10, 6283.36 examples/s]Tokenizing train set:  22%|██▏       | 19276/87180 [00:02<00:09, 7330.96 examples/s]Tokenizing train set:  24%|██▍       | 20863/87180 [00:03<00:09, 6723.46 examples/s]Tokenizing train set:  25%|██▌       | 22000/87180 [00:03<00:09, 7175.17 examples/s]Tokenizing train set:  26%|██▋       | 23000/87180 [00:03<00:10, 6136.57 examples/s]Tokenizing train set:  28%|██▊       | 24164/87180 [00:03<00:08, 7168.23 examples/s]Tokenizing train set:  30%|██▉       | 25885/87180 [00:03<00:09, 6628.67 examples/s]Tokenizing train set:  31%|███       | 27000/87180 [00:04<00:08, 7039.08 examples/s]Tokenizing train set:  32%|███▏      | 28000/87180 [00:04<00:09, 5944.40 examples/s]Tokenizing train set:  33%|███▎      | 29171/87180 [00:04<00:08, 6966.44 examples/s]Tokenizing train set:  34%|███▍      | 30000/87180 [00:04<00:09, 5770.90 examples/s]Tokenizing train set:  36%|███▌      | 31133/87180 [00:04<00:08, 6811.17 examples/s]Tokenizing train set:  37%|███▋      | 32465/87180 [00:04<00:08, 6161.62 examples/s]Tokenizing train set:  39%|███▉      | 33890/87180 [00:05<00:07, 7436.94 examples/s]Tokenizing train set:  40%|████      | 35000/87180 [00:05<00:06, 7723.56 examples/s]Tokenizing train set:  41%|████▏     | 36000/87180 [00:05<00:08, 6249.11 examples/s]Tokenizing train set:  43%|████▎     | 37124/87180 [00:05<00:06, 7197.14 examples/s]Tokenizing train set:  44%|████▎     | 38000/87180 [00:05<00:08, 5877.74 examples/s]Tokenizing train set:  45%|████▍     | 39000/87180 [00:05<00:07, 6511.12 examples/s]Tokenizing train set:  46%|████▌     | 39862/87180 [00:06<00:06, 6955.37 examples/s]Tokenizing train set:  47%|████▋     | 41000/87180 [00:06<00:08, 5508.15 examples/s]Tokenizing train set:  48%|████▊     | 42161/87180 [00:06<00:06, 6642.87 examples/s]Tokenizing train set:  49%|████▉     | 43000/87180 [00:06<00:07, 5567.87 examples/s]Tokenizing train set:  51%|█████     | 44162/87180 [00:06<00:06, 6731.74 examples/s]Tokenizing train set:  52%|█████▏    | 45108/87180 [00:06<00:07, 5658.37 examples/s]Tokenizing train set:  53%|█████▎    | 46229/87180 [00:07<00:06, 6725.82 examples/s]Tokenizing train set:  55%|█████▍    | 47663/87180 [00:07<00:06, 6274.44 examples/s]Tokenizing train set:  56%|█████▌    | 48903/87180 [00:07<00:05, 7351.39 examples/s]Tokenizing train set:  57%|█████▋    | 50000/87180 [00:07<00:04, 7604.10 examples/s]Tokenizing train set:  58%|█████▊    | 51000/87180 [00:07<00:05, 6162.76 examples/s]Tokenizing train set:  60%|█████▉    | 52241/87180 [00:07<00:04, 7361.08 examples/s]Tokenizing train set:  62%|██████▏   | 53897/87180 [00:08<00:04, 6862.30 examples/s]Tokenizing train set:  63%|██████▎   | 55000/87180 [00:08<00:04, 7247.16 examples/s]Tokenizing train set:  64%|██████▍   | 56000/87180 [00:08<00:05, 6214.38 examples/s]Tokenizing train set:  66%|██████▌   | 57215/87180 [00:08<00:04, 7314.48 examples/s]Tokenizing train set:  68%|██████▊   | 58946/87180 [00:08<00:04, 6825.12 examples/s]Tokenizing train set:  69%|██████▉   | 60000/87180 [00:09<00:03, 7163.99 examples/s]Tokenizing train set:  70%|██████▉   | 61000/87180 [00:09<00:04, 6144.05 examples/s]Tokenizing train set:  71%|███████▏  | 62244/87180 [00:09<00:03, 7290.88 examples/s]Tokenizing train set:  72%|███████▏  | 63188/87180 [00:09<00:03, 6137.21 examples/s]Tokenizing train set:  74%|███████▍  | 64924/87180 [00:09<00:02, 7807.13 examples/s]Tokenizing train set:  76%|███████▌  | 66000/87180 [00:09<00:03, 6252.46 examples/s]Tokenizing train set:  77%|███████▋  | 67234/87180 [00:10<00:02, 7334.99 examples/s]Tokenizing train set:  78%|███████▊  | 68148/87180 [00:10<00:03, 6209.77 examples/s]Tokenizing train set:  80%|████████  | 69925/87180 [00:10<00:02, 7913.14 examples/s]Tokenizing train set:  81%|████████▏ | 71000/87180 [00:10<00:02, 6258.02 examples/s]Tokenizing train set:  83%|████████▎ | 72000/87180 [00:10<00:02, 6844.82 examples/s]Tokenizing train set:  84%|████████▍ | 73108/87180 [00:11<00:02, 6062.45 examples/s]Tokenizing train set:  86%|████████▌ | 74932/87180 [00:11<00:01, 7795.19 examples/s]Tokenizing train set:  87%|████████▋ | 76000/87180 [00:11<00:01, 6246.96 examples/s]Tokenizing train set:  89%|████████▊ | 77204/87180 [00:11<00:01, 7255.03 examples/s]Tokenizing train set:  90%|████████▉ | 78227/87180 [00:11<00:01, 6277.18 examples/s]Tokenizing train set:  92%|█████████▏| 79935/87180 [00:11<00:00, 7867.66 examples/s]Tokenizing train set:  93%|█████████▎| 81000/87180 [00:12<00:00, 6292.00 examples/s]Tokenizing train set:  94%|█████████▍| 82179/87180 [00:12<00:00, 7262.26 examples/s]Tokenizing train set:  96%|█████████▌| 83346/87180 [00:12<00:00, 6390.56 examples/s]Tokenizing train set:  97%|█████████▋| 84935/87180 [00:12<00:00, 7821.36 examples/s]Tokenizing train set:  99%|█████████▊| 86007/87180 [00:12<00:00, 6272.17 examples/s]Tokenizing train set: 100%|██████████| 87180/87180 [00:13<00:00, 7185.31 examples/s]Tokenizing train set: 100%|██████████| 87180/87180 [00:13<00:00, 6691.11 examples/s]
Tokenizing val set:   0%|          | 0/19412 [00:00<?, ? examples/s]Tokenizing val set:   2%|▏         | 421/19412 [00:00<00:08, 2268.73 examples/s]Tokenizing val set:  10%|▉         | 1887/19412 [00:00<00:02, 7026.65 examples/s]Tokenizing val set:  15%|█▌        | 2980/19412 [00:00<00:02, 5597.12 examples/s]Tokenizing val set:  21%|██        | 4000/19412 [00:00<00:02, 6262.56 examples/s]Tokenizing val set:  27%|██▋       | 5179/19412 [00:00<00:01, 7657.11 examples/s]Tokenizing val set:  36%|███▌      | 6906/19412 [00:01<00:01, 6978.57 examples/s]Tokenizing val set:  41%|████      | 8000/19412 [00:01<00:01, 7397.99 examples/s]Tokenizing val set:  46%|████▋     | 8998/19412 [00:01<00:01, 6635.86 examples/s]Tokenizing val set:  52%|█████▏    | 10000/19412 [00:01<00:01, 7007.88 examples/s]Tokenizing val set:  57%|█████▋    | 11000/19412 [00:01<00:01, 5973.20 examples/s]Tokenizing val set:  63%|██████▎   | 12246/19412 [00:01<00:00, 7247.28 examples/s]Tokenizing val set:  68%|██████▊   | 13131/19412 [00:02<00:01, 5882.25 examples/s]Tokenizing val set:  77%|███████▋  | 14928/19412 [00:02<00:00, 7770.02 examples/s]Tokenizing val set:  82%|████████▏ | 16000/19412 [00:02<00:00, 6238.50 examples/s]Tokenizing val set:  89%|████████▉ | 17240/19412 [00:02<00:00, 7351.56 examples/s]Tokenizing val set:  98%|█████████▊| 19000/19412 [00:02<00:00, 6578.38 examples/s]Tokenizing val set: 100%|██████████| 19412/19412 [00:02<00:00, 6682.61 examples/s]
Tokenizing test set:   0%|          | 0/19412 [00:00<?, ? examples/s]Tokenizing test set:   3%|▎         | 501/19412 [00:00<00:06, 2782.13 examples/s]Tokenizing test set:  10%|▉         | 1876/19412 [00:00<00:02, 7144.99 examples/s]Tokenizing test set:  15%|█▍        | 2900/19412 [00:00<00:02, 5511.68 examples/s]Tokenizing test set:  21%|██        | 3993/19412 [00:00<00:02, 6923.66 examples/s]Tokenizing test set:  26%|██▌       | 5000/19412 [00:00<00:01, 7297.51 examples/s]Tokenizing test set:  31%|███       | 6000/19412 [00:00<00:02, 5905.45 examples/s]Tokenizing test set:  37%|███▋      | 7190/19412 [00:01<00:01, 7218.51 examples/s]Tokenizing test set:  43%|████▎     | 8359/19412 [00:01<00:01, 6240.20 examples/s]Tokenizing test set:  51%|█████     | 9900/19412 [00:01<00:01, 7766.29 examples/s]Tokenizing test set:  57%|█████▋    | 11002/19412 [00:01<00:01, 6140.54 examples/s]Tokenizing test set:  63%|██████▎   | 12214/19412 [00:01<00:00, 7241.30 examples/s]Tokenizing test set:  69%|██████▊   | 13320/19412 [00:02<00:00, 6303.12 examples/s]Tokenizing test set:  77%|███████▋  | 14908/19412 [00:02<00:00, 7773.54 examples/s]Tokenizing test set:  82%|████████▏ | 16000/19412 [00:02<00:00, 6241.52 examples/s]Tokenizing test set:  89%|████████▊ | 17222/19412 [00:02<00:00, 7310.35 examples/s]Tokenizing test set:  94%|█████████▍| 18280/19412 [00:02<00:00, 6338.49 examples/s]Tokenizing test set: 100%|██████████| 19412/19412 [00:02<00:00, 7028.40 examples/s]Tokenizing test set: 100%|██████████| 19412/19412 [00:02<00:00, 6704.36 examples/s]
INFO:root:DIFF_GRM(
  (embedding): Embedding(1027, 256)
  (item_mlp): Sequential(
    (0): Linear(in_features=1024, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (mask_emb_table): Embedding(4, 256)
  (pos_emb_enc): Embedding(50, 256)
  (encoder_blocks): ModuleList(
    (0-1): 2 x EncoderBlock(
      (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiHeadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): FeedForward(
        (c_fc): Linear(in_features=256, out_features=1024, bias=True)
        (c_proj): Linear(in_features=1024, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (decoder_blocks): ModuleList(
    (0-3): 4 x DecoderBlock(
      (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attn): MultiHeadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (cross_attn): MultiHeadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): FeedForward(
        (c_fc): Linear(in_features=256, out_features=1024, bias=True)
        (c_proj): Linear(in_features=1024, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (output_adapter): Identity()
  (drop): Dropout(p=0.1, inplace=False)
)
INFO:root:6,391,040
[MODEL] ▶ use SEQUENTIAL views: steps=4, paths=1, augment_factor=4
[DIFF_GRM] Using shared embedding dot-product output layer
[TRAINING] Evaluation config: start from epoch 20, interval: 2
Training - [Epoch 1]:   0%|          | 0/86 [00:00<?, ?it/s]Training - [Epoch 1]:   1%|          | 1/86 [00:11<16:41, 11.78s/it]Training - [Epoch 1]:   2%|▏         | 2/86 [00:14<09:03,  6.46s/it]Training - [Epoch 1]:   3%|▎         | 3/86 [00:15<05:20,  3.87s/it]Training - [Epoch 1]:   5%|▍         | 4/86 [00:16<03:37,  2.65s/it]Training - [Epoch 1]:   6%|▌         | 5/86 [00:16<02:39,  1.97s/it]Training - [Epoch 1]:   7%|▋         | 6/86 [00:17<02:05,  1.57s/it]Training - [Epoch 1]:   8%|▊         | 7/86 [00:18<01:43,  1.31s/it]Training - [Epoch 1]:   9%|▉         | 8/86 [00:19<01:28,  1.14s/it]Training - [Epoch 1]:  10%|█         | 9/86 [00:19<01:18,  1.02s/it]Training - [Epoch 1]:  12%|█▏        | 10/86 [00:20<01:12,  1.06it/s]Training - [Epoch 1]:  13%|█▎        | 11/86 [00:21<01:07,  1.12it/s]Training - [Epoch 1]:  14%|█▍        | 12/86 [00:22<01:03,  1.16it/s]Training - [Epoch 1]:  15%|█▌        | 13/86 [00:23<01:00,  1.20it/s]Training - [Epoch 1]:  16%|█▋        | 14/86 [00:23<00:58,  1.22it/s]Training - [Epoch 1]:  17%|█▋        | 15/86 [00:24<00:57,  1.24it/s]Training - [Epoch 1]:  19%|█▊        | 16/86 [00:25<00:55,  1.26it/s]Training - [Epoch 1]:  20%|█▉        | 17/86 [00:26<00:54,  1.27it/s]Training - [Epoch 1]:  21%|██        | 18/86 [00:26<00:53,  1.27it/s]Training - [Epoch 1]:  22%|██▏       | 19/86 [00:27<00:52,  1.28it/s]Training - [Epoch 1]:  23%|██▎       | 20/86 [00:28<00:51,  1.28it/s]Training - [Epoch 1]:  24%|██▍       | 21/86 [00:29<00:50,  1.28it/s]Training - [Epoch 1]:  26%|██▌       | 22/86 [00:30<00:49,  1.28it/s]Training - [Epoch 1]:  27%|██▋       | 23/86 [00:30<00:48,  1.29it/s]Training - [Epoch 1]:  28%|██▊       | 24/86 [00:31<00:48,  1.29it/s]Training - [Epoch 1]:  29%|██▉       | 25/86 [00:32<00:47,  1.29it/s]Training - [Epoch 1]:  30%|███       | 26/86 [00:33<00:46,  1.29it/s]Training - [Epoch 1]:  31%|███▏      | 27/86 [00:33<00:45,  1.29it/s]Training - [Epoch 1]:  33%|███▎      | 28/86 [00:34<00:45,  1.29it/s]Training - [Epoch 1]:  34%|███▎      | 29/86 [00:35<00:44,  1.29it/s]Training - [Epoch 1]:  35%|███▍      | 30/86 [00:36<00:43,  1.29it/s]Training - [Epoch 1]:  36%|███▌      | 31/86 [00:37<00:42,  1.29it/s]Training - [Epoch 1]:  37%|███▋      | 32/86 [00:37<00:41,  1.29it/s]Training - [Epoch 1]:  38%|███▊      | 33/86 [00:38<00:41,  1.29it/s]Training - [Epoch 1]:  40%|███▉      | 34/86 [00:39<00:40,  1.29it/s]Training - [Epoch 1]:  41%|████      | 35/86 [00:40<00:39,  1.29it/s]Training - [Epoch 1]:  42%|████▏     | 36/86 [00:40<00:38,  1.29it/s]Training - [Epoch 1]:  43%|████▎     | 37/86 [00:41<00:37,  1.29it/s]Training - [Epoch 1]:  44%|████▍     | 38/86 [00:42<00:37,  1.29it/s]Training - [Epoch 1]:  45%|████▌     | 39/86 [00:43<00:36,  1.29it/s]Training - [Epoch 1]:  47%|████▋     | 40/86 [00:44<00:35,  1.28it/s]Training - [Epoch 1]:  48%|████▊     | 41/86 [00:44<00:37,  1.20it/s]Training - [Epoch 1]:  49%|████▉     | 42/86 [00:45<00:35,  1.23it/s]Training - [Epoch 1]:  50%|█████     | 43/86 [00:46<00:34,  1.25it/s]Training - [Epoch 1]:  51%|█████     | 44/86 [00:47<00:33,  1.26it/s]Training - [Epoch 1]:  52%|█████▏    | 45/86 [00:48<00:32,  1.27it/s]Training - [Epoch 1]:  53%|█████▎    | 46/86 [00:48<00:31,  1.28it/s]Training - [Epoch 1]:  55%|█████▍    | 47/86 [00:49<00:30,  1.28it/s]Training - [Epoch 1]:  56%|█████▌    | 48/86 [00:50<00:29,  1.29it/s]Training - [Epoch 1]:  57%|█████▋    | 49/86 [00:51<00:28,  1.29it/s]Training - [Epoch 1]:  58%|█████▊    | 50/86 [00:51<00:27,  1.29it/s]Training - [Epoch 1]:  59%|█████▉    | 51/86 [00:52<00:27,  1.29it/s]Training - [Epoch 1]:  60%|██████    | 52/86 [00:53<00:26,  1.29it/s]Training - [Epoch 1]:  62%|██████▏   | 53/86 [00:54<00:25,  1.29it/s]Training - [Epoch 1]:  63%|██████▎   | 54/86 [00:54<00:24,  1.29it/s]Training - [Epoch 1]:  64%|██████▍   | 55/86 [00:55<00:23,  1.30it/s]Training - [Epoch 1]:  65%|██████▌   | 56/86 [00:56<00:23,  1.29it/s]Training - [Epoch 1]:  66%|██████▋   | 57/86 [00:57<00:22,  1.29it/s]Training - [Epoch 1]:  67%|██████▋   | 58/86 [00:58<00:21,  1.29it/s]Training - [Epoch 1]:  69%|██████▊   | 59/86 [00:58<00:20,  1.30it/s]Training - [Epoch 1]:  70%|██████▉   | 60/86 [00:59<00:20,  1.29it/s]Training - [Epoch 1]:  71%|███████   | 61/86 [01:00<00:19,  1.29it/s]Training - [Epoch 1]:  72%|███████▏  | 62/86 [01:01<00:18,  1.29it/s]Training - [Epoch 1]:  73%|███████▎  | 63/86 [01:01<00:17,  1.29it/s]Training - [Epoch 1]:  74%|███████▍  | 64/86 [01:02<00:17,  1.29it/s]Training - [Epoch 1]:  76%|███████▌  | 65/86 [01:03<00:16,  1.29it/s]Training - [Epoch 1]:  77%|███████▋  | 66/86 [01:04<00:15,  1.29it/s]Training - [Epoch 1]:  78%|███████▊  | 67/86 [01:05<00:14,  1.29it/s]Training - [Epoch 1]:  79%|███████▉  | 68/86 [01:05<00:13,  1.29it/s]Training - [Epoch 1]:  80%|████████  | 69/86 [01:06<00:13,  1.29it/s]Training - [Epoch 1]:  81%|████████▏ | 70/86 [01:07<00:12,  1.29it/s]Training - [Epoch 1]:  83%|████████▎ | 71/86 [01:08<00:11,  1.29it/s]Training - [Epoch 1]:  84%|████████▎ | 72/86 [01:08<00:10,  1.29it/s]Training - [Epoch 1]:  85%|████████▍ | 73/86 [01:09<00:10,  1.29it/s]Training - [Epoch 1]:  86%|████████▌ | 74/86 [01:10<00:09,  1.29it/s]Training - [Epoch 1]:  87%|████████▋ | 75/86 [01:11<00:08,  1.29it/s]Training - [Epoch 1]:  88%|████████▊ | 76/86 [01:12<00:07,  1.29it/s]Training - [Epoch 1]:  90%|████████▉ | 77/86 [01:12<00:06,  1.29it/s]Training - [Epoch 1]:  91%|█████████ | 78/86 [01:13<00:06,  1.29it/s]Training - [Epoch 1]:  92%|█████████▏| 79/86 [01:14<00:05,  1.29it/s]Training - [Epoch 1]:  93%|█████████▎| 80/86 [01:15<00:04,  1.30it/s]Training - [Epoch 1]:  94%|█████████▍| 81/86 [01:15<00:03,  1.30it/s]Training - [Epoch 1]:  95%|█████████▌| 82/86 [01:16<00:03,  1.30it/s]Training - [Epoch 1]:  97%|█████████▋| 83/86 [01:17<00:02,  1.30it/s]Training - [Epoch 1]:  98%|█████████▊| 84/86 [01:18<00:01,  1.29it/s]Training - [Epoch 1]:  99%|█████████▉| 85/86 [01:18<00:00,  1.61it/s]Training - [Epoch 1]: 100%|██████████| 86/86 [01:18<00:00,  1.10it/s]
[DIFF_GRM] Using RPG_ED-style encoder: MLP compression + fixed 50-length sequence
[DIFF_GRM] vocab_size: 1027, codebook_size: 256
[DIFF_GRM] masking_strategy: sequential
[Epoch 1] Train Loss: 5.501955
Training - [Epoch 2]:   0%|          | 0/86 [00:00<?, ?it/s]Training - [Epoch 2]:   1%|          | 1/86 [00:01<01:58,  1.39s/it]Training - [Epoch 2]:   2%|▏         | 2/86 [00:02<01:26,  1.03s/it]Training - [Epoch 2]:   3%|▎         | 3/86 [00:02<01:16,  1.09it/s]Training - [Epoch 2]:   5%|▍         | 4/86 [00:03<01:10,  1.16it/s]Training - [Epoch 2]:   6%|▌         | 5/86 [00:04<01:07,  1.20it/s]Training - [Epoch 2]:   7%|▋         | 6/86 [00:05<01:05,  1.23it/s]Training - [Epoch 2]:   8%|▊         | 7/86 [00:06<01:03,  1.25it/s]Training - [Epoch 2]:   9%|▉         | 8/86 [00:06<01:01,  1.26it/s]Training - [Epoch 2]:  10%|█         | 9/86 [00:07<01:00,  1.27it/s]Training - [Epoch 2]:  12%|█▏        | 10/86 [00:08<00:59,  1.27it/s]Training - [Epoch 2]:  13%|█▎        | 11/86 [00:09<00:58,  1.28it/s]Training - [Epoch 2]:  14%|█▍        | 12/86 [00:09<00:57,  1.28it/s]Training - [Epoch 2]:  15%|█▌        | 13/86 [00:10<00:56,  1.28it/s]Training - [Epoch 2]:  16%|█▋        | 14/86 [00:11<00:56,  1.28it/s]Training - [Epoch 2]:  17%|█▋        | 15/86 [00:12<00:55,  1.28it/s]Training - [Epoch 2]:  19%|█▊        | 16/86 [00:13<00:54,  1.28it/s]Training - [Epoch 2]:  20%|█▉        | 17/86 [00:13<00:53,  1.28it/s]Training - [Epoch 2]:  21%|██        | 18/86 [00:14<00:52,  1.28it/s]Training - [Epoch 2]:  22%|██▏       | 19/86 [00:15<00:52,  1.28it/s]Training - [Epoch 2]:  23%|██▎       | 20/86 [00:16<00:51,  1.28it/s]Training - [Epoch 2]:  24%|██▍       | 21/86 [00:16<00:50,  1.28it/s]Training - [Epoch 2]:  26%|██▌       | 22/86 [00:17<00:49,  1.28it/s]Training - [Epoch 2]:  27%|██▋       | 23/86 [00:18<00:49,  1.28it/s]Training - [Epoch 2]:  28%|██▊       | 24/86 [00:19<00:48,  1.28it/s]Training - [Epoch 2]:  29%|██▉       | 25/86 [00:20<00:47,  1.28it/s]Training - [Epoch 2]:  30%|███       | 26/86 [00:20<00:46,  1.29it/s]Training - [Epoch 2]:  31%|███▏      | 27/86 [00:21<00:45,  1.29it/s]Training - [Epoch 2]:  33%|███▎      | 28/86 [00:22<00:45,  1.29it/s]Training - [Epoch 2]:  34%|███▎      | 29/86 [00:23<00:44,  1.29it/s]Training - [Epoch 2]:  35%|███▍      | 30/86 [00:23<00:43,  1.29it/s]Training - [Epoch 2]:  36%|███▌      | 31/86 [00:24<00:42,  1.28it/s]Training - [Epoch 2]:  37%|███▋      | 32/86 [00:25<00:42,  1.29it/s]Training - [Epoch 2]:  38%|███▊      | 33/86 [00:26<00:41,  1.28it/s]Training - [Epoch 2]:  40%|███▉      | 34/86 [00:27<00:40,  1.28it/s]Training - [Epoch 2]:  41%|████      | 35/86 [00:27<00:39,  1.29it/s]Training - [Epoch 2]:  42%|████▏     | 36/86 [00:28<00:38,  1.28it/s]Training - [Epoch 2]:  43%|████▎     | 37/86 [00:29<00:38,  1.29it/s]Training - [Epoch 2]:  44%|████▍     | 38/86 [00:30<00:37,  1.29it/s]Training - [Epoch 2]:  45%|████▌     | 39/86 [00:30<00:36,  1.29it/s]Training - [Epoch 2]:  47%|████▋     | 40/86 [00:31<00:35,  1.29it/s]Training - [Epoch 2]:  48%|████▊     | 41/86 [00:32<00:34,  1.29it/s]Training - [Epoch 2]:  49%|████▉     | 42/86 [00:33<00:34,  1.28it/s]Training - [Epoch 2]:  50%|█████     | 43/86 [00:34<00:33,  1.29it/s]Training - [Epoch 2]:  51%|█████     | 44/86 [00:34<00:32,  1.29it/s]Training - [Epoch 2]:  52%|█████▏    | 45/86 [00:35<00:31,  1.29it/s]Training - [Epoch 2]:  53%|█████▎    | 46/86 [00:36<00:31,  1.29it/s]Training - [Epoch 2]:  55%|█████▍    | 47/86 [00:37<00:30,  1.29it/s]Training - [Epoch 2]:  56%|█████▌    | 48/86 [00:37<00:29,  1.29it/s]Training - [Epoch 2]:  57%|█████▋    | 49/86 [00:38<00:28,  1.29it/s]Training - [Epoch 2]:  58%|█████▊    | 50/86 [00:39<00:28,  1.29it/s]Training - [Epoch 2]:  59%|█████▉    | 51/86 [00:40<00:27,  1.29it/s]Training - [Epoch 2]:  60%|██████    | 52/86 [00:41<00:26,  1.28it/s]Training - [Epoch 2]:  62%|██████▏   | 53/86 [00:41<00:25,  1.28it/s]Training - [Epoch 2]:  63%|██████▎   | 54/86 [00:42<00:26,  1.21it/s]Training - [Epoch 2]:  64%|██████▍   | 55/86 [00:43<00:25,  1.23it/s]Training - [Epoch 2]:  65%|██████▌   | 56/86 [00:44<00:24,  1.25it/s]Training - [Epoch 2]:  66%|██████▋   | 57/86 [00:45<00:23,  1.26it/s]Training - [Epoch 2]:  67%|██████▋   | 58/86 [00:45<00:22,  1.27it/s]Training - [Epoch 2]:  69%|██████▊   | 59/86 [00:46<00:21,  1.27it/s]Training - [Epoch 2]:  70%|██████▉   | 60/86 [00:47<00:20,  1.28it/s]Training - [Epoch 2]:  71%|███████   | 61/86 [00:48<00:19,  1.28it/s]Training - [Epoch 2]:  72%|███████▏  | 62/86 [00:49<00:18,  1.28it/s]Training - [Epoch 2]:  73%|███████▎  | 63/86 [00:49<00:17,  1.28it/s]Training - [Epoch 2]:  74%|███████▍  | 64/86 [00:50<00:17,  1.29it/s]Training - [Epoch 2]:  76%|███████▌  | 65/86 [00:51<00:16,  1.29it/s]Training - [Epoch 2]:  77%|███████▋  | 66/86 [00:52<00:15,  1.29it/s]Training - [Epoch 2]:  78%|███████▊  | 67/86 [00:52<00:14,  1.29it/s]Training - [Epoch 2]:  79%|███████▉  | 68/86 [00:53<00:13,  1.29it/s]Training - [Epoch 2]:  80%|████████  | 69/86 [00:54<00:13,  1.28it/s]Training - [Epoch 2]:  81%|████████▏ | 70/86 [00:55<00:12,  1.29it/s]Training - [Epoch 2]:  83%|████████▎ | 71/86 [00:56<00:11,  1.29it/s]Training - [Epoch 2]:  84%|████████▎ | 72/86 [00:56<00:10,  1.29it/s]Training - [Epoch 2]:  85%|████████▍ | 73/86 [00:57<00:10,  1.28it/s]Training - [Epoch 2]:  86%|████████▌ | 74/86 [00:58<00:09,  1.29it/s]