/root/anaconda3/envs/lz_grm_pd/lib/python3.10/site-packages/accelerate/accelerator.py:399: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:root:Device: cuda
INFO:root:[DATASET] Amazon Reviews 2014 for category: Toys_and_Games
INFO:root:[DATASET] Reviews have been processed...
INFO:root:[DATASET] Metadata has been processed...
INFO:root:[Dataset] AmazonReviews2014
	Number of users: 19413
	Number of items: 11925
	Number of interactions: 167597
	Average item sequence length: 8.633235460773708
INFO:root:[TOKENIZER] Index factory: OPQ4,IVF1,PQ4x8
INFO:root:[TOKENIZER] Encoding sentence embeddings...
INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: Alibaba-NLP/gte-large-en-v1.5
Batches:   0%|          | 0/47 [00:00<?, ?it/s]Batches:   2%|▏         | 1/47 [00:32<24:46, 32.32s/it]Batches:   4%|▍         | 2/47 [00:40<13:33, 18.07s/it]Batches:   6%|▋         | 3/47 [00:45<08:59, 12.26s/it]Batches:   9%|▊         | 4/47 [00:51<06:48,  9.49s/it]Batches:  11%|█         | 5/47 [00:54<05:14,  7.49s/it]Batches:  13%|█▎        | 6/47 [00:58<04:10,  6.12s/it]Batches:  15%|█▍        | 7/47 [01:01<03:24,  5.12s/it]Batches:  17%|█▋        | 8/47 [01:04<02:54,  4.48s/it]Batches:  19%|█▉        | 9/47 [01:07<02:34,  4.06s/it]Batches:  21%|██▏       | 10/47 [01:11<02:21,  3.84s/it]Batches:  23%|██▎       | 11/47 [01:14<02:12,  3.67s/it]Batches:  26%|██▌       | 12/47 [01:17<01:58,  3.38s/it]Batches:  28%|██▊       | 13/47 [01:19<01:46,  3.14s/it]Batches:  30%|██▉       | 14/47 [01:22<01:36,  2.92s/it]Batches:  32%|███▏      | 15/47 [01:24<01:29,  2.79s/it]Batches:  34%|███▍      | 16/47 [01:27<01:23,  2.69s/it]Batches:  36%|███▌      | 17/47 [01:29<01:20,  2.69s/it]Batches:  38%|███▊      | 18/47 [01:32<01:16,  2.62s/it]Batches:  40%|████      | 19/47 [01:34<01:09,  2.47s/it]Batches:  43%|████▎     | 20/47 [01:36<01:03,  2.37s/it]Batches:  45%|████▍     | 21/47 [01:38<00:58,  2.25s/it]Batches:  47%|████▋     | 22/47 [01:40<00:54,  2.17s/it]Batches:  49%|████▉     | 23/47 [01:42<00:50,  2.12s/it]Batches:  51%|█████     | 24/47 [01:44<00:51,  2.23s/it]Batches:  53%|█████▎    | 25/47 [01:46<00:47,  2.14s/it]Batches:  55%|█████▌    | 26/47 [01:48<00:42,  2.04s/it]Batches:  57%|█████▋    | 27/47 [01:50<00:39,  1.97s/it]Batches:  60%|█████▉    | 28/47 [01:52<00:36,  1.90s/it]Batches:  62%|██████▏   | 29/47 [01:54<00:34,  1.93s/it]Batches:  64%|██████▍   | 30/47 [01:55<00:32,  1.88s/it]Batches:  66%|██████▌   | 31/47 [01:57<00:28,  1.80s/it]Batches:  68%|██████▊   | 32/47 [01:59<00:26,  1.76s/it]Batches:  70%|███████   | 33/47 [02:00<00:24,  1.72s/it]Batches:  72%|███████▏  | 34/47 [02:02<00:21,  1.69s/it]Batches:  74%|███████▍  | 35/47 [02:03<00:19,  1.65s/it]Batches:  77%|███████▋  | 36/47 [02:05<00:18,  1.64s/it]Batches:  79%|███████▊  | 37/47 [02:07<00:16,  1.60s/it]Batches:  81%|████████  | 38/47 [02:08<00:14,  1.60s/it]Batches:  83%|████████▎ | 39/47 [02:10<00:12,  1.52s/it]Batches:  85%|████████▌ | 40/47 [02:11<00:10,  1.45s/it]Batches:  87%|████████▋ | 41/47 [02:12<00:08,  1.35s/it]Batches:  89%|████████▉ | 42/47 [02:13<00:06,  1.24s/it]Batches:  91%|█████████▏| 43/47 [02:14<00:04,  1.15s/it]Batches:  94%|█████████▎| 44/47 [02:15<00:03,  1.04s/it]Batches:  96%|█████████▌| 45/47 [02:15<00:01,  1.06it/s]Batches:  98%|█████████▊| 46/47 [02:16<00:00,  1.16it/s]Batches: 100%|██████████| 47/47 [02:16<00:00,  1.43it/s]Batches: 100%|██████████| 47/47 [02:16<00:00,  2.91s/it]
INFO:root:[TOKENIZER] Applying PCA to sentence embeddings...
INFO:root:[TOKENIZER] Items for training: 11861 of 11924
INFO:root:[TOKENIZER] Training items sample: ['1581174837', 'B003AN0OOQ', 'B00BG8F0H8', 'B000FCUS14', 'B00CFELU3A', 'B000CDC87I', 'B0007ZULSY', 'B005XTLPAC', 'B007XVYO5Q', 'B0035A2PKU']
INFO:root:[TOKENIZER] Mask shape: (11924,), True count: 11861
INFO:root:[TOKENIZER] Sentence embeddings shape: (11924, 256)
INFO:root:[TOKENIZER] Force regenerating OPQ quantization results...
INFO:root:[TOKENIZER] Items for training: 11861 of 11924
INFO:root:[TOKENIZER] Training items sample: ['1581174837', 'B003AN0OOQ', 'B00BG8F0H8', 'B000FCUS14', 'B00CFELU3A', 'B000CDC87I', 'B0007ZULSY', 'B005XTLPAC', 'B007XVYO5Q', 'B0035A2PKU']
INFO:root:[TOKENIZER] Mask shape: (11924,), True count: 11861
INFO:faiss.loader:Loading faiss with AVX2 support.
INFO:faiss.loader:Successfully loaded faiss with AVX2 support.
INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
INFO:root:[TOKENIZER] sent_embs shape: (11924, 256)
INFO:root:[TOKENIZER] train_mask shape: (11924,)
INFO:root:[TOKENIZER] train_mask True count: 11861
INFO:root:[TOKENIZER] Training index...
INFO:root:[TOKENIZER] Saving semantic IDs to cache/AmazonReviews2014/Toys_and_Games/processed/gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8.sem_ids...
INFO:root:[TOKENIZER] Loading semantic IDs from cache/AmazonReviews2014/Toys_and_Games/processed/gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8.sem_ids...
INFO:root:[TOKENIZER] Force regenerate OPQ enabled, ignoring existing mapping files
INFO:root:[TOKENIZER] Force regenerate OPQ enabled, generating new mappings
INFO:root:[TOKENIZER] Saved mappings with tag: gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8_4d to cache/AmazonReviews2014/Toys_and_Games/processed
INFO:root:[TOKENIZER] Files: item_id2tokens_gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8_4d.npy, tokens2item_gte-large-en-v1.5_pca256_OPQ4,IVF1,PQ4x8_4d.pkl
Tokenizing train set:   0%|          | 0/87180 [00:00<?, ? examples/s]Tokenizing train set:   1%|          | 1000/87180 [00:00<00:08, 9899.23 examples/s]Tokenizing train set:   2%|▏         | 2000/87180 [00:00<00:16, 5318.85 examples/s]Tokenizing train set:   4%|▎         | 3087/87180 [00:00<00:11, 7032.69 examples/s]Tokenizing train set:   5%|▍         | 4196/87180 [00:00<00:10, 8274.41 examples/s]Tokenizing train set:   7%|▋         | 5900/87180 [00:00<00:11, 7032.33 examples/s]Tokenizing train set:   8%|▊         | 6848/87180 [00:01<00:14, 5720.35 examples/s]Tokenizing train set:   9%|▉         | 7931/87180 [00:01<00:11, 6694.25 examples/s]Tokenizing train set:  10%|█         | 9000/87180 [00:01<00:11, 7070.89 examples/s]Tokenizing train set:  11%|█▏        | 10000/87180 [00:01<00:12, 5982.31 examples/s]Tokenizing train set:  13%|█▎        | 11161/87180 [00:01<00:10, 7094.75 examples/s]Tokenizing train set:  14%|█▍        | 12148/87180 [00:01<00:12, 6016.39 examples/s]Tokenizing train set:  16%|█▌        | 13890/87180 [00:02<00:09, 7732.48 examples/s]Tokenizing train set:  17%|█▋        | 15000/87180 [00:02<00:11, 6204.21 examples/s]Tokenizing train set:  19%|█▊        | 16135/87180 [00:02<00:09, 7125.57 examples/s]Tokenizing train set:  20%|█▉        | 17108/87180 [00:02<00:11, 6110.81 examples/s]Tokenizing train set:  21%|██        | 18145/87180 [00:02<00:09, 6914.60 examples/s]Tokenizing train set:  23%|██▎       | 19663/87180 [00:02<00:10, 6450.15 examples/s]Tokenizing train set:  24%|██▍       | 20851/87180 [00:03<00:08, 7369.94 examples/s]Tokenizing train set:  25%|██▌       | 22000/87180 [00:03<00:08, 7683.96 examples/s]Tokenizing train set:  26%|██▋       | 23000/87180 [00:03<00:10, 6405.68 examples/s]Tokenizing train set:  28%|██▊       | 24173/87180 [00:03<00:08, 7441.51 examples/s]Tokenizing train set:  30%|██▉       | 25893/87180 [00:03<00:08, 6968.94 examples/s]Tokenizing train set:  31%|███       | 27000/87180 [00:03<00:08, 7312.53 examples/s]Tokenizing train set:  32%|███▏      | 28000/87180 [00:04<00:09, 6194.08 examples/s]Tokenizing train set:  33%|███▎      | 29161/87180 [00:04<00:08, 7188.67 examples/s]Tokenizing train set:  34%|███▍      | 30000/87180 [00:04<00:09, 5937.04 examples/s]Tokenizing train set:  36%|███▌      | 31165/87180 [00:04<00:07, 7031.98 examples/s]Tokenizing train set:  37%|███▋      | 32465/87180 [00:04<00:08, 6332.10 examples/s]Tokenizing train set:  39%|███▉      | 33876/87180 [00:04<00:07, 7563.43 examples/s]Tokenizing train set:  40%|████      | 35000/87180 [00:05<00:06, 7815.49 examples/s]Tokenizing train set:  41%|████▏     | 36000/87180 [00:05<00:08, 6382.49 examples/s]Tokenizing train set:  43%|████▎     | 37173/87180 [00:05<00:06, 7415.53 examples/s]Tokenizing train set:  45%|████▍     | 38891/87180 [00:05<00:07, 6898.08 examples/s]Tokenizing train set:  46%|████▌     | 40000/87180 [00:05<00:06, 7262.18 examples/s]Tokenizing train set:  47%|████▋     | 41000/87180 [00:06<00:07, 6260.24 examples/s]Tokenizing train set:  48%|████▊     | 42191/87180 [00:06<00:06, 7306.28 examples/s]Tokenizing train set:  50%|█████     | 43911/87180 [00:06<00:06, 6962.31 examples/s]Tokenizing train set:  52%|█████▏    | 45000/87180 [00:06<00:05, 7268.26 examples/s]Tokenizing train set:  53%|█████▎    | 45979/87180 [00:06<00:06, 6603.13 examples/s]Tokenizing train set:  54%|█████▍    | 47000/87180 [00:06<00:05, 6950.46 examples/s]Tokenizing train set:  55%|█████▌    | 48000/87180 [00:07<00:06, 5961.54 examples/s]Tokenizing train set:  56%|█████▋    | 49176/87180 [00:07<00:05, 7059.50 examples/s]Tokenizing train set:  58%|█████▊    | 50227/87180 [00:07<00:06, 6132.40 examples/s]Tokenizing train set:  60%|█████▉    | 51940/87180 [00:07<00:04, 7790.62 examples/s]Tokenizing train set:  61%|██████    | 53000/87180 [00:07<00:05, 6201.22 examples/s]Tokenizing train set:  62%|██████▏   | 54172/87180 [00:07<00:04, 7186.68 examples/s]Tokenizing train set:  63%|██████▎   | 55346/87180 [00:08<00:05, 6355.42 examples/s]Tokenizing train set:  65%|██████▌   | 56935/87180 [00:08<00:03, 7807.46 examples/s]Tokenizing train set:  67%|██████▋   | 58000/87180 [00:08<00:04, 6237.90 examples/s]Tokenizing train set:  68%|██████▊   | 59198/87180 [00:08<00:03, 7257.06 examples/s]Tokenizing train set:  69%|██████▉   | 60465/87180 [00:08<00:04, 6452.65 examples/s]Tokenizing train set:  71%|███████   | 61953/87180 [00:09<00:03, 7727.99 examples/s]Tokenizing train set:  72%|███████▏  | 63000/87180 [00:09<00:03, 7856.77 examples/s]Tokenizing train set:  73%|███████▎  | 64000/87180 [00:09<00:03, 6535.39 examples/s]Tokenizing train set:  75%|███████▍  | 65186/87180 [00:09<00:02, 7567.74 examples/s]Tokenizing train set:  77%|███████▋  | 66922/87180 [00:09<00:02, 7064.79 examples/s]Tokenizing train set:  78%|███████▊  | 68000/87180 [00:09<00:02, 7376.86 examples/s]Tokenizing train set:  79%|███████▉  | 69000/87180 [00:10<00:02, 6316.89 examples/s]Tokenizing train set:  81%|████████  | 70214/87180 [00:10<00:02, 7393.70 examples/s]Tokenizing train set:  83%|████████▎ | 71928/87180 [00:10<00:02, 6985.70 examples/s]Tokenizing train set:  84%|████████▎ | 72907/87180 [00:10<00:01, 7428.65 examples/s]Tokenizing train set:  85%|████████▍ | 73986/87180 [00:10<00:02, 6329.60 examples/s]Tokenizing train set:  86%|████████▌ | 75000/87180 [00:10<00:01, 6730.84 examples/s]Tokenizing train set:  87%|████████▋ | 76000/87180 [00:11<00:01, 5824.21 examples/s]Tokenizing train set:  89%|████████▊ | 77183/87180 [00:11<00:01, 6937.56 examples/s]Tokenizing train set:  90%|████████▉ | 78227/87180 [00:11<00:01, 6051.16 examples/s]Tokenizing train set:  92%|█████████▏| 79928/87180 [00:11<00:00, 7722.48 examples/s]Tokenizing train set:  93%|█████████▎| 81000/87180 [00:11<00:01, 6159.01 examples/s]Tokenizing train set:  94%|█████████▍| 82191/87180 [00:11<00:00, 7183.27 examples/s]Tokenizing train set:  96%|█████████▌| 83346/87180 [00:12<00:00, 6347.38 examples/s]Tokenizing train set:  97%|█████████▋| 84937/87180 [00:12<00:00, 7803.53 examples/s]Tokenizing train set:  99%|█████████▊| 86000/87180 [00:12<00:00, 6201.22 examples/s]Tokenizing train set: 100%|██████████| 87180/87180 [00:12<00:00, 7142.80 examples/s]Tokenizing train set: 100%|██████████| 87180/87180 [00:12<00:00, 6851.82 examples/s]
Tokenizing val set:   0%|          | 0/19412 [00:00<?, ? examples/s]Tokenizing val set:   2%|▏         | 421/19412 [00:00<00:07, 2516.64 examples/s]Tokenizing val set:  10%|▉         | 1879/19412 [00:00<00:02, 7333.10 examples/s]Tokenizing val set:  15%|█▌        | 2980/19412 [00:00<00:02, 5673.34 examples/s]Tokenizing val set:  21%|██        | 4000/19412 [00:00<00:02, 6290.31 examples/s]Tokenizing val set:  27%|██▋       | 5171/19412 [00:00<00:01, 7661.82 examples/s]Tokenizing val set:  36%|███▌      | 6894/19412 [00:01<00:01, 6933.32 examples/s]Tokenizing val set:  41%|████      | 8000/19412 [00:01<00:01, 7351.12 examples/s]Tokenizing val set:  46%|████▌     | 8958/19412 [00:01<00:01, 6572.34 examples/s]Tokenizing val set:  52%|█████▏    | 10000/19412 [00:01<00:01, 6996.88 examples/s]Tokenizing val set:  57%|█████▋    | 11000/19412 [00:01<00:01, 5970.94 examples/s]Tokenizing val set:  63%|██████▎   | 12219/19412 [00:01<00:01, 7189.17 examples/s]Tokenizing val set:  68%|██████▊   | 13145/19412 [00:02<00:01, 5977.18 examples/s]Tokenizing val set:  77%|███████▋  | 14914/19412 [00:02<00:00, 7804.99 examples/s]Tokenizing val set:  82%|████████▏ | 16000/19412 [00:02<00:00, 6281.74 examples/s]Tokenizing val set:  89%|████████▊ | 17198/19412 [00:02<00:00, 7312.74 examples/s]Tokenizing val set:  97%|█████████▋| 18912/19412 [00:02<00:00, 6780.01 examples/s]Tokenizing val set: 100%|██████████| 19412/19412 [00:02<00:00, 6686.78 examples/s]
Tokenizing test set:   0%|          | 0/19412 [00:00<?, ? examples/s]Tokenizing test set:   3%|▎         | 501/19412 [00:00<00:07, 2553.49 examples/s]Tokenizing test set:  10%|▉         | 1877/19412 [00:00<00:02, 6767.28 examples/s]Tokenizing test set:  15%|█▍        | 2900/19412 [00:00<00:03, 5395.97 examples/s]Tokenizing test set:  21%|██        | 3995/19412 [00:00<00:02, 6821.22 examples/s]Tokenizing test set:  26%|██▌       | 5000/19412 [00:00<00:01, 7206.79 examples/s]Tokenizing test set:  31%|███       | 6000/19412 [00:01<00:02, 5861.56 examples/s]Tokenizing test set:  37%|███▋      | 7174/19412 [00:01<00:01, 7141.31 examples/s]Tokenizing test set:  43%|████▎     | 8359/19412 [00:01<00:01, 6217.24 examples/s]Tokenizing test set:  51%|█████     | 9897/19412 [00:01<00:01, 7706.14 examples/s]Tokenizing test set:  57%|█████▋    | 11000/19412 [00:01<00:01, 6118.20 examples/s]Tokenizing test set:  63%|██████▎   | 12195/19412 [00:01<00:01, 7188.47 examples/s]Tokenizing test set:  69%|██████▊   | 13320/19412 [00:02<00:00, 6321.06 examples/s]Tokenizing test set:  77%|███████▋  | 14907/19412 [00:02<00:00, 7752.53 examples/s]Tokenizing test set:  82%|████████▏ | 16000/19412 [00:02<00:00, 6149.30 examples/s]Tokenizing test set:  89%|████████▊ | 17201/19412 [00:02<00:00, 7183.17 examples/s]Tokenizing test set:  94%|█████████▍| 18280/19412 [00:02<00:00, 6295.62 examples/s]Tokenizing test set: 100%|██████████| 19412/19412 [00:02<00:00, 7001.86 examples/s]Tokenizing test set: 100%|██████████| 19412/19412 [00:02<00:00, 6634.79 examples/s]
INFO:root:DIFF_GRM(
  (embedding): Embedding(1027, 256)
  (item_mlp): Sequential(
    (0): Linear(in_features=1024, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (mask_emb_table): Embedding(4, 256)
  (pos_emb_enc): Embedding(50, 256)
  (encoder_blocks): ModuleList(
    (0-1): 2 x EncoderBlock(
      (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (attn): MultiHeadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): FeedForward(
        (c_fc): Linear(in_features=256, out_features=1024, bias=True)
        (c_proj): Linear(in_features=1024, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (decoder_blocks): ModuleList(
    (0-3): 4 x DecoderBlock(
      (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (self_attn): MultiHeadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (cross_attn): MultiHeadAttention(
        (qkv): Linear(in_features=256, out_features=768, bias=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): FeedForward(
        (c_fc): Linear(in_features=256, out_features=1024, bias=True)
        (c_proj): Linear(in_features=1024, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (output_adapter): Identity()
  (drop): Dropout(p=0.1, inplace=False)
)
INFO:root:6,391,040
[MODEL] ▶ use SEQUENTIAL views: steps=4, paths=2, augment_factor=8
[DIFF_GRM] Using shared embedding dot-product output layer
[TRAINING] Evaluation config: start from epoch 20, interval: 2
Training - [Epoch 1]:   0%|          | 0/86 [00:00<?, ?it/s]Training - [Epoch 1]:   1%|          | 1/86 [00:12<18:00, 12.71s/it]Training - [Epoch 1]:   2%|▏         | 2/86 [00:13<08:04,  5.77s/it]Training - [Epoch 1]:   3%|▎         | 3/86 [00:14<04:54,  3.55s/it]Training - [Epoch 1]:   5%|▍         | 4/86 [00:15<03:25,  2.50s/it]Training - [Epoch 1]:   6%|▌         | 5/86 [00:16<02:47,  2.07s/it]Training - [Epoch 1]:   7%|▋         | 6/86 [00:17<02:13,  1.67s/it]Training - [Epoch 1]:   8%|▊         | 7/86 [00:18<01:52,  1.42s/it]Training - [Epoch 1]:   9%|▉         | 8/86 [00:19<01:37,  1.25s/it]Training - [Epoch 1]:  10%|█         | 9/86 [00:20<01:28,  1.14s/it]Training - [Epoch 1]:  12%|█▏        | 10/86 [00:21<01:21,  1.07s/it]Training - [Epoch 1]:  13%|█▎        | 11/86 [00:22<01:16,  1.02s/it]Training - [Epoch 1]:  14%|█▍        | 12/86 [00:23<01:12,  1.02it/s]Training - [Epoch 1]:  15%|█▌        | 13/86 [00:23<01:10,  1.04it/s]Training - [Epoch 1]:  16%|█▋        | 14/86 [00:24<01:07,  1.06it/s]Training - [Epoch 1]:  17%|█▋        | 15/86 [00:25<01:06,  1.07it/s]Training - [Epoch 1]:  19%|█▊        | 16/86 [00:26<01:04,  1.08it/s]Training - [Epoch 1]:  20%|█▉        | 17/86 [00:27<01:03,  1.09it/s]Training - [Epoch 1]:  21%|██        | 18/86 [00:28<01:02,  1.09it/s]Training - [Epoch 1]:  22%|██▏       | 19/86 [00:29<01:01,  1.10it/s]Training - [Epoch 1]:  23%|██▎       | 20/86 [00:30<00:59,  1.10it/s]Training - [Epoch 1]:  24%|██▍       | 21/86 [00:31<00:58,  1.10it/s]Training - [Epoch 1]:  26%|██▌       | 22/86 [00:32<00:57,  1.10it/s]Training - [Epoch 1]:  27%|██▋       | 23/86 [00:32<00:57,  1.10it/s]Training - [Epoch 1]:  28%|██▊       | 24/86 [00:33<00:56,  1.10it/s]Training - [Epoch 1]:  29%|██▉       | 25/86 [00:34<00:55,  1.10it/s]Training - [Epoch 1]:  30%|███       | 26/86 [00:35<00:54,  1.10it/s]Training - [Epoch 1]:  31%|███▏      | 27/86 [00:36<00:53,  1.10it/s]Training - [Epoch 1]:  33%|███▎      | 28/86 [00:37<00:52,  1.10it/s]Training - [Epoch 1]:  34%|███▎      | 29/86 [00:38<00:51,  1.10it/s]Training - [Epoch 1]:  35%|███▍      | 30/86 [00:39<00:50,  1.10it/s]Training - [Epoch 1]:  36%|███▌      | 31/86 [00:40<00:49,  1.11it/s]Training - [Epoch 1]:  37%|███▋      | 32/86 [00:41<00:48,  1.11it/s]Training - [Epoch 1]:  38%|███▊      | 33/86 [00:42<00:48,  1.10it/s]Training - [Epoch 1]:  40%|███▉      | 34/86 [00:42<00:47,  1.10it/s]Training - [Epoch 1]:  41%|████      | 35/86 [00:43<00:46,  1.11it/s]Training - [Epoch 1]:  42%|████▏     | 36/86 [00:44<00:45,  1.11it/s]Training - [Epoch 1]:  43%|████▎     | 37/86 [00:45<00:44,  1.10it/s]Training - [Epoch 1]:  44%|████▍     | 38/86 [00:46<00:43,  1.11it/s]Training - [Epoch 1]:  45%|████▌     | 39/86 [00:47<00:42,  1.11it/s]Training - [Epoch 1]:  47%|████▋     | 40/86 [00:48<00:41,  1.11it/s]Training - [Epoch 1]:  48%|████▊     | 41/86 [00:49<00:42,  1.05it/s]Training - [Epoch 1]:  49%|████▉     | 42/86 [00:50<00:41,  1.07it/s]Training - [Epoch 1]:  50%|█████     | 43/86 [00:51<00:39,  1.08it/s]Training - [Epoch 1]:  51%|█████     | 44/86 [00:52<00:38,  1.09it/s]Training - [Epoch 1]:  52%|█████▏    | 45/86 [00:53<00:37,  1.10it/s]Training - [Epoch 1]:  53%|█████▎    | 46/86 [00:53<00:36,  1.10it/s]Training - [Epoch 1]:  55%|█████▍    | 47/86 [00:54<00:35,  1.10it/s]Training - [Epoch 1]:  56%|█████▌    | 48/86 [00:55<00:34,  1.10it/s]Training - [Epoch 1]:  57%|█████▋    | 49/86 [00:56<00:33,  1.11it/s]Training - [Epoch 1]:  58%|█████▊    | 50/86 [00:57<00:32,  1.11it/s]Training - [Epoch 1]:  59%|█████▉    | 51/86 [00:58<00:31,  1.11it/s]Training - [Epoch 1]:  60%|██████    | 52/86 [00:59<00:30,  1.11it/s]Training - [Epoch 1]:  62%|██████▏   | 53/86 [01:00<00:29,  1.11it/s]Training - [Epoch 1]:  63%|██████▎   | 54/86 [01:01<00:28,  1.11it/s]Training - [Epoch 1]:  64%|██████▍   | 55/86 [01:02<00:27,  1.11it/s]Training - [Epoch 1]:  65%|██████▌   | 56/86 [01:02<00:27,  1.11it/s]Training - [Epoch 1]:  66%|██████▋   | 57/86 [01:03<00:26,  1.11it/s]Training - [Epoch 1]:  67%|██████▋   | 58/86 [01:04<00:25,  1.11it/s]Training - [Epoch 1]:  69%|██████▊   | 59/86 [01:05<00:24,  1.11it/s]Training - [Epoch 1]:  70%|██████▉   | 60/86 [01:06<00:23,  1.11it/s]Training - [Epoch 1]:  71%|███████   | 61/86 [01:07<00:22,  1.11it/s]Training - [Epoch 1]:  72%|███████▏  | 62/86 [01:08<00:21,  1.11it/s]Training - [Epoch 1]:  73%|███████▎  | 63/86 [01:09<00:20,  1.11it/s]Training - [Epoch 1]:  74%|███████▍  | 64/86 [01:10<00:19,  1.11it/s]Training - [Epoch 1]:  76%|███████▌  | 65/86 [01:11<00:18,  1.11it/s]Training - [Epoch 1]:  77%|███████▋  | 66/86 [01:11<00:18,  1.11it/s]Training - [Epoch 1]:  78%|███████▊  | 67/86 [01:12<00:17,  1.11it/s]Training - [Epoch 1]:  79%|███████▉  | 68/86 [01:13<00:16,  1.11it/s]Training - [Epoch 1]:  80%|████████  | 69/86 [01:14<00:15,  1.11it/s]Training - [Epoch 1]:  81%|████████▏ | 70/86 [01:15<00:14,  1.11it/s]Training - [Epoch 1]:  83%|████████▎ | 71/86 [01:16<00:13,  1.11it/s]Training - [Epoch 1]:  84%|████████▎ | 72/86 [01:17<00:12,  1.11it/s]Training - [Epoch 1]:  85%|████████▍ | 73/86 [01:18<00:11,  1.11it/s]Training - [Epoch 1]:  86%|████████▌ | 74/86 [01:19<00:10,  1.11it/s]Training - [Epoch 1]:  87%|████████▋ | 75/86 [01:20<00:09,  1.11it/s]Training - [Epoch 1]:  88%|████████▊ | 76/86 [01:20<00:09,  1.11it/s]Training - [Epoch 1]:  90%|████████▉ | 77/86 [01:21<00:08,  1.11it/s]Training - [Epoch 1]:  91%|█████████ | 78/86 [01:22<00:07,  1.11it/s]Training - [Epoch 1]:  92%|█████████▏| 79/86 [01:23<00:06,  1.11it/s]Training - [Epoch 1]:  93%|█████████▎| 80/86 [01:24<00:05,  1.11it/s]Training - [Epoch 1]:  94%|█████████▍| 81/86 [01:25<00:04,  1.11it/s]Training - [Epoch 1]:  95%|█████████▌| 82/86 [01:26<00:03,  1.11it/s]Training - [Epoch 1]:  97%|█████████▋| 83/86 [01:27<00:02,  1.11it/s]Training - [Epoch 1]:  98%|█████████▊| 84/86 [01:28<00:01,  1.11it/s]Training - [Epoch 1]:  99%|█████████▉| 85/86 [01:28<00:00,  1.34it/s]Training - [Epoch 1]: 100%|██████████| 86/86 [01:28<00:00,  1.03s/it]
[DIFF_GRM] Using RPG_ED-style encoder: MLP compression + fixed 50-length sequence
[DIFF_GRM] vocab_size: 1027, codebook_size: 256
[DIFF_GRM] masking_strategy: sequential
[Epoch 1] Train Loss: 5.500631
Training - [Epoch 2]:   0%|          | 0/86 [00:00<?, ?it/s]Training - [Epoch 2]:   1%|          | 1/86 [00:01<02:07,  1.50s/it]Training - [Epoch 2]:   2%|▏         | 2/86 [00:02<01:36,  1.15s/it]Training - [Epoch 2]:   3%|▎         | 3/86 [00:03<01:26,  1.04s/it]Training - [Epoch 2]:   5%|▍         | 4/86 [00:04<01:20,  1.02it/s]Training - [Epoch 2]:   6%|▌         | 5/86 [00:05<01:17,  1.05it/s]Training - [Epoch 2]:   7%|▋         | 6/86 [00:06<01:14,  1.07it/s]Training - [Epoch 2]:   8%|▊         | 7/86 [00:06<01:12,  1.09it/s]Training - [Epoch 2]:   9%|▉         | 8/86 [00:07<01:11,  1.09it/s]Training - [Epoch 2]:  10%|█         | 9/86 [00:08<01:10,  1.10it/s]Training - [Epoch 2]:  12%|█▏        | 10/86 [00:09<01:09,  1.10it/s]Training - [Epoch 2]:  13%|█▎        | 11/86 [00:10<01:07,  1.10it/s]Training - [Epoch 2]:  14%|█▍        | 12/86 [00:11<01:06,  1.11it/s]Training - [Epoch 2]:  15%|█▌        | 13/86 [00:12<01:06,  1.11it/s]Training - [Epoch 2]:  16%|█▋        | 14/86 [00:13<01:05,  1.11it/s]Training - [Epoch 2]:  17%|█▋        | 15/86 [00:14<01:03,  1.11it/s]Training - [Epoch 2]:  19%|█▊        | 16/86 [00:15<01:02,  1.11it/s]Training - [Epoch 2]:  20%|█▉        | 17/86 [00:15<01:01,  1.11it/s]Training - [Epoch 2]:  21%|██        | 18/86 [00:16<01:01,  1.11it/s]Training - [Epoch 2]:  22%|██▏       | 19/86 [00:17<01:00,  1.11it/s]Training - [Epoch 2]:  23%|██▎       | 20/86 [00:18<00:59,  1.11it/s]Training - [Epoch 2]:  24%|██▍       | 21/86 [00:19<00:58,  1.11it/s]Training - [Epoch 2]:  26%|██▌       | 22/86 [00:20<00:57,  1.11it/s]