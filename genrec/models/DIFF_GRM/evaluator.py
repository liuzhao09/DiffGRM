# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import numpy as np


class DIFF_GRMEvaluator:
    """
    DIFF_GRMæ¨¡å‹çš„è¯„ä¼°å™¨
    """
    def __init__(self, config, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.metric2func = {
            'recall': self.recall_at_k,
            'ndcg': self.ndcg_at_k
        }

        self.pad_token = self.tokenizer.pad_token
        self.maxk = max(config['topk'])
        
        # ğŸš€ æ–°å¢ï¼šæ€»ä½“ç»Ÿè®¡ç´¯åŠ å™¨
        self.total_seqs = 0
        self.total_legals = 0
        self.total_unique = 0
        
        # ğŸš€ æ–°å¢ï¼šSIDç»„åˆç»Ÿè®¡ç´¯åŠ å™¨ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
        self.batch_legal_ratios = []
        self.batch_duplicate_ratios = []
        self.batch_dup10_ratios = []  # æ–°å¢ï¼šç”¨æˆ·å†…éƒ¨top-10é‡å¤ç‡
        
        # è°ƒè¯•ä¿¡æ¯ï¼šç¡®è®¤ä½¿ç”¨äº†DIFF_GRMEvaluator
        print(f'>> Using evaluator = {self.__class__.__name__} (fixed duplicate scoring bug)')
        print(f'>> Recall: uses any() to avoid duplicate scoring')
        print(f'>> NDCG: uses first-hit-only to avoid duplicate DCG accumulation')
        print(f'>> Fixed: index bounds checking to prevent out-of-bounds errors')
        print(f'>> Added: illegal sequence filtering for more accurate evaluation')



    def calculate_pos_index(self, preds, labels):
        """
        è®¡ç®—é¢„æµ‹ç»“æœä¸çœŸæ ‡ç­¾çš„åŒ¹é…æƒ…å†µï¼ˆbeam searchå·²ä¿è¯åˆæ³•æ€§ï¼‰
        
        Args:
            preds: (batch_size, maxk, n_digit) - ç”Ÿæˆçš„SIDåºåˆ—ï¼ˆå·²è¿‡æ»¤ä¸ºåˆæ³•ï¼‰
            labels: (batch_size, n_digit) - çœŸæ ‡ç­¾åºåˆ—
        
        Returns:
            pos_index: (batch_size, maxk) - æ¯ä¸ªä½ç½®æ˜¯å¦åŒ¹é…çœŸæ ‡ç­¾
        """
        preds = preds.detach().cpu()
        labels = labels.detach().cpu()
        
        B, maxk, n_digit = preds.shape
        
        # ğŸš€ ç®€åŒ–ï¼šbeam searchå·²ä¿è¯è¿”å›çš„åºåˆ—éƒ½æ˜¯åˆæ³•çš„
        # ç›´æ¥è®¡ç®—åŒ¹é…æƒ…å†µï¼Œæ— éœ€å†è¿‡æ»¤
        pos_index = torch.zeros((B, maxk), dtype=torch.bool)
        
        for i in range(B):
            # è·å–çœŸæ ‡ç­¾
            cur_label = labels[i].tolist()  # [n_digit]
            
            for j in range(maxk):
                # è·å–é¢„æµ‹åºåˆ—
                cur_pred = preds[i, j].tolist()  # [n_digit]
                
                # æ¯”è¾ƒcodebook IDsï¼ˆ0-255èŒƒå›´ï¼‰
                if cur_pred == cur_label:
                    pos_index[i, j] = True
                    break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤è®¡åˆ†
        
        return pos_index

    def recall_at_k(self, pos_index, k):
        """è®¡ç®—Recall@kï¼ˆä¿®å¤ï¼šé‡å¤å‘½ä¸­åªè®¡1åˆ†ï¼‰"""
        # pos_index: (batch_size, maxk) - å·²ç»è¿‡æ»¤ä¸ºåˆæ³•åºåˆ—
        # ä¿®å¤ï¼šä½¿ç”¨any()é¿å…é‡å¤è®¡åˆ†ï¼Œåªè¦top-kä¸­æœ‰â‰¥1ä¸ªåŒ¹é…å°±å¾—1åˆ†
        return pos_index[:, :k].any(dim=1).cpu().float()

    def ndcg_at_k(self, pos_index, k):
        """è®¡ç®—NDCG@kï¼ˆä¿®å¤ï¼šé‡å¤å‘½ä¸­åªè®¡ç¬¬ä¸€æ¬¡çš„DCGï¼‰"""
        # pos_index: (batch_size, maxk) - å·²ç»è¿‡æ»¤ä¸ºåˆæ³•åºåˆ—
        batch_size, maxk = pos_index.shape
        device = pos_index.device
        
        # åˆ›å»ºrankæƒé‡ï¼š1/log2(rank+1)
        ranks = torch.arange(1, maxk + 1, device=device).float()
        dcg_weights = 1.0 / torch.log2(ranks + 1)
        
        # ä¿®å¤ï¼šåªè®¡ç®—ç¬¬ä¸€ä¸ªåŒ¹é…ä½ç½®çš„DCGï¼Œé¿å…é‡å¤è®¡åˆ†ï¼ˆå‘é‡åŒ–å®ç°ï¼‰
        # åˆ›å»ºä½ç½®ç´¢å¼•çŸ©é˜µ
        position_matrix = torch.arange(maxk, device=device).expand(batch_size, -1)
        
        # æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªåŒ¹é…çš„ä½ç½®ï¼ˆå‘é‡åŒ–ï¼‰
        # å°†éåŒ¹é…ä½ç½®è®¾ä¸ºä¸€ä¸ªå¾ˆå¤§çš„æ•°ï¼Œè¿™æ ·minæ“ä½œä¼šå¿½ç•¥å®ƒä»¬
        masked_positions = torch.where(pos_index, position_matrix, torch.full_like(position_matrix, maxk))
        first_hit_positions = masked_positions.min(dim=1).values  # [batch_size]
        
        # è®¡ç®—DCGï¼šåªæœ‰åœ¨top-kå†…ä¸”æœ‰åŒ¹é…æ—¶æ‰å¾—åˆ†
        # ä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼šç¡®ä¿onlyçœŸæ­£æœ‰åŒ¹é…ä¸”åœ¨top-kå†…çš„æ ·æœ¬æ‰è®¡åˆ†
        has_hit = first_hit_positions < maxk  # æ˜¯å¦æœ‰åŒ¹é…
        in_topk = first_hit_positions < k  # åŒ¹é…æ˜¯å¦åœ¨top-kå†…
        valid_mask = has_hit & in_topk  # æ—¢æœ‰åŒ¹é…åˆåœ¨top-kå†…
        
        # å®‰å…¨çš„ç´¢å¼•è®¿é—®ï¼šå…ˆé™åˆ¶ç´¢å¼•èŒƒå›´ï¼Œå†è®¡ç®—å¾—åˆ†
        safe_positions = torch.clamp(first_hit_positions, 0, maxk - 1)
        dcg_scores = torch.where(valid_mask, dcg_weights[safe_positions], torch.tensor(0.0, device=device))
        
        # å¯¹äºå•æ ‡ç­¾æ¨èä»»åŠ¡ï¼ŒIDCG=1.0ï¼Œæ‰€ä»¥DCGå°±æ˜¯NDCG
        return dcg_scores.cpu().float()

    def _dup_ratio_per_user(self, preds, k=10):
        """
        è®¡ç®—ä¸€ä¸ª batch å†…"ç”¨æˆ·å†…éƒ¨"çš„é‡å¤ç‡ã€‚
        preds: [B, maxk, n_digit]ï¼ˆå·²ä¿è¯ maxk â‰¥ kï¼‰
        è¿”å›:  [B] æ¯ä¸ªç”¨æˆ·è‡ªå·±çš„é‡å¤ç‡
        """
        B, _, n_digit = preds.shape
        dup_ratios = []

        for b in range(B):
            # ä»…å–å‰ k æ¡
            seqs = preds[b, :k]                       # [k, n_digit]
            k_seqs = [tuple(s.tolist()) for s in seqs]
            unique_cnt = len(set(k_seqs))
            dup_ratios.append(1 - unique_cnt / k)     # in [0,1]

        return torch.tensor(dup_ratios, dtype=torch.float32)

    def calculate_weighted_score(self, preds, labels):
        """
        è®¡ç®—åŠ æƒç»¼åˆåˆ†æ•°ï¼šNDCG@10 * 0.6 + RECALL@10 * 0.4
        
        Args:
            preds: (batch_size, beam_size, n_digit) - ç”Ÿæˆçš„SIDåºåˆ—
            labels: (batch_size, n_digit) - çœŸæ ‡ç­¾åºåˆ—
        
        Returns:
            weighted_score: åŠ æƒç»¼åˆåˆ†æ•°
        """
        pos_index = self.calculate_pos_index(preds, labels)
        
        # è®¡ç®—NDCG@10
        ndcg_10 = self.ndcg_at_k(pos_index, k=10)
        
        # è®¡ç®—RECALL@10
        recall_10 = self.recall_at_k(pos_index, k=10)
        
        # è®¡ç®—åŠ æƒåˆ†æ•°ï¼šNDCG@10 * 0.6 + RECALL@10 * 0.4
        weighted_score = 0.8 * ndcg_10 + 0.2 * recall_10
        
        return weighted_score

    def calculate_metrics(self, preds, labels, suffix=""):
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        results = {}
        pos_index = self.calculate_pos_index(preds, labels)
        
        # ğŸš€ æ›´æ–°æ€»ä½“ç»Ÿè®¡
        B, maxk, n_digit = preds.shape
        self.total_seqs += preds.numel() // n_digit
        self.total_legals += sum(
            self.tokenizer.codebooks_to_item_id(seq.tolist()) is not None
            for seq in preds.view(-1, n_digit)
        )
        self.total_unique += len({
            tuple(seq.tolist()) for seq in preds.view(-1, n_digit)
        })
        
        # ğŸš€ è®¡ç®—å½“å‰batchçš„SIDç»„åˆç»Ÿè®¡
        # ä¿®å¤åˆæ³•ç‡è®¡ç®—ï¼šä½¿ç”¨åºåˆ—æ•°ä½œä¸ºåˆ†æ¯ï¼Œè€Œä¸æ˜¯tokenæ•°
        total_seqs = preds.numel() // n_digit
        current_legal_ratio = sum(
            self.tokenizer.codebooks_to_item_id(seq.tolist()) is not None
            for seq in preds.view(-1, n_digit)
        ) / total_seqs
        
        current_duplicate_ratio = 1 - len({
            tuple(seq.tolist()) for seq in preds.view(-1, n_digit)
        }) / total_seqs
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯ç”¨äºè®¡ç®—å¹³å‡å€¼
        self.batch_legal_ratios.append(current_legal_ratio)
        self.batch_duplicate_ratios.append(current_duplicate_ratio)
        
        # ---------- è®¡ç®—"ç”¨æˆ·å†…éƒ¨"Top-10 é‡å¤ç‡ ----------
        dup10 = self._dup_ratio_per_user(preds, k=10)     # [B]
        results[f'dup@10{suffix}'] = dup10                         # ä¼šè¢«å¹³å‡åå†™å…¥ final_results
        
        # é¡ºä¾¿ç´¯è®¡åˆ° batch ç»Ÿè®¡ï¼ˆæƒ³çœ‹å…¨å±€å¹³å‡ï¼‰
        self.batch_dup10_ratios.append(dup10.mean().item())
        
        for metric in self.config['metrics']:
            for k in self.config['topk']:
                results[f"{metric}@{k}{suffix}"] = self.metric2func[metric](pos_index, k)
        
        # æ·»åŠ åŠ æƒç»¼åˆåˆ†æ•°ï¼ˆåªåœ¨confidenceæ¨¡å¼ä¸‹è®¡ç®—ï¼‰
        if suffix == "":  # ä»…confidenceæ¨¡å¼æ‰ç®—weighted_score
            weighted_score = self.calculate_weighted_score(preds, labels)
            results['weighted_score'] = weighted_score
        
        return results
    
    def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ"""
        if self.total_seqs > 0:
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            legal_ratio = self.total_legals / self.total_seqs
            # ä¿®å¤é‡å¤ç‡è®¡ç®—ï¼šä½¿ç”¨æ­£ç¡®çš„å…¬å¼
            duplicate_ratio = 1 - self.total_unique / self.total_seqs
            
            # è®¡ç®—batchå¹³å‡å€¼ï¼ˆæ›´å‡†ç¡®ï¼‰
            if self.batch_legal_ratios:
                avg_legal_ratio = sum(self.batch_legal_ratios) / len(self.batch_legal_ratios)
                avg_duplicate_ratio = sum(self.batch_duplicate_ratios) / len(self.batch_duplicate_ratios)
                
                print(f"[SID_STATS] å¹³å‡åˆæ³•ç‡: {avg_legal_ratio:.3f}, å¹³å‡é‡å¤ç‡: {avg_duplicate_ratio:.3f}")
                
                # æ–°å¢ï¼šç”¨æˆ·å†…éƒ¨top-10é‡å¤ç‡ç»Ÿè®¡
                if self.batch_dup10_ratios:
                    avg_dup10 = sum(self.batch_dup10_ratios) / len(self.batch_dup10_ratios)
                    print(f"[SID_STATS] ç”¨æˆ·å†…éƒ¨ Top-10 å¹³å‡é‡å¤ç‡: {avg_dup10:.3f}")
            else:
                print(f"[SID_STATS] æ€»ä½“åˆæ³•ç‡: {legal_ratio:.3f}, æ€»ä½“é‡å¤ç‡: {duplicate_ratio:.3f}") 