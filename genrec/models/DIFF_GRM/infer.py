#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DIFF_GRM Offline Inference Script

ä½¿ç”¨å·²è®­ç»ƒçš„checkpointè¿›è¡Œæ¨ç†ï¼Œæ”¯æŒï¼š
- è‡ªå®šä¹‰beam searchå‚æ•°
- é‡å¤åºåˆ—æ£€æµ‹
- è¯¦ç»†çš„ä¸­é—´ç»“æœè¾“å‡º
- è¯„ä¼°æŒ‡æ ‡è®¡ç®—
"""

import argparse
import os
import sys
import yaml
import torch
import numpy as np
from tqdm import tqdm
from collections import Counter
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°pathï¼ˆå› ä¸ºæ­¤è„šæœ¬ä½äºgenrec/models/DIFF_GRM/ï¼‰
PROJ_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
sys.path.insert(0, PROJ_ROOT)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from genrec.utils import get_dataset, get_tokenizer, get_model

# å¯¼å…¥æœ¬æ¨¡å—çš„ç»„ä»¶
from genrec.models.DIFF_GRM.model import DIFF_GRM
from genrec.models.DIFF_GRM.tokenizer import DIFF_GRMTokenizer
from genrec.models.DIFF_GRM.evaluator import DIFF_GRMEvaluator
from genrec.models.DIFF_GRM.collate import collate_fn_test
from genrec.models.DIFF_GRM.beam import fast_beam_search_for_eval


def get_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="DIFF_GRM Inference")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--config", default="genrec/models/DIFF_GRM/config_infer.yaml",
                       help="æ¨ç†é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", required=True,
                       help="æ¨¡å‹checkpointè·¯å¾„ (*.bin)")
    parser.add_argument("--category", default=None,
                       help="æ•°æ®é›†categoryï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨configä¸­çš„è®¾ç½®")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--split", default="test", choices=["val", "test"],
                       help="è¯„ä¼°æ•°æ®é›†split")
    parser.add_argument("--batch_size", type=int, default=None,
                       help="æ¨ç†batch sizeï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨configä¸­çš„eval_batch_size")
    
    # æ¨¡å‹æ¶æ„å‚æ•°ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    parser.add_argument("--n_digit", type=int, default=None,
                       help="SIDä½æ•°ï¼Œå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´")
    parser.add_argument("--encoder_n_layer", type=int, default=None,
                       help="Encoderå±‚æ•°")
    parser.add_argument("--decoder_n_layer", type=int, default=None,
                       help="Decoderå±‚æ•°")
    parser.add_argument("--n_head", type=int, default=None,
                       help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--n_embd", type=int, default=None,
                       help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--n_inner", type=int, default=None,
                       help="å‰é¦ˆç½‘ç»œå†…éƒ¨ç»´åº¦")
    parser.add_argument("--codebook_size", type=int, default=None,
                       help="Codebookå¤§å°")
    
    # Beam Searchå‚æ•°
    parser.add_argument("--top_k", type=int, default=None,
                       help="æœ€ç»ˆè¿”å›çš„åºåˆ—æ•°é‡ï¼Œè¦†ç›–configä¸­çš„top_k_final")
    parser.add_argument("--beam_act", type=int, default=None,
                       help="å‰å‡ æ­¥çš„å›ºå®šbeamæ•°é‡")
    parser.add_argument("--beam_max", type=int, default=None,
                       help="æœ€å¤§beamæ•°é‡")
    parser.add_argument("--dedup_strategy", choices=["none", "simple", "weighted"], default="simple",
                       help="å»é‡ç­–ç•¥: none(ä¸å»é‡), simple(ç®€å•å»é‡), weighted(æ¦‚ç‡åŠ æƒå»é‡)")
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument("--print_duplicates", action="store_true",
                       help="æ‰“å°é‡å¤åºåˆ—ç»Ÿè®¡")
    parser.add_argument("--print_examples", type=int, default=5,
                       help="æ‰“å°å‰Nä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ")
    parser.add_argument("--save_results", action="store_true",
                       help="ä¿å­˜æ¨ç†ç»“æœåˆ°æ–‡ä»¶")
    parser.add_argument("--output_dir", default=None,
                       help="è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸checkpointåŒç›®å½•")
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument("--debug", action="store_true",
                       help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--max_samples", type=int, default=None,
                       help="é™åˆ¶æ¨ç†æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    return parser.parse_args()


def load_config(config_path, args):
    """åŠ è½½å¹¶è°ƒæ•´é…ç½®"""
    print(f"Loading config from: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.category:
        config['category'] = args.category
        print(f"Override category: {args.category}")
    
    if args.batch_size:
        config['eval_batch_size'] = args.batch_size
        print(f"Override batch_size: {args.batch_size}")
    
    # æ¨¡å‹æ¶æ„å‚æ•°è¦†ç›–ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    if args.n_digit:
        config['n_digit'] = args.n_digit
        print(f"Override n_digit: {args.n_digit}")
    
    if args.encoder_n_layer:
        config['encoder_n_layer'] = args.encoder_n_layer
        print(f"Override encoder_n_layer: {args.encoder_n_layer}")
        
    if args.decoder_n_layer:
        config['decoder_n_layer'] = args.decoder_n_layer
        print(f"Override decoder_n_layer: {args.decoder_n_layer}")
        
    if args.n_head:
        config['n_head'] = args.n_head
        print(f"Override n_head: {args.n_head}")
        
    if args.n_embd:
        config['n_embd'] = args.n_embd
        print(f"Override n_embd: {args.n_embd}")
        
    if args.n_inner:
        config['n_inner'] = args.n_inner
        print(f"Override n_inner: {args.n_inner}")
        
    if args.codebook_size:
        config['codebook_size'] = args.codebook_size
        print(f"Override codebook_size: {args.codebook_size}")
    
    # Beam Searchå‚æ•°è¦†ç›–
    if 'vectorized_beam_search' not in config:
        config['vectorized_beam_search'] = {}
    
    if args.top_k:
        config['vectorized_beam_search']['top_k_final'] = args.top_k
        print(f"Override top_k_final: {args.top_k}")
    
    if args.beam_act:
        config['vectorized_beam_search']['beam_act'] = args.beam_act
        print(f"Override beam_act: {args.beam_act}")
        
    if args.beam_max:
        config['vectorized_beam_search']['beam_max'] = args.beam_max
        print(f"Override beam_max: {args.beam_max}")
    
    # å»é‡ç­–ç•¥å‚æ•°
    config['dedup_strategy'] = args.dedup_strategy
    print(f"Deduplication strategy: {args.dedup_strategy}")
    
    # ç¡®ä¿å¿…è¦çš„è®¾å¤‡é…ç½®
    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'
    config['num_proc'] = 1  # æ¨ç†æ—¶ä½¿ç”¨å•è¿›ç¨‹é¿å…é—®é¢˜
    
    # ç¡®ä¿datasetå­—æ®µå­˜åœ¨ï¼ˆç”¨äºget_datasetå‡½æ•°ï¼‰
    config['dataset'] = 'AmazonReviews2014'
    
    # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´çš„åˆå§‹åŒ–ï¼ˆå‚è€ƒpipeline.pyï¼‰
    from accelerate import Accelerator
    from genrec.utils import init_seed, init_logger
    from logging import getLogger
    
    # åˆ›å»ºacceleratorï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    project_dir = os.path.join(
        config.get('tensorboard_log_dir', 'runs'),
        config.get('dataset', 'AmazonReviews2014'),
        config.get('model', 'DIFF_GRM')
    )
    # æ¨ç†æ—¶ä¸éœ€è¦tensorboardæ—¥å¿—ï¼Œç®€åŒ–acceleratoråˆ›å»º
    try:
        accelerator = Accelerator(log_with='tensorboard', project_dir=project_dir)
    except:
        # å¦‚æœtensorboardä¸å¯ç”¨ï¼Œä½¿ç”¨æ— æ—¥å¿—çš„accelerator
        accelerator = Accelerator()
        print("Warning: Tensorboard not available, using accelerator without logging")
    config['accelerator'] = accelerator
    
    # åˆå§‹åŒ–ç§å­å’Œæ—¥å¿—ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    init_seed(config.get('rand_seed', config.get('seed', 42)), 
              config.get('reproducibility', True))
    init_logger(config)
    logger = getLogger()
    
    print(f"Final config - Category: {config.get('category')}, "
          f"Dataset: {config.get('dataset')}, "
          f"Batch size: {config.get('eval_batch_size')}, "
          f"Device: {config['device']}")
    
    return config


def setup_dataset_and_tokenizer(config):
    """è®¾ç½®æ•°æ®é›†å’Œtokenizer"""
    print("Setting up dataset and tokenizer...")
    
    # è·å–æ•°æ®é›† - éœ€è¦å…ˆè·å–ç±»ï¼Œç„¶åå®ä¾‹åŒ–
    dataset_name = config.get('dataset', 'AmazonReviews2014')  # é»˜è®¤ä½¿ç”¨AmazonReviews2014
    dataset_class = get_dataset(dataset_name)
    dataset = dataset_class(config)
    print(f"Dataset: {dataset.__class__.__name__}, Items: {dataset.n_items}")
    
    # è·å–tokenizer - éœ€è¦å…ˆè·å–ç±»ï¼Œç„¶åå®ä¾‹åŒ–
    model_name = config.get('model', 'DIFF_GRM')
    tokenizer_class = get_tokenizer(model_name)
    tokenizer = tokenizer_class(config, dataset)
    print(f"Tokenizer: {tokenizer.__class__.__name__}, Vocab size: {tokenizer.vocab_size}")
    
    return dataset, tokenizer


def setup_model(config, dataset, tokenizer, checkpoint_path):
    """è®¾ç½®æ¨¡å‹å¹¶åŠ è½½checkpoint"""
    print("Setting up model...")
    
    # åˆ›å»ºæ¨¡å‹
    model = DIFF_GRM(config, dataset, tokenizer)
    print(f"Model: {model.__class__.__name__}, Parameters: {model.n_parameters}")
    
    # åŠ è½½checkpoint
    print(f"Loading checkpoint from: {checkpoint_path}")
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # å¤„ç†å¯èƒ½çš„é”®åå·®å¼‚
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict, strict=True)
    print("Checkpoint loaded successfully!")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®è¯„ä¼°æ¨¡å¼
    device = torch.device(config['device'])
    model = model.to(device)
    model.eval()
    
    return model, device


def setup_dataloader(config, dataset, tokenizer, split):
    """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
    print(f"Setting up {split} dataloader...")
    
    # è·å–tokenizedæ•°æ®é›†
    datasets = dataset.split()
    tokenized_datasets = tokenizer.tokenize(datasets)
    
    if split not in tokenized_datasets:
        raise ValueError(f"Split '{split}' not found in dataset")
    
    eval_dataset = tokenized_datasets[split]
    print(f"{split.capitalize()} dataset size before filtering: {len(eval_dataset)}")
    
    # ğŸš€ æ–°å¢ï¼šè¿‡æ»¤æ‰ç©ºå†å²çš„æ ·æœ¬ï¼Œç¡®ä¿å…¬å¹³è¯„ä¼°
    if split in ['val', 'test']:
        def has_valid_history(example):
            history_sid = example['history_sid']
            # æ£€æŸ¥æ˜¯å¦æœ‰éPADçš„å†å²
            if isinstance(history_sid, list):
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å†å²éƒ½æ˜¯PAD (token=0)
                non_pad_count = 0
                for seq in history_sid:
                    if isinstance(seq, list):
                        if any(token != 0 for token in seq):  # 0æ˜¯PAD token
                            non_pad_count += 1
                return non_pad_count > 0
            return True
        
        # è¿‡æ»¤æ•°æ®é›†
        eval_dataset = eval_dataset.filter(has_valid_history)
        print(f"{split.capitalize()} dataset size after filtering: {len(eval_dataset)}")
        print(f"Filtered out samples with empty history for fair evaluation")
    
    # åˆ›å»ºDataLoader
    from torch.utils.data import DataLoader
    dataloader = DataLoader(
        eval_dataset,
        batch_size=config['eval_batch_size'],
        shuffle=False,
        collate_fn=collate_fn_test,
        num_workers=0,  # æ¨ç†æ—¶ä½¿ç”¨å•è¿›ç¨‹
        pin_memory=True if config['device'] == 'cuda' else False
    )
    
    return dataloader


def analyze_dataset_characteristics(dataloader):
    """åˆ†ææ•°æ®é›†ç‰¹æ€§ï¼Œå¸®åŠ©ç†è§£æ€§èƒ½å·®å¼‚"""
    print("\n" + "="*50)
    print("DATASET CHARACTERISTICS ANALYSIS")
    print("="*50)
    
    history_lengths = []
    empty_history_count = 0
    
    for batch in dataloader:
        if 'history_sid' in batch:
            history_sid = batch['history_sid']  # [batch_size, max_len, n_digit]
            batch_size = history_sid.size(0)
            
            for i in range(batch_size):
                user_history = history_sid[i]  # [max_len, n_digit]
                # è®¡ç®—éPADçš„å†å²é•¿åº¦
                non_pad_seqs = 0
                for seq in user_history:
                    if torch.any(seq != 0):  # 0æ˜¯PAD token
                        non_pad_seqs += 1
                
                history_lengths.append(non_pad_seqs)
                if non_pad_seqs == 0:
                    empty_history_count += 1
    
    if history_lengths:
        import numpy as np
        history_lengths = np.array(history_lengths)
        print(f"Total samples: {len(history_lengths)}")
        print(f"Empty history samples: {empty_history_count} ({empty_history_count/len(history_lengths)*100:.1f}%)")
        print(f"History length stats:")
        print(f"  Mean: {history_lengths.mean():.2f}")
        print(f"  Median: {np.median(history_lengths):.2f}")
        print(f"  Min: {history_lengths.min()}")
        print(f"  Max: {history_lengths.max()}")
        print(f"  Distribution:")
        for length in range(0, min(6, history_lengths.max() + 1)):
            count = np.sum(history_lengths == length)
            print(f"    Length {length}: {count} samples ({count/len(history_lengths)*100:.1f}%)")


def analyze_predictions(predictions, labels, tokenizer, args, config=None):
    """åˆ†æé¢„æµ‹ç»“æœ"""
    print("\n" + "="*50)
    print("PREDICTION ANALYSIS")
    print("="*50)
    
    batch_size, top_k, n_digit = predictions.shape
    print(f"Predictions shape: {predictions.shape}")
    print(f"Labels shape: {labels.shape}")
    
    # 1. é‡å¤åºåˆ—ç»Ÿè®¡
    if args.print_duplicates:
        print("\n--- DUPLICATE ANALYSIS ---")
        total_duplicates = 0
        
        # æ·»åŠ è¯¦ç»†çš„beam searchåˆ†æ
        print("\n--- BEAM SEARCH QUALITY ANALYSIS ---")
        
        for i in range(min(3, batch_size)):  # åˆ†æå‰3ä¸ªæ ·æœ¬
            preds_i = predictions[i]  # [top_k, n_digit]
            label_i = labels[i].tolist()
            
            print(f"\nSample {i} analysis:")
            print(f"  True label: {label_i}")
            
            # è½¬æ¢ä¸ºtupleä»¥ä¾¿å»é‡
            pred_tuples = [tuple(pred.tolist()) for pred in preds_i]
            unique_preds = set(pred_tuples)
            duplicates = len(pred_tuples) - len(unique_preds)
            total_duplicates += duplicates
            
            # ç»Ÿè®¡æ¯ä¸ªåºåˆ—çš„å‡ºç°æ¬¡æ•°å’Œä½ç½®
            counter = Counter(pred_tuples) 
            unique_sequences = list(counter.keys())
            
            print(f"  Total predictions: {len(pred_tuples)}")
            print(f"  Unique sequences: {len(unique_sequences)}")
            print(f"  Duplicates: {duplicates}")
            
            # æ˜¾ç¤ºæ¯ä¸ªå”¯ä¸€åºåˆ—çš„è¯¦ç»†ä¿¡æ¯
            print("  Sequence distribution:")
            for j, (seq, count) in enumerate(counter.most_common()):
                is_correct = list(seq) == label_i
                marker = "âœ“" if is_correct else " "
                first_pos = pred_tuples.index(seq) + 1  # ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®
                print(f"    {j+1:2d}: {list(seq)} (appears {count}x, first@{first_pos}) {marker}")
                if j >= 9:  # åªæ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€åºåˆ—
                    break
        
        # ç»§ç»­ç»Ÿè®¡å‰©ä½™æ ·æœ¬
        for i in range(3, batch_size):
            preds_i = predictions[i]
            pred_tuples = [tuple(pred.tolist()) for pred in preds_i]
            unique_preds = set(pred_tuples)
            duplicates = len(pred_tuples) - len(unique_preds)
            total_duplicates += duplicates
        
        print(f"\nOverall statistics:")
        print(f"Total duplicates across all samples: {total_duplicates}")
        print(f"Average duplicates per sample: {total_duplicates / batch_size:.2f}")
        
        # æ ¹æ®å»é‡ç­–ç•¥ç»™å‡ºè§£é‡Š
        dedup_strategy = config.get('dedup_strategy', 'weighted') if config else 'weighted'
        if dedup_strategy == "none":
            print("Note: No deduplication applied - duplicates are expected")
        elif dedup_strategy == "simple":
            print("Note: Simple deduplication - duplicates removed by first occurrence")
        else:  # weighted
            print("Note: Probability-weighted deduplication - duplicate probabilities aggregated")
    
    # 2. æ‰“å°è¯¦ç»†ç¤ºä¾‹
    if args.print_examples > 0:
        print(f"\n--- DETAILED EXAMPLES (first {args.print_examples}) ---")
        n_examples = min(args.print_examples, batch_size)
        
        for i in range(n_examples):
            print(f"\nSample {i}:")
            print(f"  True label: {labels[i].tolist()}")
            print(f"  Predictions (top-{top_k}):")
            
            for j in range(top_k):
                pred = predictions[i, j].tolist()
                is_correct = pred == labels[i].tolist()
                marker = "âœ“" if is_correct else " "
                print(f"    {j:2d}: {pred} {marker}")
    
    # 3. åºåˆ—åˆ†å¸ƒç»Ÿè®¡
    print(f"\n--- SEQUENCE STATISTICS ---")
    all_predictions = predictions.view(-1, n_digit)  # [batch_size * top_k, n_digit]
    
    # ç»Ÿè®¡æ¯ä¸ªä½ç½®çš„åˆ†å¸ƒ
    for digit in range(n_digit):
        digit_values = all_predictions[:, digit]
        unique_values = torch.unique(digit_values)
        print(f"Digit {digit}: {len(unique_values)} unique values "
              f"(range: {digit_values.min().item()}-{digit_values.max().item()})")
    
    return total_duplicates


def run_inference(model, dataloader, device, tokenizer, args):
    """è¿è¡Œæ¨ç†"""
    print("\n" + "="*50)
    print("RUNNING INFERENCE")
    print("="*50)
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    total_batches = len(dataloader)
    if args.max_samples:
        max_batches = (args.max_samples + dataloader.batch_size - 1) // dataloader.batch_size
        total_batches = min(total_batches, max_batches)
        print(f"Limiting to {total_batches} batches ({args.max_samples} samples)")
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Inference", total=total_batches)
        for batch_idx, batch in enumerate(pbar):
            if args.max_samples and batch_idx >= total_batches:
                break
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            for key in batch:
                if torch.is_tensor(batch[key]):
                    batch[key] = batch[key].to(device)
            
            # è·å–encoderè¾“å‡º
            encoder_outputs = model.forward(batch, return_loss=False)
            encoder_hidden = encoder_outputs.hidden_states
            
            # ğŸš€ è®¾ç½®å½“å‰splitä¸ºtestï¼Œä½¿ç”¨è¾ƒå¤§çš„beam size
            model.config["current_split"] = "test"
            
            # ä½¿ç”¨beam searchç”Ÿæˆåºåˆ—
            predictions = fast_beam_search_for_eval(
                model=model,
                encoder_hidden=encoder_hidden,
                beam_size=10,  # è¿™ä¸ªå‚æ•°ä¼šè¢«configä¸­çš„top_k_finalè¦†ç›–
                max_len=model.n_digit,
                tokenizer=tokenizer
            )
            
            # æ”¶é›†ç»“æœ
            all_predictions.append(predictions.cpu())
            all_labels.append(batch['labels'].cpu())
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'batch': f"{batch_idx+1}/{total_batches}",
                'samples': f"{len(all_predictions) * dataloader.batch_size}"
            })
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_predictions = torch.cat(all_predictions, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    
    print(f"\nInference completed!")
    print(f"Total samples: {len(all_predictions)}")
    print(f"Predictions shape: {all_predictions.shape}")
    print(f"Labels shape: {all_labels.shape}")
    
    return all_predictions, all_labels


def evaluate_results(predictions, labels, config, tokenizer):
    """è¯„ä¼°ç»“æœ"""
    print("\n" + "="*50)
    print("EVALUATING RESULTS")
    print("="*50)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = DIFF_GRMEvaluator(config, tokenizer)
    
    # è®¡ç®—æŒ‡æ ‡
    results = evaluator.calculate_metrics(predictions, labels)
    
    # æ‰“å°ç»“æœ
    print("Evaluation Results:")
    for metric_name, scores in results.items():
        if metric_name == 'weighted_score':
            avg_score = torch.mean(scores).item()
            print(f"  {metric_name}: {avg_score:.4f}")
        else:
            avg_score = torch.mean(scores).item()
            print(f"  {metric_name}: {avg_score:.4f}")
    
    return results


def save_results(predictions, labels, results, args, config):
    """ä¿å­˜ç»“æœ"""
    if not args.save_results:
        return
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        output_dir = os.path.dirname(args.checkpoint)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = f"inference_{args.split}_{timestamp}"
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    pred_path = os.path.join(output_dir, f"{base_name}_predictions.pt")
    torch.save({
        'predictions': predictions,
        'labels': labels,
        'config': config,
        'args': vars(args)
    }, pred_path)
    print(f"Predictions saved to: {pred_path}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results_path = os.path.join(output_dir, f"{base_name}_results.json")
    results_json = {}
    for key, value in results.items():
        if torch.is_tensor(value):
            results_json[key] = value.mean().item()
        else:
            results_json[key] = value
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            'results': results_json,
            'config': config,
            'args': vars(args),
            'timestamp': timestamp
        }, f, indent=2, ensure_ascii=False)
    print(f"Results saved to: {results_path}")


def main():
    """ä¸»å‡½æ•°"""
    args = get_args()
    
    print("="*60)
    print("DIFF_GRM INFERENCE")
    print("="*60)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Config: {args.config}")
    print(f"Split: {args.split}")
    print(f"Debug mode: {args.debug}")
    
    try:
        # 1. åŠ è½½é…ç½®
        config = load_config(args.config, args)
        
        # 2. è®¾ç½®æ•°æ®é›†å’Œtokenizer
        dataset, tokenizer = setup_dataset_and_tokenizer(config)
        
        # 3. è®¾ç½®æ¨¡å‹
        model, device = setup_model(config, dataset, tokenizer, args.checkpoint)
        
        # 4. è®¾ç½®æ•°æ®åŠ è½½å™¨
        dataloader = setup_dataloader(config, dataset, tokenizer, args.split)
        
        # 4.5. åˆ†ææ•°æ®é›†ç‰¹æ€§ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if args.debug:
            analyze_dataset_characteristics(dataloader)
        
        # 5. è¿è¡Œæ¨ç†
        predictions, labels = run_inference(model, dataloader, device, tokenizer, args)
        
        # 6. åˆ†æé¢„æµ‹ç»“æœ
        total_duplicates = analyze_predictions(predictions, labels, tokenizer, args, config)
        
        # 7. è¯„ä¼°ç»“æœ
        results = evaluate_results(predictions, labels, config, tokenizer)
        
        # 8. ä¿å­˜ç»“æœ
        save_results(predictions, labels, results, args, config)
        
        print("\n" + "="*60)
        print("INFERENCE COMPLETED SUCCESSFULLY!")
        print("="*60)
        
    except Exception as e:
        print(f"\nError occurred during inference: {str(e)}")
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 