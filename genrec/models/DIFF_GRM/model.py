# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from genrec.model import AbstractModel
from genrec.dataset import AbstractDataset
from genrec.tokenizer import AbstractTokenizer


def make_norm(norm_type: str, dim: int, eps: float):
    if (norm_type or "layernorm").lower() == "rmsnorm":
        return nn.RMSNorm(dim, eps=eps)
    return nn.LayerNorm(dim, eps=eps)


class MultiHeadAttention(nn.Module):

    def __init__(self, emb_dim, n_head, attn_drop=0.1, resid_drop=0.1):
        super().__init__()
        assert emb_dim % n_head == 0
        self.n_head = n_head
        self.emb_dim = emb_dim
        self.head_dim = emb_dim // n_head

        # Combined QKV projection for efficiency
        self.qkv = nn.Linear(emb_dim, 3 * emb_dim, bias=False)
        self.proj = nn.Linear(emb_dim, emb_dim)

        self.attn_dropout = nn.Dropout(attn_drop)
        self.resid_dropout = nn.Dropout(resid_drop)

        # Initialize weights
        nn.init.normal_(self.qkv.weight, std=0.02)
        nn.init.normal_(self.proj.weight, std=0.02)

    def forward(self, x, attention_mask=None, key_value=None, past_key_value=None, use_cache=False, is_decoder_self_attn=False):
        B, T, C = x.size()

        if key_value is not None:
            # Cross attention: Q from x, K,V from key_value
            q = self.qkv(x)[:, :, :self.emb_dim]  # Only take Q part
            k, v = key_value.chunk(2, dim=-1)  # key_value should be [B, T_enc, 2*emb_dim]
            T_kv = k.size(1)
        else:
            # Self attention
            q, k, v = self.qkv(x).chunk(3, dim=-1)
            T_kv = T

        # Handle past key-value cache for incremental decoding
        if past_key_value is not None and use_cache:
            past_k, past_v = past_key_value
            k = torch.cat([past_k, k], dim=1)
            v = torch.cat([past_v, v], dim=1)
            T_kv = k.size(1)

        # ä¿å­˜æ‹¼æ¥åçš„å®Œæ•´kå’Œvç”¨äºcacheï¼ˆåœ¨reshapeä¹‹å‰ï¼‰
        k_for_cache = k
        v_for_cache = v

        # Reshape for multi-head attention
        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, n_head, T, head_dim)
        k = k.view(B, T_kv, self.n_head, self.head_dim).transpose(1, 2)  # (B, n_head, T_kv, head_dim)
        v = v.view(B, T_kv, self.n_head, self.head_dim).transpose(1, 2)  # (B, n_head, T_kv, head_dim)

        # Scaled dot-product attention
        scale = 1.0 / (self.head_dim ** 0.5)
        att = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B, n_head, T, T_kv)

        # Apply attention mask if provided
        if attention_mask is not None:
            # attention_mask: (B, T, T_kv) or (B, 1, T, T_kv)
            if attention_mask.dim() == 3:
                attention_mask = attention_mask.unsqueeze(1)  # Add head dimension
            att = att.masked_fill(attention_mask == 0, float('-inf'))

        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)

        # Apply attention to values
        y = torch.matmul(att, v)  # (B, n_head, T, head_dim)
        y = y.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, emb_dim)

        # Output projection
        y = self.resid_dropout(self.proj(y))

        # Prepare cache for next iteration - ä¿å­˜åŸå§‹çš„3ç»´kå’Œv
        present_key_value = (k_for_cache, v_for_cache) if use_cache else None

        return y, present_key_value


class FeedForward(nn.Module):

    def __init__(self, emb_dim, n_inner, resid_drop=0.1, act='gelu'):
        super().__init__()
        self.c_fc = nn.Linear(emb_dim, n_inner)
        self.c_proj = nn.Linear(n_inner, emb_dim)
        self.dropout = nn.Dropout(resid_drop)
        self.act = F.gelu if act == 'gelu' else F.relu

    def forward(self, x):
        x = self.c_fc(x)
        x = self.act(x)
        x = self.c_proj(x)
        return self.dropout(x)


class EncoderBlock(nn.Module):

    def __init__(self, emb_dim, n_head, n_inner, attn_drop=0.1, resid_drop=0.1, 
                 act='gelu', norm_type='layernorm', norm_eps=1e-6):
        super().__init__()
        self.ln_1 = make_norm(norm_type, emb_dim, norm_eps)
        self.attn = MultiHeadAttention(emb_dim, n_head, attn_drop, resid_drop)
        self.ln_2 = make_norm(norm_type, emb_dim, norm_eps)
        self.mlp = FeedForward(emb_dim, n_inner, resid_drop, act)

    def forward(self, x, attention_mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ï¼ˆédecoderè‡ªæ³¨æ„åŠ›ï¼‰
        attn_output, _ = self.attn(self.ln_1(x), attention_mask=attention_mask, is_decoder_self_attn=False)
        x = x + attn_output
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        x = x + self.mlp(self.ln_2(x))
        return x


class DecoderBlock(nn.Module):

    def __init__(self, emb_dim, n_head, n_inner, attn_drop=0.1, resid_drop=0.1, 
                 act='gelu', norm_type='layernorm', norm_eps=1e-6):
        super().__init__()
        self.ln_1 = make_norm(norm_type, emb_dim, norm_eps)
        self.self_attn = MultiHeadAttention(emb_dim, n_head, attn_drop, resid_drop)
        self.ln_2 = make_norm(norm_type, emb_dim, norm_eps)
        self.cross_attn = MultiHeadAttention(emb_dim, n_head, attn_drop, resid_drop)
        self.ln_3 = make_norm(norm_type, emb_dim, norm_eps)
        self.mlp = FeedForward(emb_dim, n_inner, resid_drop, act)

    def forward(self, x, encoder_hidden=None, attention_mask=None, 
                past_key_value=None, use_cache=False, cross_key_value=None):
        # ä¿®æ”¹ï¼šå»é™¤å› æœæ©ç ï¼Œå› ä¸ºdiffusionæ¨¡å‹ä¸éœ€è¦ä¸¥æ ¼çš„åºåˆ—é¡ºåº
        # è‡ªæ³¨æ„åŠ›ï¼ˆä¸ä½¿ç”¨å› æœæ©ç ï¼‰
        self_past_kv = None
        cross_past_kv = None
        if past_key_value is not None:
            if len(past_key_value) >= 1:
                self_past_kv = past_key_value[0]
            if len(past_key_value) >= 2:
                cross_past_kv = past_key_value[1]
        
        attn_output, present_key_value = self.self_attn(
            self.ln_1(x), 
            attention_mask=None,  # ä¸ä½¿ç”¨å› æœæ©ç 
            past_key_value=self_past_kv,
            use_cache=use_cache,
            is_decoder_self_attn=True
        )
        x = x + attn_output

        # äº¤å‰æ³¨æ„åŠ›
        if encoder_hidden is not None:
            if cross_key_value is not None:
                # ğŸš€ ä½¿ç”¨é¢„è®¡ç®—çš„KVï¼Œé¿å…é‡å¤è®¡ç®—
                encoder_kv = cross_key_value
            else:
                # å…¼å®¹æ—§é€»è¾‘ï¼šé‡æ–°è®¡ç®—ï¼ˆä»…ç”¨äºéä¼˜åŒ–è·¯å¾„ï¼‰
                encoder_kv = torch.cat([encoder_hidden, encoder_hidden], dim=-1)  # Concat K and V
            
            cross_attn_output, cross_present = self.cross_attn(
                self.ln_2(x),
                key_value=encoder_kv,
                past_key_value=cross_past_kv,
                use_cache=use_cache
            )
            x = x + cross_attn_output
            
            if use_cache:
                present_key_value = (present_key_value, cross_present)
        
        # å‰é¦ˆç½‘ç»œ
        x = x + self.mlp(self.ln_3(x))
        
        return_dict = {}
        return_dict['hidden_states'] = x
        if use_cache:
            return_dict['present_key_value'] = present_key_value
        
        return return_dict


class ModelOutput:

    def __init__(self):
        self.loss = None
        self.logits = None
        self.hidden_states = None
        self.past_key_values = None


class DIFF_GRM(AbstractModel):

    def __init__(
        self,
        config: dict,
        dataset: AbstractDataset,
        tokenizer: AbstractTokenizer
    ):
        super().__init__(config, dataset, tokenizer)
        
        self.config = config
        self.tokenizer = tokenizer
        self.n_digit = config['n_digit']
        self.codebook_size = config['codebook_size']
        self.vocab_size = tokenizer.vocab_size
        
        # Model dimensions
        self.n_embd = config['n_embd']
        self.n_head = config['n_head']
        self.n_inner = config['n_inner']
        self.dropout = config['dropout']
        
        # Encoder layers
        self.encoder_n_layer = config['encoder_n_layer']
        self.decoder_n_layer = config['decoder_n_layer']
        
        # Normalization configuration
        self.norm_type = (config.get('norm_type', 'layernorm') or 'layernorm').lower()
        self.norm_eps  = float(config.get('norm_eps', 1e-6 if self.norm_type=='rmsnorm' else 1e-5))
        
        # ==== è¯»å–æ–°ç­–ç•¥ ====
        self.masking_strategy = config.get('masking_strategy', 'random')  # random | sequential
        
        if self.masking_strategy == 'sequential':
            # è¿è´¯å¤šè§†å›¾
            seq_cfg = config.get('sequential_steps', 'auto')
            self.seq_steps = self.n_digit if seq_cfg in (None, 'auto') else int(seq_cfg)
            assert 1 <= self.seq_steps <= self.n_digit, \
                f"sequential_steps must be 1~{self.n_digit}, got {self.seq_steps}"
            
            # æ–°å¢ï¼šå¤šè·¯å¾„æ”¯æŒ
            self.sequential_paths = config.get('sequential_paths', 1)
            assert self.sequential_paths >= 1, \
                f"sequential_paths must be >= 1, got {self.sequential_paths}"
            
            self.augment_factor = self.seq_steps * self.sequential_paths  # æ›´æ–°è®¡ç®—æ–¹å¼
            print(f"[MODEL] â–¶ use SEQUENTIAL views: steps={self.seq_steps}, "
                  f"paths={self.sequential_paths}, augment_factor={self.augment_factor}")
            # ç§»é™¤ä¸å¿…è¦çš„mask_probsè®¾ç½®ï¼ŒèŠ‚çœå†…å­˜
            self.mask_probs = None
        elif self.masking_strategy == 'guided':
            # ç½®ä¿¡åº¦å¼•å¯¼çš„è¿è´¯å¤šè§†å›¾ï¼ˆæ¯ä¸ªbatchç”±æ¨¡å‹å†³å®šæ­ç¤ºé¡ºåºï¼‰
            guided_cfg = config.get('guided_steps', 'auto')
            self.guided_steps = self.n_digit if guided_cfg in (None, 'auto') else int(guided_cfg)
            # é™åˆ¶æœ€å¤š 4 æ­¥ï¼ˆä½ ç°åœ¨ n_digit=4ï¼Œå› æ­¤åˆšå¥½ 4ï¼‰
            self.guided_steps = min(self.guided_steps, self.n_digit, 4)
            self.guided_conf_metric = config.get('guided_conf_metric', 'msp')
            assert self.guided_conf_metric in ('msp', 'entropy'), \
                f"guided_conf_metric must be one of ['msp','entropy'], got {self.guided_conf_metric}"
            # æ–°å¢ï¼šé€‰æ‹©æ­ç¤ºâ€œæœ€æœ‰æŠŠæ¡(most)â€æˆ–â€œæœ€ä¸æŠŠæ¡(least)â€çš„ä½ç½®
            self.guided_select = config.get('guided_select', 'most')
            assert self.guided_select in ('most', 'least'), \
                f"guided_select must be one of ['most','least'], got {self.guided_select}"
            self.augment_factor = self.guided_steps
            print(f"[MODEL] â–¶ GUIDED: steps={self.guided_steps}, metric={self.guided_conf_metric}, "
                  f"select={self.guided_select}, augment_factor={self.augment_factor}")
            self.mask_probs = None
        else:
            # æ—§çš„éšæœºæ©ç åˆ†æ”¯ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            # Diffusion specific parameters - å¤šæ¦‚ç‡æ©ç é…ç½®
            # æ–°å¢ï¼šæ”¯æŒæŒ‰åŒºé—´éšæœºé‡‡æ ·å•ä¸€æ©ç æ¦‚ç‡ï¼Œå¹¶å¯é€šè¿‡augment_factoré‡å¤è¯¥æ¦‚ç‡
            self.mask_prob_random = bool(config.get('mask_prob_random', False))
            if self.mask_prob_random:
                low = float(config.get('mask_prob_random_min', 0.0))
                high = float(config.get('mask_prob_random_max', 1.0))
                if not (0.0 <= low <= high <= 1.0):
                    raise ValueError(
                        f"mask_prob_random_min/max must satisfy 0.0 <= min <= max <= 1.0, got min={low}, max={high}"
                    )
                sampled_prob = float(np.random.uniform(low, high))
                # æŒ‰éœ€æ±‚ï¼šå¼€å¯éšæœºæ©ç æ¦‚ç‡æ—¶ä¸åšå¤šè§†å›¾æ‰©å¢
                self.augment_factor = 1
                self.mask_probs = [sampled_prob]
                self.sampled_mask_prob = sampled_prob
                print(
                    f"[MODEL] Using RANDOMLY-SAMPLED masking prob: {sampled_prob:.4f} (range [{low}, {high}]); disable multi-view (augment_factor=1)"
                )
            elif 'mask_probs' in config and config['mask_probs'] is not None:
                # æ–°æ–¹å¼ï¼šç›´æ¥æŒ‡å®šå¤šä¸ªæ©ç æ¦‚ç‡
                mask_probs_raw = config['mask_probs']
                
                if isinstance(mask_probs_raw, str):
                    # å­—ç¬¦ä¸²æ ¼å¼ï¼š"1.0,0.75,0.5,0.25"
                    self.mask_probs = [float(p.strip()) for p in mask_probs_raw.split(',')]
                elif isinstance(mask_probs_raw, (list, tuple)):
                    # åˆ—è¡¨æˆ–å…ƒç»„æ ¼å¼ï¼š[1.0, 0.75, 0.5, 0.25]
                    self.mask_probs = [float(p) for p in mask_probs_raw]
                elif isinstance(mask_probs_raw, (int, float)):
                    # å•ä¸ªæ•°å€¼ï¼Œè½¬æ¢ä¸ºå•å…ƒç´ åˆ—è¡¨
                    self.mask_probs = [float(mask_probs_raw)]
                else:
                    # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†è§£æ
                    try:
                        mask_probs_str = str(mask_probs_raw)
                        self.mask_probs = [float(p.strip()) for p in mask_probs_str.split(',')]
                    except (ValueError, AttributeError):
                        raise ValueError(f"Cannot parse mask_probs: {mask_probs_raw} (type: {type(mask_probs_raw)}). "
                                       "Expected string like '1.0,0.75,0.5,0.25' or list like [1.0, 0.75, 0.5, 0.25]")
                
                self.augment_factor = len(self.mask_probs)  # è‡ªåŠ¨è®¾ç½®å¢å¼ºå€æ•°
                print(f"[MODEL] Using multi-probability masking: {self.mask_probs}")
            else:
                # æ—§æ–¹å¼ï¼šå•ä¸€æ©ç æ¦‚ç‡ + å¢å¼ºå€æ•°
                mask_prob = config.get('mask_prob', 0.5)
                self.augment_factor = config.get('augment_factor', 4)
                self.mask_probs = [float(mask_prob)] * self.augment_factor  # é‡å¤ç›¸åŒæ¦‚ç‡
                print(f"[MODEL] Using single-probability masking: {mask_prob} x {self.augment_factor}")
        
        # éªŒè¯æ©ç æ¦‚ç‡çš„æœ‰æ•ˆæ€§ï¼ˆä»…å¯¹randomç­–ç•¥æœ‰æ•ˆï¼‰
        if self.masking_strategy == 'random' and self.mask_probs is not None:
            for i, prob in enumerate(self.mask_probs):
                if not (0.0 <= prob <= 1.0):
                    raise ValueError(f"mask_probs[{i}] = {prob} is not in valid range [0.0, 1.0]")
        
        # Embeddings
        self.embedding = nn.Embedding(self.vocab_size, self.n_embd)
        
        # æ·»åŠ ä¸RPG_EDä¸€è‡´çš„item_mlpï¼šå°†n_digitä¸ªSID tokenå‹ç¼©ä¸º1ä¸ªtoken
        self.item_mlp = nn.Sequential(
            nn.Linear(self.n_digit * self.n_embd, self.n_embd),  # n_digitÃ—d â†’ d
            nn.ReLU(),
            nn.Linear(self.n_embd, self.n_embd)
        )
        
        # æ–°å¢ï¼šæ©ç åµŒå…¥è¡¨ï¼Œç”¨äºè¡¨ç¤ºè¢«æ©ç çš„ä½ç½®
        self.mask_emb_table = nn.Embedding(self.n_digit, self.n_embd)
        
        # ä½ç½®ç¼–ç ï¼šåªä¸ºencoderæ·»åŠ ç»å¯¹ä½ç½®ç¼–ç ï¼ˆä¸RPG_EDä¸€è‡´ï¼‰
        self.max_history_len = config.get('max_history_len', 50)  # ä»configè¯»å–ï¼Œé»˜è®¤50
        self.pos_emb_enc = nn.Embedding(self.max_history_len, self.n_embd)
        # ç§»é™¤decoderä½ç½®ç¼–ç ï¼Œdecoderåªä½¿ç”¨æ©ç 
        
        # Encoder blocks
        self.encoder_blocks = nn.ModuleList([
            EncoderBlock(
                self.n_embd, self.n_head, self.n_inner,
                config['attn_pdrop'], config['resid_pdrop'],
                act='gelu',
                norm_type=self.norm_type, norm_eps=self.norm_eps
            )
            for _ in range(self.encoder_n_layer)
        ])
        
        # Decoder blocks  
        self.decoder_blocks = nn.ModuleList([
            DecoderBlock(
                self.n_embd, self.n_head, self.n_inner,
                config['attn_pdrop'], config['resid_pdrop'],
                act='gelu',
                norm_type=self.norm_type, norm_eps=self.norm_eps
            )
            for _ in range(self.decoder_n_layer)
        ])
        
        # Layer normalization
        self.ln_f = make_norm(self.norm_type, self.n_embd, self.norm_eps)
        
        # -- 1.1 åˆ é™¤æ—§çš„ç‹¬ç«‹ headsï¼Œæ”¹ä¸ºå…±äº« embedding dot-product --
        share_out = self.config.get('share_decoder_output_embedding', True)
        if share_out:
            # ç›´æ¥ weight-tyingï¼Œä¸æ–°å¢å‚æ•°
            self.output_adapter = nn.Identity()
            print(f"[DIFF_GRM] Using shared embedding dot-product output layer")
        else:
            # è‹¥ä»¥åè¦å›æ»šåˆ°ç‹¬ç«‹ headï¼Œç”¨è¿™ä¸€è¡Œ
            self.output_adapter = nn.Linear(self.n_embd, self.n_embd, bias=False)
            print(f"[DIFF_GRM] Using independent Linear output adapter")
        # -------------------------------------------------------------
        
        # Dropout
        self.drop = nn.Dropout(self.dropout)
        
        # Initialize weights
        self.apply(self._init_weights)

    def resample_mask_prob_if_needed(self):
        """
        å½“é‡‡ç”¨ random + mask_prob_random=true æ—¶ï¼Œåœ¨è®­ç»ƒçš„æ¯ä¸ª epoch å¼€å§‹è°ƒç”¨ï¼Œ
        é‡æ–°ä»è®¾å®šåŒºé—´é‡‡æ ·ä¸€æ¬¡æ©ç æ¦‚ç‡ï¼Œå¹¶æ›´æ–°å½“å‰ epoch ä½¿ç”¨çš„æ©ç ç‡ä¸lossç¼©æ”¾ã€‚
        """
        if self.masking_strategy == 'random' and getattr(self, 'mask_prob_random', False):
            low = float(getattr(self, 'mask_prob_random_min', 0.0)) if hasattr(self, 'mask_prob_random_min') else float(self.config.get('mask_prob_random_min', 0.0))
            high = float(getattr(self, 'mask_prob_random_max', 1.0)) if hasattr(self, 'mask_prob_random_max') else float(self.config.get('mask_prob_random_max', 1.0))
            if not (0.0 <= low <= high <= 1.0):
                raise ValueError(
                    f"mask_prob_random_min/max must satisfy 0.0 <= min <= max <= 1.0, got min={low}, max={high}"
                )
            sampled_prob = float(np.random.uniform(low, high))
            self.mask_probs = [sampled_prob]  # å•è§†å›¾
            self.sampled_mask_prob = sampled_prob
            print(f"[MODEL] [Epoch-Resample] RANDOM masking prob resampled to {sampled_prob:.4f} (range [{low}, {high}]); augment_factor=1")

    def set_masking_mode(self, strategy: str, **kw):
        """
        è®­ç»ƒè¿‡ç¨‹ä¸­çš„çƒ­åˆ‡æ¢ï¼š
        - strategy: 'guided' | 'sequential' | 'random'
        - kw: å¯¹åº”ç­–ç•¥éœ€è¦çš„è¶…å‚ï¼ˆè§ä¸‹ï¼‰
        """
        self.masking_strategy = strategy

        if strategy == 'sequential':
            # steps
            seq_cfg = kw.get('sequential_steps', self.config.get('sequential_steps', 'auto'))
            self.seq_steps = self.n_digit if seq_cfg in (None, 'auto') else int(seq_cfg)
            self.sequential_paths = int(kw.get('sequential_paths', self.config.get('sequential_paths', 1)))
            self.augment_factor = self.seq_steps * self.sequential_paths
            self.mask_probs = None
            print(f"[SCHEDULE] â†’ SEQUENTIAL: steps={self.seq_steps}, paths={self.sequential_paths}, augment_factor={self.augment_factor}")

        elif strategy == 'guided':
            guided_cfg = kw.get('guided_steps', self.config.get('guided_steps', 'auto'))
            self.guided_steps = self.n_digit if guided_cfg in (None, 'auto') else int(guided_cfg)
            self.guided_steps = min(self.guided_steps, self.n_digit, 4)
            self.guided_conf_metric = kw.get('guided_conf_metric', self.config.get('guided_conf_metric', 'msp'))
            self.guided_select = kw.get('guided_select', self.config.get('guided_select', 'least'))
            # æ³¨æ„ï¼šforward é‡Œè¯» self.config['guided_refresh_each_step']ï¼Œæ‰€ä»¥è¦åŒæ­¥å› config
            self.config['guided_refresh_each_step'] = bool(kw.get(
                'guided_refresh_each_step',
                self.config.get('guided_refresh_each_step', False)
            ))
            self.augment_factor = self.guided_steps
            self.mask_probs = None
            print(f"[SCHEDULE] â†’ GUIDED({self.guided_select}): steps={self.guided_steps}, metric={self.guided_conf_metric}, refresh={self.config['guided_refresh_each_step']}, augment_factor={self.augment_factor}")

        elif strategy == 'random':
            # ä¿ç•™æ—§é€»è¾‘ï¼ŒæŒ‰éœ€è¦†ç›–
            self.mask_prob_random = bool(kw.get('mask_prob_random', self.config.get('mask_prob_random', False)))
            if self.mask_prob_random:
                self.mask_probs = [float(np.random.uniform(
                    float(kw.get('mask_prob_random_min', self.config.get('mask_prob_random_min', 0.0))),
                    float(kw.get('mask_prob_random_max', self.config.get('mask_prob_random_max', 1.0)))
                ))]
                self.augment_factor = 1
            else:
                if 'mask_probs' in kw and kw['mask_probs'] is not None:
                    self.mask_probs = [float(p) for p in (kw['mask_probs'] if isinstance(kw['mask_probs'], (list, tuple)) else str(kw['mask_probs']).split(','))]
                    self.augment_factor = len(self.mask_probs)
                else:
                    mp = float(kw.get('mask_prob', self.config.get('mask_prob', 0.5)))
                    af = int(kw.get('augment_factor', self.config.get('augment_factor', 4)))
                    self.mask_probs = [mp] * af
                    self.augment_factor = af
            print(f"[SCHEDULE] â†’ RANDOM: mask_probs={self.mask_probs}, augment_factor={self.augment_factor}")
        else:
            raise ValueError(f"Unknown masking strategy: {strategy}")

    def _compute_digit_logits(self, hidden_last, digit):
        """
        ä½¿ç”¨å…±äº«embeddingçš„dot-productè®¡ç®—logits
        
        Args:
            hidden_last: (B, d_model) - decoderè¾“å‡ºçš„éšè—çŠ¶æ€
            digit: 0..n_digit-1 - è¦é¢„æµ‹çš„digitä½ç½®
            
        Returns:
            logits: (B, codebook_size) - é¢„æµ‹logits
        """
        if digit is None:
            raise ValueError("digitå‚æ•°ä¸èƒ½ä¸ºNoneï¼Œå¿…é¡»æŒ‡å®šè¦è®¡ç®—çš„codebookä½ç½®")
        
        if digit >= self.n_digit:
            raise ValueError(f"digit={digit} è¶…å‡ºèŒƒå›´ï¼Œåº”è¯¥åœ¨ [0, {self.n_digit-1}]")
        
        # 2.1 å–å‡º embedding matrix çš„ç›¸åº”åˆ‡ç‰‡
        # token ID å¸ƒå±€ = [PAD, BOS, EOS, digit0 256 ä¸ª, digit1 256 ä¸ª, ...]
        start = self.tokenizer.sid_offset + digit * self.codebook_size
        end = start + self.codebook_size  # ä¸å« end
        # shape: (codebook_size, d_model)
        E_sub = self.embedding.weight[start:end]
        
        # 2.2 optional adapter
        h = self.output_adapter(hidden_last)  # (B, d_model)
        
        # 2.3 dot-product å¾— logits
        # (B, d_model) @ (d_model, codebook_size).T â†’ (B, codebook_size)
        logits = torch.matmul(h, E_sub.t())
        
        return logits

    @property
    def n_parameters(self) -> str:
        """
        Return the number of parameters in the model.
        """
        n_params = sum(p.numel() for p in self.parameters())
        return f"{n_params:,}"

    def forward(self, batch: dict, return_loss=True) -> ModelOutput:
        """
        Diffusionè®­ç»ƒï¼šå¤„ç†æ©ç æ•°æ®ï¼Œé¢„æµ‹è¢«æ©ç çš„ä½ç½®
        
        Args:
            batch: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - history_sid: å†å²SIDåºåˆ— [B, seq_len, n_digit]
                - decoder_input_ids: decoderè¾“å…¥ [B, n_digit] 
                - decoder_labels: çœŸå®æ ‡ç­¾ [B, n_digit]
        """
        device = next(self.parameters()).device
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if hasattr(self, '_debug_printed'):
            pass
        else:
            print(f"[DIFF_GRM] Using RPG_ED-style encoder: MLP compression + fixed 50-length sequence")
            print(f"[DIFF_GRM] vocab_size: {self.vocab_size}, codebook_size: {self.codebook_size}")
            print(f"[DIFF_GRM] masking_strategy: {self.masking_strategy}")
            if self.masking_strategy == 'random' and self.mask_probs is not None:
                print(f"[DIFF_GRM] mask_probs: {self.mask_probs}")
            self._debug_printed = True
        
        # --- Encoder ---
        history_sid = batch['history_sid'].to(device)  # [B, seq_len, n_digit]
        B, seq_len, n_digit = history_sid.shape
        
        # æ–­è¨€ï¼šhistory_sid åº”è¯¥æ˜¯ codebook id (0..K-1) æˆ– PAD (-1)
        valid_hist = ((history_sid == -1) | ((history_sid >= 0) & (history_sid < self.codebook_size))).all()
        assert bool(valid_hist), \
            f"history_sid åº”ä¸º codebook id(0..{self.codebook_size-1}) æˆ– -1(PAD)ï¼Œä½†å‘ç°è¶Šç•Œå€¼"
        
        # 1. å°†history SIDè½¬æ¢ä¸ºtoken IDs
        history_tokens = torch.zeros(B, seq_len, n_digit, dtype=torch.long, device=device)
        for d in range(n_digit):
            # å¤„ç†PADï¼š-1æ˜ å°„åˆ°token_id=0(PAD)ï¼Œå…¶ä»–codebook_idæ­£å¸¸åŠ offset
            codebook_ids = history_sid[:, :, d]
            token_ids = torch.where(
                codebook_ids == -1,  # PADä½ç½®
                torch.zeros_like(codebook_ids),  # æ˜ å°„åˆ°token_id=0(PAD)
                codebook_ids + self.tokenizer.sid_offset + d * self.codebook_size  # æ­£å¸¸åŠ offset
            )
            # ç¡®ä¿token IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
            token_ids = torch.clamp(token_ids, 0, self.vocab_size - 1)
            history_tokens[:, :, d] = token_ids
        
        # 2. è·å–tokenåµŒå…¥
        tok_emb = self.embedding(history_tokens)  # [B, seq_len, n_digit, d]
        B, S, _, d = tok_emb.shape
        
        # 3. é‡å¡‘å¹¶é€šè¿‡MLPå‹ç¼©ï¼šn_digitä¸ªSID token â†’ 1ä¸ªitem token
        item_emb = tok_emb.reshape(B, S, self.n_digit * d)  # [B, S, n_digit*d]
        item_emb = self.item_mlp(item_emb)  # [B, S, d]
        
        # 4. æ·»åŠ ä½ç½®ç¼–ç ï¼ˆä¸RPG_EDä¸€è‡´ï¼‰
        pos_ids = torch.arange(S, device=item_emb.device)  # (S,)
        pos_emb = self.pos_emb_enc(pos_ids)  # (S, d)
        pos_emb = pos_emb.unsqueeze(0).expand(B, -1, -1)  # (B, S, d)
        
        # 5. å°†ä½ç½®ç¼–ç åŠ åˆ°item_embä¸Š
        encoder_hidden = item_emb + pos_emb  # [B, S, d]
        encoder_hidden = self.drop(encoder_hidden)
        
        # 6. å¤„ç†PADä½ç½®çš„æ³¨æ„åŠ›æ©ç 
        if 'history_mask' in batch:
            history_mask = batch['history_mask'].to(device)  # [B, seq_len]
            # åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼šTrue=æœ‰æ•ˆä½ç½®ï¼ŒFalse=PADä½ç½®
            attention_mask = history_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, seq_len]
            attention_mask = attention_mask.expand(-1, -1, seq_len, -1)  # [B, 1, seq_len, seq_len]
        else:
            attention_mask = None
        
        # Pass through encoder blocks
        encoder_hidden = encoder_hidden
        for block in self.encoder_blocks:
            encoder_hidden = block(encoder_hidden, attention_mask=attention_mask)
        
        encoder_hidden = self.ln_f(encoder_hidden)  # [B, seq_len*n_digit, emb_dim]
        
        # >>> æ–°å¢ï¼šå°† PAD ä½ç½®çš„ encoder_hidden æ¸…é›¶ï¼Œé¿å… cross-attn çœ‹åˆ°æ— æ•ˆKV <<<
        if 'history_mask' in batch:
            history_mask = batch['history_mask'].to(device)  # [B, S]ï¼ŒTrue=æœ‰æ•ˆ
            encoder_hidden = encoder_hidden * history_mask.unsqueeze(-1).float()
        
        if not return_loss:
            # æ¨ç†æ¨¡å¼ï¼Œç›´æ¥è¿”å›encoderè¾“å‡º
            output = ModelOutput()
            output.hidden_states = encoder_hidden
            return output
        
        # --- å¤šæ¦‚ç‡æ©ç æ‰©å±• ---
        decoder_input_ids = batch['decoder_input_ids'].to(device)  # [B, n_digit]
        decoder_labels = batch['decoder_labels'].to(device)  # [B, n_digit]
        
        # ç¡®ä¿decoderè¾“å…¥åœ¨æœ‰æ•ˆèŒƒå›´å†…
        decoder_input_ids = torch.clamp(decoder_input_ids, 0, self.codebook_size - 1)
        decoder_labels = torch.clamp(decoder_labels, 0, self.codebook_size - 1)
        
        # ---------- æ„é€ è®­ç»ƒè§†å›¾ ----------
        all_masked_input_ids = []
        all_labels = []
        all_mask_positions = []
        all_encoder_hidden = []
        
        if self.masking_strategy == 'sequential':
            # è¿è´¯å¤šè§†å›¾ï¼šæ”¯æŒå¤šè·¯å¾„å¹¶è¡Œ
            for p in range(self.sequential_paths):  # å…ˆç”Ÿæˆå¤šæ¡è·¯å¾„
                # â‘  æœ¬æ¡è·¯å¾„å„ä¸ªæ ·æœ¬çš„éšæœºé¡ºåº
                orders = torch.argsort(torch.rand(B, self.n_digit, device=device), dim=1)

                # â‘¡ step-0: å…¨ MASK
                full_mask = torch.ones(B, self.n_digit, dtype=torch.bool, device=device)
                inp0 = decoder_input_ids.new_zeros(B, self.n_digit)        # å…¨ 0 â†’ MASK
                all_masked_input_ids.append(inp0)
                all_labels.append(decoder_labels)
                all_mask_positions.append(full_mask.float())
                all_encoder_hidden.append(encoder_hidden)

                # â‘¢ step-1 â€¦ step-(seq_steps-1) ï¼šæŒ‰éšæœºé¡ºåºé€æ­¥æ­å¼€
                for reveal in range(1, self.seq_steps):        # 1 .. seq_steps-1
                    mask_pos = torch.ones_like(full_mask)      # å…ˆå…¨éƒ¨ MASK

                    # orders[:, :reveal] å½¢çŠ¶ (B, reveal)
                    reveal_idx = orders[:, :reveal]            # æ¯æ¡æ ·æœ¬æœ¬æ¬¡éœ€è¦æ­å¼€çš„åˆ—
                    mask_pos.scatter_(1, reveal_idx, 0)        # ç½® 0 è¡¨ç¤ºã€Œä¸æ©ç ã€

                    inp = decoder_input_ids.clone()
                    inp[mask_pos] = 0                          # æ©ç ä½å†™ 0

                    all_masked_input_ids.append(inp)
                    all_labels.append(decoder_labels)
                    all_mask_positions.append(mask_pos.float())
                    all_encoder_hidden.append(encoder_hidden)
        elif self.masking_strategy == 'guided':
            B = decoder_labels.size(0)
            device = decoder_labels.device

            def score_with_mask(cur_mask: torch.Tensor):
                # cur_mask: [B, n_digit], True=è¢«æ©ç›–ï¼ˆéœ€è¦é¢„æµ‹ï¼‰
                cur_inp = decoder_input_ids.new_zeros(B, self.n_digit)
                cur_inp[~cur_mask] = decoder_labels[~cur_mask]  # æœªæ©ç›–çš„ä½ç½®æ”¾"çœŸæ ‡ç­¾"

                _was_training = self.training
                self.eval()
                with torch.no_grad():
                    if B == 1:  # åªåœ¨å•æ ·æœ¬æ—¶æ‰“å°ï¼Œé¿å…å¤šworkeråˆ·å±
                        print(f"[GUIDED] scoring: self.training={self.training}")  # è¿™é‡Œåº”ä¸º False
                    logits = self.forward_decoder_only(
                        {
                            'decoder_input_ids': cur_inp,
                            'encoder_hidden': encoder_hidden,
                            'mask_positions': cur_mask.float()
                        },
                        return_loss=False, digit=None, use_cache=False
                    ).logits  # [B, n_digit, K]
                if _was_training:
                    self.train()

                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆä¸æ¨ç†ä¸€è‡´ï¼‰
                probs = F.softmax(logits, dim=-1)
                if self.guided_conf_metric == 'entropy':
                    ent = -(probs * probs.clamp_min(1e-12).log()).sum(dim=-1)
                    conf = -ent
                else:  # 'msp'
                    conf = probs.max(dim=-1).values  # â˜… è¿™é‡Œç”¨ max(...).values

                return conf  # [B, n_digit]

            refresh = str(self.config.get('guided_refresh_each_step', False)).lower() in ('1','true','yes','y')
            all_masked_input_ids, all_labels, all_mask_positions, all_encoder_hidden = [], [], [], []

            if not refresh:
                # ------- ä¸€æ¬¡æ€§æ’åºï¼Œä¸åˆ·æ–° -------
                full_mask = torch.ones(B, self.n_digit, dtype=torch.bool, device=device)
                conf = score_with_mask(full_mask)  # ç”¨å…¨æ©ç›–æ‰“åˆ†å¾—åˆ° rank
                if self.guided_select == 'most':
                    order = torch.argsort(conf, 1, True)
                else:
                    order = torch.argsort(conf, 1, False)

                for t in range(1, self.guided_steps + 1):
                    cur_mask = torch.zeros(B, self.n_digit, dtype=torch.bool, device=device)
                    cols = order[:, :t]
                    cur_mask.scatter_(1, cols, True)

                    cur_inp = decoder_input_ids.new_zeros(B, self.n_digit)
                    cur_inp[~cur_mask] = decoder_labels[~cur_mask]

                    all_masked_input_ids.append(cur_inp)
                    all_labels.append(decoder_labels)
                    all_mask_positions.append(cur_mask.float())
                    all_encoder_hidden.append(encoder_hidden)
            else:
                # ------- æ¯æ­¥åˆ·æ–° -------
                cur_mask = torch.zeros(B, self.n_digit, dtype=torch.bool, device=device)
                for t in range(1, self.guided_steps + 1):
                    conf = score_with_mask(cur_mask)  # æœ¬æ­¥ç½®ä¿¡åº¦

                    # å·²ç»æ©ç›–è¿‡çš„åˆ—ä¸å†é€‰æ‹©
                    if self.guided_select == 'most':
                        conf = conf.masked_fill(cur_mask, -1e9)
                        cols = torch.argmax(conf, dim=1, keepdim=True)  # æ¯ä¸ªæ ·æœ¬æŒ‘ 1 åˆ—
                    else:
                        conf = conf.masked_fill(cur_mask,  1e9)
                        cols = torch.argmin(conf, dim=1, keepdim=True)

                    cur_mask.scatter_(1, cols, True)

                    cur_inp = decoder_input_ids.new_zeros(B, self.n_digit)
                    cur_inp[~cur_mask] = decoder_labels[~cur_mask]

                    all_masked_input_ids.append(cur_inp)
                    all_labels.append(decoder_labels)
                    all_mask_positions.append(cur_mask.float())
                    all_encoder_hidden.append(encoder_hidden)
        
        else:
            # æ—§çš„éšæœºæ©ç åˆ†æ”¯ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            # LLaDAé£æ ¼ï¼šè‹¥å¯ç”¨mask_prob_randomï¼Œåˆ™æ¯ä¸ªbatchç‹¬ç«‹é‡‡æ ·ä¸€æ¬¡æ©ç ç‡
            batch_mask_prob = None
            if getattr(self, 'mask_prob_random', False):
                low = float(self.config.get('mask_prob_random_min', 0.0))
                high = float(self.config.get('mask_prob_random_max', 1.0))
                # ä½¿ç”¨torché‡‡æ ·ä»¥ä¾¿ä¸å…¨å±€éšæœºç§å­ä¸€è‡´
                batch_mask_prob = float(torch.empty(1).uniform_(low, high).item())
            for view_idx, mask_prob in enumerate(self.mask_probs):
                if batch_mask_prob is not None:
                    mask_prob = batch_mask_prob
                # ä¸ºå½“å‰æ©ç æ¦‚ç‡ç”Ÿæˆæ©ç 
                mask_positions = torch.rand(B, self.n_digit, device=device) < mask_prob  # [B, n_digit]
                
                # ç¡®ä¿æ¯ä¸ªæ ·æœ¬è‡³å°‘æœ‰ä¸€ä¸ªä½ç½®è¢«æ©ç 
                no_mask_samples = ~mask_positions.any(dim=1)  # [B]
                if no_mask_samples.any():
                    # å¯¹äºæ²¡æœ‰æ©ç çš„æ ·æœ¬ï¼Œå¼ºåˆ¶æ©ç ç¬¬ä¸€ä¸ªä½ç½®
                    mask_positions[no_mask_samples, 0] = True
                
                # åº”ç”¨æ©ç ï¼šè¢«æ©ç çš„ä½ç½®è®¾ä¸º0
                masked_input_ids = decoder_input_ids.clone()  # [B, n_digit]
                masked_input_ids[mask_positions] = 0
                
                # å­˜å‚¨å½“å‰è§†å›¾çš„æ•°æ®
                all_masked_input_ids.append(masked_input_ids)
                all_labels.append(decoder_labels)  # æ ‡ç­¾ä¿æŒä¸å˜
                all_mask_positions.append(mask_positions.float())
                all_encoder_hidden.append(encoder_hidden)  # æ¯ä¸ªè§†å›¾ä½¿ç”¨ç›¸åŒçš„encoderè¾“å‡º
        
        # åˆå¹¶æ‰€æœ‰è§†å›¾ï¼š[B*n_views, ...]
        decoder_input_ids = torch.cat(all_masked_input_ids, dim=0)  # [B*n_views, n_digit]
        decoder_labels = torch.cat(all_labels, dim=0)  # [B*n_views, n_digit]
        mask_positions = torch.cat(all_mask_positions, dim=0)  # [B*n_views, n_digit]
        encoder_hidden = torch.cat(all_encoder_hidden, dim=0)  # [B*n_views, seq_len*n_digit, emb_dim]
        
        # æ›´æ–°batchå¤§å°å¹¶éªŒè¯å½¢çŠ¶
        B_expanded = B * self.augment_factor
        
        # å½¢çŠ¶éªŒè¯
        assert decoder_input_ids.shape[0] == B_expanded, f"decoder_input_ids shape mismatch: {decoder_input_ids.shape[0]} vs {B_expanded}"
        assert decoder_labels.shape[0] == B_expanded, f"decoder_labels shape mismatch: {decoder_labels.shape[0]} vs {B_expanded}"
        assert mask_positions.shape[0] == B_expanded, f"mask_positions shape mismatch: {mask_positions.shape[0]} vs {B_expanded}"
        assert encoder_hidden.shape[0] == B_expanded, f"encoder_hidden shape mismatch: {encoder_hidden.shape[0]} vs {B_expanded}"
        
        # ä¸€è‡´æ€§æ£€æŸ¥ï¼šguidedç­–ç•¥åº”è¯¥é€æ­¥å¢åŠ æ©ç æ•°
        if self.masking_strategy == 'guided':
            m = mask_positions.view(B, self.augment_factor, self.n_digit).sum(-1)  # [B, 4]
            assert torch.all(m[:, 1:] >= m[:, :-1]), "guided views should increase masked count monotonically"
        
        # --- Decoder (è®­ç»ƒæ¨¡å¼) ---
        # ğŸš€ è®­ç»ƒé˜¶æ®µä¹Ÿä½¿ç”¨ä¸æ¨ç†ä¸€è‡´çš„cross-attentionæŠ•å½±
        encoder_kv_list = []
        for blk in self.decoder_blocks:
            # æ‰§è¡Œ W_k/W_v æŠ•å½±ï¼Œä¸æ¨ç†ä¿æŒå®Œå…¨ä¸€è‡´
            kv_proj = blk.cross_attn.qkv(encoder_hidden)  # [B_expanded, seq_len, 3*emb_dim]
            # æå–Kå’ŒVéƒ¨åˆ†ï¼ˆè·³è¿‡Qéƒ¨åˆ†ï¼‰
            k = kv_proj[..., self.n_embd:2*self.n_embd]  # [B_expanded, seq_len, emb_dim]
            v = kv_proj[..., 2*self.n_embd:]              # [B_expanded, seq_len, emb_dim]
            # æ‹¼æ¥Kå’ŒV
            layer_kv = torch.cat([k, v], dim=-1)  # [B_expanded, seq_len, 2*emb_dim]
            encoder_kv_list.append(layer_kv)
        
        # æ„å»ºdecoderè¾“å…¥åµŒå…¥
        decoder_emb = torch.zeros(B_expanded, self.n_digit, self.n_embd, device=device)
        
        for d in range(self.n_digit):
            # è·å–å½“å‰digitçš„codebook IDs
            codebook_ids = decoder_input_ids[:, d]  # [B_expanded]
            
            # è½¬æ¢ä¸ºtoken IDsï¼Œæ·»åŠ å®‰å…¨æ£€æŸ¥
            token_ids = codebook_ids + self.tokenizer.sid_offset + d * self.codebook_size
            token_ids = torch.clamp(token_ids, 0, self.vocab_size - 1)
            
            # å®‰å…¨çš„embeddingæŸ¥æ‰¾
            token_emb = self.embedding(token_ids)  # [B_expanded, emb_dim]
            
            # è·å–å½“å‰digitçš„mask embedding
            mask_emb = self.mask_emb_table.weight[d]  # [emb_dim] - æ— é¢å¤–å¼ é‡åˆ›å»º
            mask_emb = mask_emb.unsqueeze(0).expand(B_expanded, -1)  # [B_expanded, emb_dim]
            
            # æ ¹æ®mask_positionså†³å®šä½¿ç”¨å“ªç§embedding
            is_masked = mask_positions[:, d].unsqueeze(-1)  # [B_expanded, 1]
            decoder_emb[:, d, :] = torch.where(is_masked.bool(), mask_emb, token_emb)
        
        # ç§»é™¤ä½ç½®ç¼–ç ï¼šdecoderåªä½¿ç”¨æ©ç ï¼Œä¸éœ€è¦ä½ç½®ç¼–ç 
        decoder_emb = self.drop(decoder_emb)
        
        # Pass through decoder blocks with consistent cross-attention
        decoder_hidden = decoder_emb
        for i, block in enumerate(self.decoder_blocks):
            block_output = block(
                decoder_hidden, 
                encoder_hidden=encoder_hidden,     # ä»ä¼ é€’Hï¼Œæ–¹ä¾¿fallback
                past_key_value=None,               # è®­ç»ƒæ—¶ä¸ä½¿ç”¨KV cache
                use_cache=False,                   # è®­ç»ƒæ—¶ä¸ä½¿ç”¨KV cache
                cross_key_value=encoder_kv_list[i] # ğŸš€ ä½¿ç”¨é¢„è®¡ç®—çš„KVï¼Œä¸æ¨ç†ä¸€è‡´
            )
            decoder_hidden = block_output['hidden_states']
        
        decoder_hidden = self.ln_f(decoder_hidden)  # [B_expanded, n_digit, emb_dim]
        
        # è®¡ç®—æŸå¤±
        if self.masking_strategy == 'random' and getattr(self, 'mask_prob_random', False):
            # LLaDA é£æ ¼ï¼šå¯¹æ¯ä¸ªæ ·æœ¬å…ˆæŒ‰æ©ç ä½æ±‡æ€»ï¼Œå†ä¹˜ä»¥ 1/tï¼Œæœ‰æ•ˆæŠ‘åˆ¶ä¸åŒæ©ç ç‡å¸¦æ¥çš„å°ºåº¦å·®å¼‚
            # è¿™é‡Œçš„ t ä½¿ç”¨â€œå®é™…æ©ç ç‡â€è€Œéé‡‡æ ·å‚æ•°ï¼Œé¿å…æå° t è¢«å¼ºåˆ¶æ©ä¸€ä¸ªä½æ—¶äº§ç”Ÿè¿‡å¤§æƒé‡
            per_sample_loss = torch.zeros(B_expanded, device=device)
            for d in range(self.n_digit):
                logits_d = self._compute_digit_logits(decoder_hidden[:, d, :], digit=d)
                labels_d = decoder_labels[:, d]
                mask_d = mask_positions[:, d].float()
                loss_d = F.cross_entropy(
                    logits_d, labels_d, reduction='none',
                    label_smoothing=self.config.get('label_smoothing', 0.1)
                )
                per_sample_loss += loss_d * mask_d  # åªè®¡æ©ç ä½
            # å®é™…æ©ç ç‡ t_iï¼šæ¯ä¸ªæ ·æœ¬è¢«æ©çš„æ¯”ä¾‹
            t_actual = mask_positions.float().mean(dim=1)  # [B_expanded]
            t_actual = torch.clamp(t_actual, min=1e-6)
            total_loss = (per_sample_loss / t_actual).mean()  # æŒ‰batchæ±‚å¹³å‡
        else:
            # åŸé€»è¾‘ï¼šåªåœ¨æ©ç ä½è®¡ç®—æŸå¤±ï¼Œå¹¶æŒ‰è¢«æ©ç tokenæ•°åšå¹³å‡
            total_loss = 0.0
            total_weight = 0.0
            for d in range(self.n_digit):
                logits_d = self._compute_digit_logits(decoder_hidden[:, d, :], digit=d)
                labels_d = decoder_labels[:, d]
                mask_d = mask_positions[:, d].float()
                loss_d = F.cross_entropy(
                    logits_d, labels_d, reduction='none',
                    label_smoothing=self.config.get('label_smoothing', 0.1)
                )
                total_loss += (loss_d * mask_d).sum()
                total_weight += mask_d.sum()
            if total_weight > 0:
                total_loss = total_loss / total_weight
            else:
                total_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        output = ModelOutput()
        output.loss = total_loss
        output.hidden_states = decoder_hidden
        output.logits = None  # ä¸è¿”å›æ‰€æœ‰logitsï¼ŒèŠ‚çœå†…å­˜
        
        return output

    def forward_decoder_only(self, batch: dict, return_loss=False, digit=None, 
                            past_key_values=None, use_cache=False) -> ModelOutput:
        """
        ä»…è¿è¡Œdecoderéƒ¨åˆ†ï¼Œç”¨äºæ¨ç†æ—¶çš„è¿­ä»£é¢„æµ‹
        
        Args:
            batch: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - decoder_input_ids: decoderè¾“å…¥ [B, n_digit]
                - encoder_hidden: encoderè¾“å‡º [B, seq_len, emb_dim]
                - mask_positions: æ©ç ä½ç½® [B, n_digit] (å¯é€‰)
            digit: è¦é¢„æµ‹çš„digitä½ç½®
            past_key_values: ç¼“å­˜çš„key-valueå¯¹ï¼Œç”¨äºåŠ é€Ÿæ¨ç†
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
        """
        device = next(self.parameters()).device
        
        decoder_input_ids = batch['decoder_input_ids'].to(device)  # [B, n_digit]
        encoder_hidden = batch['encoder_hidden'].to(device)  # [B, seq_len, emb_dim]
        B, n_digit = decoder_input_ids.shape
        
        # è·å–æ©ç ä½ç½®ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™å‡è®¾æ‰€æœ‰ä½ç½®éƒ½ä¸è¢«æ©ç 
        if 'mask_positions' in batch:
            mask_positions = batch['mask_positions'].to(device)  # [B, n_digit]
        else:
            mask_positions = torch.zeros(B, n_digit, device=device)
        
        # ğŸš€ Cross-KV Cacheä¼˜åŒ–ï¼šç¬¬ä¸€æ­¥è®¡ç®—ï¼Œåç»­æ­¥ä»past_key_valueså¤ç”¨
        encoder_kv_list = None
        
        if past_key_values is None and use_cache:
            # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯å±‚é¢„è®¡ç®—cross-attentionçš„KV
            encoder_kv_list = []
            for blk in self.decoder_blocks:
                with torch.no_grad():
                    kv_proj = blk.cross_attn.qkv(encoder_hidden)  # [B, seq_len, 3*emb_dim]
                    k = kv_proj[..., self.n_embd:2*self.n_embd]  # [B, seq_len, emb_dim]
                    v = kv_proj[..., 2*self.n_embd:]              # [B, seq_len, emb_dim]
                    layer_kv = torch.cat([k, v], dim=-1)  # [B, seq_len, 2*emb_dim]
                encoder_kv_list.append(layer_kv)
        elif past_key_values is not None:
            # åç»­æ­¥ï¼šä»past_key_valuesä¸­æå–cross-KVï¼Œå®ç°çœŸæ­£çš„cacheå¤ç”¨
            encoder_kv_list = []
            for layer_cache in past_key_values:
                if layer_cache is not None and len(layer_cache) >= 2:
                    _, cross_kv = layer_cache
                    if cross_kv is not None:
                        cross_key, cross_value = cross_kv
                        layer_kv = torch.cat([cross_key, cross_value], dim=-1)
                        encoder_kv_list.append(layer_kv)
                    else:
                        encoder_kv_list.append(None)
                else:
                    encoder_kv_list.append(None)
        
        # æ„å»ºdecoderè¾“å…¥åµŒå…¥
        decoder_emb = torch.zeros(B, n_digit, self.n_embd, device=device)
        
        for d in range(n_digit):
            # è·å–å½“å‰digitçš„token IDsï¼Œæ·»åŠ å®‰å…¨æ£€æŸ¥
            token_ids = decoder_input_ids[:, d] + self.tokenizer.sid_offset + d * self.codebook_size
            token_ids = torch.clamp(token_ids, 0, self.vocab_size - 1)
            token_emb = self.embedding(token_ids)  # [B, emb_dim]
            
            # è·å–å½“å‰digitçš„mask embedding
            mask_emb = self.mask_emb_table.weight[d]  # [emb_dim] - æ— é¢å¤–å¼ é‡åˆ›å»º
            mask_emb = mask_emb.unsqueeze(0).expand(B, -1)  # [B, emb_dim]
            
            # æ ¹æ®mask_positionså†³å®šä½¿ç”¨å“ªç§embedding
            is_masked = mask_positions[:, d].unsqueeze(-1)  # [B, 1]
            decoder_emb[:, d, :] = torch.where(is_masked.bool(), mask_emb, token_emb)
        
        # ç§»é™¤ä½ç½®ç¼–ç ï¼šdecoderåªä½¿ç”¨æ©ç ï¼Œä¸éœ€è¦ä½ç½®ç¼–ç 
        decoder_emb = self.drop(decoder_emb)
        
        # Pass through decoder blocks with KV cache support
        decoder_hidden = decoder_emb
        present_key_values = []
        
        for i, block in enumerate(self.decoder_blocks):
            # è·å–å½“å‰å±‚çš„past_key_value
            layer_past = past_key_values[i] if past_key_values is not None else None
            
            # ğŸš€ ä¼ å…¥é¢„è®¡ç®—çš„cross-KVï¼Œå®ç°cacheå¤ç”¨
            current_cross_kv = encoder_kv_list[i] if encoder_kv_list is not None else None
            
            block_output = block(
                decoder_hidden, 
                encoder_hidden=encoder_hidden,     # ä»ä¼ é€’Hï¼Œæ–¹ä¾¿fallback
                past_key_value=layer_past,
                use_cache=use_cache,
                cross_key_value=current_cross_kv   # åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ä¼ å…¥
            )
            decoder_hidden = block_output['hidden_states']
            
            # æ”¶é›†æ–°çš„key-value cache
            if use_cache:
                layer_present = block_output.get('present_key_value')
                if layer_present is not None and len(layer_present) >= 2:
                    self_present, cross_present = layer_present
                    # ç¡®ä¿cross_presentä¿å­˜åˆ†ç¦»çš„Kå’ŒVç”¨äºä¸‹æ¬¡ç¼“å­˜
                    if cross_present is not None:
                        # cross_presentåº”è¯¥æ˜¯(K, V)æ ¼å¼
                        layer_kv = encoder_kv_list[i] if encoder_kv_list is not None else None
                        if layer_kv is not None:
                            k, v = layer_kv.chunk(2, dim=-1)  # åˆ†ç¦»Kå’ŒV
                            cross_present = (k, v)  # ä¿å­˜åˆ†ç¦»æ ¼å¼
                    present_key_values.append((self_present, cross_present))
                else:
                    present_key_values.append(layer_present)
        
        # å¦‚æœä¸ä½¿ç”¨cacheï¼Œè®¾ä¸ºNone
        if not use_cache:
            present_key_values = None
        
        decoder_hidden = self.ln_f(decoder_hidden)  # [B, n_digit, emb_dim]
        
        # è®¡ç®—æŒ‡å®šdigitçš„logits
        if digit is not None:
            logits = self._compute_digit_logits(decoder_hidden[:, digit, :], digit=digit)
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šdigitï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„logits
            logits = []
            for d in range(n_digit):
                logits_d = self._compute_digit_logits(decoder_hidden[:, d, :], digit=d)
                logits.append(logits_d)
            logits = torch.stack(logits, dim=1)  # [B, n_digit, codebook_size]
        
        output = ModelOutput()
        output.hidden_states = decoder_hidden
        output.logits = logits
        output.past_key_values = present_key_values
        
        return output

    def generate(self, batch, n_return_sequences=1, mode="confidence"):
        """
        ä½¿ç”¨å‘é‡åŒ–è¿­ä»£å¼æ©ç å¡«å……è¿›è¡Œæ¨ç†ç”Ÿæˆ
        
        Args:
            batch: åŒ…å«encoderè¾“å…¥çš„æ‰¹æ¬¡æ•°æ®
            n_return_sequences: è¿”å›åºåˆ—æ•°é‡
            mode: "confidence" æˆ– "random"
        
        Returns:
            generated_sequences: [B, top_k_final, n_digit]
        """
        from .beam import fast_beam_search_for_eval
        
        # ğŸš€ ç¡®ä¿æ¨ç†æ—¶ä½¿ç”¨evalæ¨¡å¼ï¼Œå…³é—­dropout
        was_training = self.training
        self.eval()
        
        try:
            # è·å–encoderè¾“å‡º
            with torch.no_grad():
                encoder_outputs = self.forward(batch, return_loss=False)
                encoder_hidden = encoder_outputs.hidden_states
                
                # ä½¿ç”¨å¿«é€Ÿå‘é‡åŒ–beam search
                generated_sequences = fast_beam_search_for_eval(
                    model=self,
                    encoder_hidden=encoder_hidden,
                    beam_size=n_return_sequences,
                    max_len=self.n_digit,
                    tokenizer=self.tokenizer,
                    mode=mode,
                    rand_cfg=self.config.get("random_beam", {})
                )
            
            return generated_sequences
            
        finally:
            # æ¢å¤åŸå§‹è®­ç»ƒçŠ¶æ€ï¼Œé¿å…å½±å“åç»­è®­ç»ƒ
            if was_training:
                self.train()

    def _init_weights(self, module):
        """Initialize the weights"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, (nn.LayerNorm, nn.RMSNorm)):
            # LN: æœ‰ biasï¼›RMSNorm: åªæœ‰ weightï¼ˆæ—  biasï¼‰
            if hasattr(module, "bias") and module.bias is not None:
                torch.nn.init.zeros_(module.bias)
            if hasattr(module, "weight") and module.weight is not None:
                torch.nn.init.ones_(module.weight)
        # æ³¨æ„ï¼šoutput_adapterå¦‚æœæ˜¯Identity()ï¼Œä¸éœ€è¦åˆå§‹åŒ– 