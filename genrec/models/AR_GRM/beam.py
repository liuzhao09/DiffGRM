# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import torch.nn.functional as F


def _beam_step_select(mode,
                      logp_matrix,      # [B, act, n_digit*VOC]
                      cur_beam_logp,    # [B, act]
                      beam_ids,         # [B, act, n_digit]  (çˆ¶èŠ‚ç‚¹)
                      n_digit, VOC, beam_act,
                      rand_cfg):
    """
    ç»Ÿä¸€çš„ä¸€æ­¥åˆ†æ”¯é€‰æ‹©é€»è¾‘
    
    Args:
        mode: "confidence" æˆ– "random"
        logp_matrix: å½“å‰æ­¥éª¤çš„logæ¦‚ç‡çŸ©é˜µ [B, act, n_digit*VOC]
        cur_beam_logp: å½“å‰beamçš„logæ¦‚ç‡ [B, act]
        beam_ids: å½“å‰beamçš„tokenåºåˆ— [B, act, n_digit]
        n_digit: æ•°å­—ä½æ•°
        VOC: è¯æ±‡è¡¨å¤§å°
        beam_act: æ´»è·ƒbeamæ•°é‡
        rand_cfg: éšæœºé‡‡æ ·é…ç½®å­—å…¸
    
    Returns:
        next_lp: ä¸‹ä¸€æ­¥çš„logæ¦‚ç‡ [B, act]
        next_ids: ä¸‹ä¸€æ­¥çš„tokenåºåˆ— [B, act, n_digit]
    """
    B = logp_matrix.size(0)

    if mode == "confidence":
        # ç½®ä¿¡åº¦æ¨¡å¼ï¼šé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è·¯å¾„
        cand_lp  = cur_beam_logp.unsqueeze(-1) + logp_matrix      # logP
        flat_lp  = cand_lp.view(B, -1)
        best_lp, flat_idx = torch.topk(flat_lp, k=beam_act)       # [B, act]
    else:   # "random"
        # éšæœºæ¨¡å¼ï¼šä½¿ç”¨temperatureå’Œtop-p/top-ké‡‡æ ·
        temperature = rand_cfg.get("temperature", 1.0)
        logits = (cur_beam_logp.unsqueeze(-1) + logp_matrix) / temperature      # [B, act, *]

        # top-kæˆªæ–­
        top_k = rand_cfg.get("top_k")
        if top_k is not None:
            kth_vals, _ = logits.topk(top_k, dim=-1)
            min_valid   = kth_vals[..., -1:].detach()
            logits      = torch.where(logits < min_valid, logits.new_full((), -1e9), logits)

        # top-p (nucleus) é‡‡æ ·
        top_p = rand_cfg.get("top_p")
        if top_p is not None:
            sorted_logits, _ = torch.sort(logits, descending=True, dim=-1)
            cumsum_probs = torch.cumsum(sorted_logits.softmax(-1), dim=-1)
            mask = cumsum_probs > top_p
            # æŠŠä¸åœ¨ nucleus çš„ logits ç½® -inf
            first_mask = mask[..., 0:1].expand_as(mask)
            nucleus_mask = torch.where(mask, first_mask, mask)
            logits = torch.where(nucleus_mask, logits.new_full((), -1e9), logits)

        probs = torch.softmax(logits, dim=-1)                   # çœŸæ¦‚ç‡
        flat_prob = probs.view(B, -1)

        # å›ºå®š seedï¼ˆå¯é€‰ï¼‰
        seed = rand_cfg.get("seed")
        if seed is not None:
            torch.manual_seed(seed)

        flat_idx = torch.multinomial(flat_prob, beam_act, replacement=False)  # [B, act]
        idx_rows = torch.arange(B, device=flat_idx.device).unsqueeze(1)
        best_lp  = logits.view(B, -1)[idx_rows, flat_idx]        # å¯¹åº” logP
    # -------------------------------------------------------------------------

    parent   = flat_idx // (n_digit * VOC)
    remain   = flat_idx %  (n_digit * VOC)
    d_pos    = remain // VOC
    tok      = remain %  VOC

    batch_idx = torch.arange(B, device=beam_ids.device).unsqueeze(1)
    next_ids  = beam_ids[batch_idx, parent].clone()
    next_ids.scatter_(2, d_pos.unsqueeze(-1), tok.unsqueeze(-1))
    return best_lp, next_ids


def expand_cross_kv_for_beams(initial_kv_cache, beam_size):
    """
    æŠŠç¬¬ä¸€æ­¥å¾—åˆ°çš„cross-KVå¤åˆ¶åˆ°æ¯ä¸ªbeamï¼Œself-KVä»è®¾ä¸ºNoneï¼›
    è¿™æ ·DecoderBlockçš„è‡ªæ³¨æ„KVä¼šç»§ç»­ç´¯åŠ ï¼Œè€Œcross-KVä¸ä¼šé‡å¤è®¡ç®—ã€‚
    
    Args:
        initial_kv_cache: åˆå§‹KV cache
        beam_size: beamå¤§å°
    
    Returns:
        æ‰©å±•åçš„KV cache
    """
    if initial_kv_cache is None:
        return None

    expanded = []
    for layer_cache in initial_kv_cache:
        if layer_cache is None:
            expanded.append(None)
            continue

        self_kv, cross_kv = layer_cache        # self_kvä»…ç¬¬ä¸€æ­¥æœ‰ï¼Œåç»­é cacheç´¯åŠ 
        if cross_kv is not None:
            k, v = cross_kv                    # [B, S, d]
            k = k.unsqueeze(1).repeat(1, beam_size, 1, 1).view(-1, *k.shape[1:])
            v = v.unsqueeze(1).repeat(1, beam_size, 1, 1).view(-1, *v.shape[1:])
            cross_kv = (k, v)
        # âš  self_kvè®¾Noneï¼Œé¿å…æŠŠç¬¬ä¸€æ­¥decoderçš„tokené‡å¤broadcast
        expanded.append((None, cross_kv))
    return expanded



def iterative_mask_decode(model, encoder_hidden, n_return_sequences=1, tokenizer=None, mode="confidence", rand_cfg=None):
    """
    å‘é‡åŒ–è¿­ä»£å¼æ©ç å¡«å……è§£ç ï¼Œå®Œå…¨æ¶ˆé™¤Pythonå¾ªç¯ç“¶é¢ˆ
    
    Args:
        model: DIFF_GRMæ¨¡å‹
        encoder_hidden: encoderè¾“å‡º [B, seq_len, emb_dim]
        n_return_sequences: è¿”å›åºåˆ—æ•°é‡(ä¼šè¢«top_k_finalè¦†ç›–)
        tokenizer: tokenizerå¯¹è±¡
        mode: "confidence" æˆ– "random"
        rand_cfg: éšæœºé‡‡æ ·é…ç½®å­—å…¸
    
    Returns:
        generated_sequences: [B, top_k_final, n_digit] ç”Ÿæˆçš„åºåˆ—
    """
    device = encoder_hidden.device
    batch_size = encoder_hidden.size(0)
    n_digit = model.n_digit
    codebook_size = model.codebook_size
    
    # ğŸš€ ä»é…ç½®ä¸­è·å–å‘é‡åŒ–beam searchå‚æ•°ï¼ˆæ”¯æŒsplit-specificé…ç½®ï¼‰
    if hasattr(model, 'config') and 'vectorized_beam_search' in model.config:
        beam_config = model.config['vectorized_beam_search']
        
        # è·å–å½“å‰splitï¼ˆé»˜è®¤ä¸ºvalï¼‰
        split = model.config.get("current_split", "val")   # "val" / "test"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºsplit-specificé…ç½®ï¼ˆæ”¯æŒä¸‰ç§å†™æ³•ï¼‰
        if split in beam_config:                           # â† å…ˆæŸ¥ split-specific
            BEAM_ACT = int(beam_config[split]["beam_act"])
            BEAM_MAX = int(beam_config[split]["beam_max"])
        elif isinstance(beam_config.get("beam_act"), dict): # å…¼å®¹å¦ä¸€ç§å†™æ³•ï¼šbeam_act æœ¬èº«å°±æ˜¯ dict
            BEAM_ACT = int(beam_config["beam_act"].get(split,
                                                       beam_config["beam_act"]["val"]))
            BEAM_MAX = int(beam_config["beam_max"].get(split,
                                                       beam_config["beam_max"]["val"]))
        else:                                              # æœ€åæ‰è½å›å…¨å±€
            BEAM_ACT = int(beam_config["beam_act"])
            BEAM_MAX = int(beam_config["beam_max"])
        
        TOP_K_FINAL = min(int(beam_config['top_k_final']), n_return_sequences)
        # ğŸš€ ä¿®å¤ï¼šç¡®ä¿NEG_INFå€¼æ˜¯floatç±»å‹ï¼Œé¿å…YAMLå­—ç¬¦ä¸²é—®é¢˜
        NEG_INF_FP32 = float(beam_config['neg_inf_fp32'])
        NEG_INF_FP16 = float(beam_config['neg_inf_fp16'])
        # ç¡®ä¿beam_actä¸è¶…è¿‡beam_max
        assert BEAM_ACT <= BEAM_MAX, "beam_act should not exceed beam_max"
    else:
        # ğŸš€ ä¿®å¤ï¼šé…ç½®ç»Ÿä¸€ï¼Œä¸å†æœ‰fallback
        raise ValueError("Missing 'vectorized_beam_search' configuration in model.config")
    
    # ---------- â‘  è§£æ beam_sizeï¼ˆrandomæ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼‰ ----------
    if mode == "random":
        # å¦‚æœ random_beam æŒ‡å®šäº† beam_act/beam_max å°±è¦†ç›–
        rb_cfg = model.config.get("random_beam", {})
        BEAM_ACT = int(rb_cfg.get("beam_act", BEAM_ACT))
        BEAM_MAX = int(rb_cfg.get("beam_max", BEAM_MAX))
        # ç¡®ä¿beam_actä¸è¶…è¿‡beam_max
        assert BEAM_ACT <= BEAM_MAX, "random_beam.beam_act should not exceed random_beam.beam_max"
    
    # ---------- â‘¡ éšæœºä¸€æ¬¡åˆ—é¡ºåºï¼ˆä»…randomæ¨¡å¼ï¼‰ ----------
    decode_order = None
    if mode == "random":
        seed = model.config.get("random_beam", {}).get("seed")
        if seed is not None:
            torch.manual_seed(seed)
        decode_order = torch.randperm(n_digit).tolist()      # e.g. [1,5,3,7,0,2,6,4]
        if batch_size == 1:  # åªåœ¨å•æ ·æœ¬æ—¶æ‰“å°ï¼Œé¿å…å¤šworkeråˆ·å±
            print(f"[RANDOM_BEAM] ğŸ² Decode order: {decode_order}")
    
    # å¸¸é‡
    MASK_ID = tokenizer.mask_token if tokenizer is not None else -1
    VOC = codebook_size
    
    # å‡å°‘æ—¥å¿—å™ªéŸ³
    if batch_size == 1:  # åªåœ¨å•æ ·æœ¬æ—¶æ‰“å°ï¼Œé¿å…å¤šworkeråˆ·å±
        print(f"[VECTORIZED_BEAM] ğŸš€ Using optimized beam search:")
        print(f"[VECTORIZED_BEAM] BEAM_ACT: {BEAM_ACT}, BEAM_MAX: {BEAM_MAX}, TOP_K_FINAL: {TOP_K_FINAL}")
    
    # Step 0: å…¨æ©ç é¢„æµ‹ï¼Œè·å–æ‰€æœ‰ä½ç½®çš„æ¦‚ç‡
    with torch.no_grad():
        # æ„å»ºmask_positionsï¼šå…¨1è¡¨ç¤ºå…¨éƒ¨è¢«æ©ç 
        mask_positions = torch.ones(batch_size, n_digit, device=device)
        
        # æ„å»ºbatch
        batch_dict = {
            'decoder_input_ids': torch.zeros(batch_size, n_digit, device=device, dtype=torch.long),
            'encoder_hidden': encoder_hidden,
            'mask_positions': mask_positions
        }
        
        # å‰å‘ä¼ æ’­ - å¯ç”¨KV cacheä»¥åŠ é€Ÿåç»­æ¨ç†
        outputs = model.forward_decoder_only(batch_dict, digit=None, use_cache=True)
        all_logits = outputs.logits  # [B, n_digit, codebook_size]
        initial_kv_cache = outputs.past_key_values  # ä¿å­˜åˆå§‹KV cache
        
        # è®¡ç®—log probabilities
        all_log_probs = F.log_softmax(all_logits, dim=-1)  # [B, n_digit, codebook_size]
        
        if mode == "random":
            # === randomæ¨¡å¼ï¼šåªçœ‹ç¬¬ä¸€åˆ— ===
            first_col = decode_order[0]
            probs_col = all_log_probs[:, first_col, :]          # [B, VOC]
            top_k_probs, top_k_idx = torch.topk(probs_col, k=BEAM_ACT, dim=-1)  # [B, BEAM_ACT]
            
            # è§£æä½ç½®å’Œtoken
            first_col_tensor = torch.full((batch_size, BEAM_ACT), first_col, device=device, dtype=torch.long)
            first_token = top_k_idx
        else:
            # === confidenceæ¨¡å¼ï¼šå…¨å±€top-k ===
            # æ‹¼æ¥æ‰€æœ‰ä½ç½®çš„æ¦‚ç‡: [B, n_digit * codebook_size]  
            flattened_log_probs = all_log_probs.view(batch_size, -1)
            
            # å–top BEAM_ACTä¸ªå€™é€‰
            top_k_probs, top_k_indices = torch.topk(flattened_log_probs, k=BEAM_ACT)
            
            # è§£æä½ç½®å’Œtoken
            first_col_tensor = top_k_indices // VOC      # ç¬¬å‡ ä¸ªdigit [B, BEAM_ACT]
            first_token = top_k_indices % VOC     # codebookå†…çš„ID [B, BEAM_ACT]
        
        # ğŸš€ å›ºå®šå¤§å°beam tensorï¼Œä¸€æ¬¡åˆ†é…ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
        beam_ids = torch.full((batch_size, BEAM_MAX, n_digit), MASK_ID, 
                             dtype=torch.long, device=device)
        
        # ç¡®å®šNEG_INFå€¼
        NEG_INF = NEG_INF_FP16 if top_k_probs.dtype == torch.float16 else NEG_INF_FP32
        beam_logp = torch.full((batch_size, BEAM_MAX), NEG_INF, 
                              dtype=top_k_probs.dtype, device=device)
        
        # å¡«å……ç¬¬ä¸€æ­¥çš„ç»“æœ
        batch_indices = torch.arange(batch_size, device=device).unsqueeze(1)  # [B, 1]
        beam_indices = torch.arange(BEAM_ACT, device=device).unsqueeze(0)     # [1, BEAM_ACT]
        
        beam_ids[batch_indices, beam_indices, first_col_tensor] = first_token
        beam_logp[:, :BEAM_ACT] = top_k_probs
        
        # ğŸš€ ä¿®å¤ï¼šæ‰©å±•åˆ°BEAM_MAXç¡®ä¿å……è¶³å®¹é‡
        encoder_hidden_expanded = encoder_hidden.unsqueeze(1).repeat(1, BEAM_MAX, 1, 1)
        encoder_hidden_expanded = encoder_hidden_expanded.view(-1, encoder_hidden.size(1), encoder_hidden.size(2))
        
        # Step-0ç»“æŸåï¼Œç”Ÿæˆä¸€æ¬¡broadcaståçš„cacheä¾›åç»­å¤ç”¨
        kv_cache_for_act = expand_cross_kv_for_beams(initial_kv_cache, BEAM_ACT)
        kv_cache_final = expand_cross_kv_for_beams(initial_kv_cache, BEAM_ACT)  # ç”¨äºæœ€åä¸€æ­¥
    
    # Steps 1-2: å‘é‡åŒ–beamæ‰©å±•ï¼ˆå®Œå…¨æ¶ˆé™¤Pythonå¾ªç¯ï¼‰
    if mode == "random":
        # === randomæ¨¡å¼ï¼šæŒ‰decode_orderå¾ªç¯ ===
        for step, cur_col in enumerate(decode_order[1:], 1):
            with torch.no_grad():
                # åªä½¿ç”¨å‰BEAM_ACTä¸ªæœ‰æ•ˆbeam
                active_beam_ids = beam_ids[:, :BEAM_ACT, :]      # [B, BEAM_ACT, n_digit]
                active_beam_logp = beam_logp[:, :BEAM_ACT]       # [B, BEAM_ACT]
                
                # æ„å»ºå½“å‰çŠ¶æ€çš„mask_positions
                mask_positions = (active_beam_ids == MASK_ID).float()  # [B, BEAM_ACT, n_digit]
                
                # é‡å¡‘ä¸ºdecoderè¾“å…¥æ ¼å¼
                decoder_input = torch.clamp(active_beam_ids, min=0).view(-1, n_digit)  # [B*BEAM_ACT, n_digit]
                mask_pos_flat = mask_positions.view(-1, n_digit)  # [B*BEAM_ACT, n_digit]
                
                # ğŸš€ ä½¿ç”¨é¢„ç”Ÿæˆçš„KV cacheï¼Œå®ç°çœŸæ­£çš„cacheå¤ç”¨
                expanded_kv_cache = kv_cache_for_act
                
                # æ„å»ºbatch
                batch_dict = {
                    'decoder_input_ids': decoder_input,
                    'encoder_hidden': encoder_hidden_expanded[:batch_size * BEAM_ACT],  # åªä½¿ç”¨å‰BEAM_ACTéƒ¨åˆ†
                    'mask_positions': mask_pos_flat
                }
                
                # å‰å‘ä¼ æ’­
                outputs = model.forward_decoder_only(batch_dict, digit=None, 
                                                   past_key_values=expanded_kv_cache, use_cache=True)
                all_logits = outputs.logits  # [B*BEAM_ACT, n_digit, codebook_size]
                
                # é‡å¡‘ä¸ºbeamç»´åº¦
                all_logits = all_logits.view(batch_size, BEAM_ACT, n_digit, codebook_size)
                
                # ğŸš€ å‘é‡åŒ–æ©ç å¤„ç†ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
                all_log_probs = F.log_softmax(all_logits, dim=-1)
                
                # åªè€ƒè™‘è¢«æ©ç çš„ä½ç½®
                mask_expanded = mask_positions.unsqueeze(-1)  # [B, BEAM_ACT, n_digit, 1]
                masked_log_probs = all_log_probs + (1 - mask_expanded) * NEG_INF
                
                # === randomæ¨¡å¼ï¼šåªçœ‹å½“å‰åˆ— ===
                logits = masked_log_probs[:, :, cur_col, :]                     # [B, BEAM_ACT, VOC]
                
                joint_lp = logits + active_beam_logp.unsqueeze(-1)              # [B, BEAM_ACT, VOC]
                flat_lp  = joint_lp.view(batch_size, -1)                        # [B, BEAM_ACT*VOC]
                best_lp, flat_idx = torch.topk(flat_lp, k=BEAM_ACT)            # â† top-kï¼Œä¸é‡‡æ ·
                
                # è§£æç´¢å¼•
                parent_beam_ids = flat_idx // VOC                               # [B, BEAM_ACT]
                token_ids = flat_idx % VOC                                      # [B, BEAM_ACT]
                
                # æ›´æ–°beam
                batch_range = torch.arange(batch_size, device=device).unsqueeze(1)  # [B, 1]
                new_beam_ids = active_beam_ids[batch_range, parent_beam_ids]        # [B, BEAM_ACT, n_digit]
                new_beam_ids.scatter_(2, torch.full((batch_size, BEAM_ACT), cur_col, device=device, dtype=torch.long).unsqueeze(-1), token_ids.unsqueeze(-1))
                
                # æ›´æ–°beamçŠ¶æ€
                beam_ids[:, :BEAM_ACT, :] = new_beam_ids
                beam_logp[:, :BEAM_ACT] = best_lp
                
                # æ¸…ç©ºæ— æ•ˆbeamï¼ˆä¿æŒBEAM_MAXå¤§å°ï¼‰
                if BEAM_ACT < BEAM_MAX:
                    beam_ids[:, BEAM_ACT:, :] = MASK_ID
                    beam_logp[:, BEAM_ACT:] = NEG_INF
    else:
        # === confidenceæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘ ===
        for step in range(1, n_digit - 1):
            with torch.no_grad():
                # åªä½¿ç”¨å‰BEAM_ACTä¸ªæœ‰æ•ˆbeam
                active_beam_ids = beam_ids[:, :BEAM_ACT, :]      # [B, BEAM_ACT, n_digit]
                active_beam_logp = beam_logp[:, :BEAM_ACT]       # [B, BEAM_ACT]
                
                # æ„å»ºå½“å‰çŠ¶æ€çš„mask_positions
                mask_positions = (active_beam_ids == MASK_ID).float()  # [B, BEAM_ACT, n_digit]
                
                # é‡å¡‘ä¸ºdecoderè¾“å…¥æ ¼å¼
                decoder_input = torch.clamp(active_beam_ids, min=0).view(-1, n_digit)  # [B*BEAM_ACT, n_digit]
                mask_pos_flat = mask_positions.view(-1, n_digit)  # [B*BEAM_ACT, n_digit]
                
                # ğŸš€ ä½¿ç”¨é¢„ç”Ÿæˆçš„KV cacheï¼Œå®ç°çœŸæ­£çš„cacheå¤ç”¨
                expanded_kv_cache = kv_cache_for_act
                
                # æ„å»ºbatch
                batch_dict = {
                    'decoder_input_ids': decoder_input,
                    'encoder_hidden': encoder_hidden_expanded[:batch_size * BEAM_ACT],  # åªä½¿ç”¨å‰BEAM_ACTéƒ¨åˆ†
                    'mask_positions': mask_pos_flat
                }
                
                # å‰å‘ä¼ æ’­
                outputs = model.forward_decoder_only(batch_dict, digit=None, 
                                                   past_key_values=expanded_kv_cache, use_cache=True)
                all_logits = outputs.logits  # [B*BEAM_ACT, n_digit, codebook_size]
                
                # é‡å¡‘ä¸ºbeamç»´åº¦
                all_logits = all_logits.view(batch_size, BEAM_ACT, n_digit, codebook_size)
                
                # ğŸš€ å‘é‡åŒ–æ©ç å¤„ç†ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
                all_log_probs = F.log_softmax(all_logits, dim=-1)
                
                # åªè€ƒè™‘è¢«æ©ç çš„ä½ç½®
                mask_expanded = mask_positions.unsqueeze(-1)  # [B, BEAM_ACT, n_digit, 1]
                masked_log_probs = all_log_probs + (1 - mask_expanded) * NEG_INF
                
                # æ‹¼æ¥æ‰€æœ‰å¯èƒ½çš„å€™é€‰ï¼š[B, BEAM_ACT, n_digit * codebook_size]
                flattened_log_probs = masked_log_probs.view(batch_size, BEAM_ACT, -1)
                
                # ğŸš€ ä½¿ç”¨ç»Ÿä¸€çš„åˆ†æ”¯é€‰æ‹©é€»è¾‘
                best_logprobs, new_beam_ids = _beam_step_select(
                    mode=mode,
                    logp_matrix=flattened_log_probs,          # [B, act, n_digit*VOC]
                    cur_beam_logp=active_beam_logp,           # [B, act]
                    beam_ids=active_beam_ids,                 # [B, act, n_digit]
                    n_digit=n_digit, VOC=VOC, beam_act=BEAM_ACT,
                    rand_cfg=rand_cfg or {}
                )
                
                # æ›´æ–°beamçŠ¶æ€
                beam_ids[:, :BEAM_ACT, :] = new_beam_ids
                beam_logp[:, :BEAM_ACT] = best_logprobs
                
                # æ¸…ç©ºæ— æ•ˆbeamï¼ˆä¿æŒBEAM_MAXå¤§å°ï¼‰
                if BEAM_ACT < BEAM_MAX:
                    beam_ids[:, BEAM_ACT:, :] = MASK_ID
                    beam_logp[:, BEAM_ACT:] = NEG_INF
    
    # æœ€ç»ˆæ­¥éª¤: å¡«å……æœ€åä¸€ä¸ªä½ç½®å¹¶é€‰æ‹©top-K
    with torch.no_grad():
        if mode == "random":
            # === randomæ¨¡å¼ï¼šå·²ç»é€šè¿‡å¾ªç¯å¡«å®Œäº†æ‰€æœ‰ä½ç½®ï¼Œç›´æ¥ä½¿ç”¨å½“å‰ç»“æœ ===
            active_beam_ids = beam_ids[:, :BEAM_ACT, :]
            final_beam_logp = beam_logp[:, :BEAM_ACT]
        else:
            # === confidenceæ¨¡å¼ï¼šéœ€è¦å¡«å……æœ€åä¸€ä¸ªä½ç½® ===
            # åªå¤„ç†å‰BEAM_ACTä¸ªbeam
            active_beam_ids = beam_ids[:, :BEAM_ACT, :]
            active_beam_logp = beam_logp[:, :BEAM_ACT]
            
            # æ‰¾åˆ°æ¯ä¸ªbeamçš„æœ€åä¸€ä¸ªMASKä½ç½®
            mask_positions = (active_beam_ids == MASK_ID).float()
            
            # æ„å»ºdecoderè¾“å…¥
            decoder_input = torch.clamp(active_beam_ids, min=0).view(-1, n_digit)
            mask_pos_flat = mask_positions.view(-1, n_digit)
            
            # ä½¿ç”¨é¢„ç”Ÿæˆçš„KV cacheç”¨äºæœ€ç»ˆæ­¥éª¤
            final_expanded_kv_cache = kv_cache_final
            
            batch_dict = {
                'decoder_input_ids': decoder_input,
                'encoder_hidden': encoder_hidden_expanded[:batch_size * BEAM_ACT],  # åªä½¿ç”¨å‰BEAM_ACTéƒ¨åˆ†
                'mask_positions': mask_pos_flat
            }
            
            # è·å–æ‰€æœ‰ä½ç½®çš„logits
            outputs = model.forward_decoder_only(batch_dict, digit=None, 
                                               past_key_values=final_expanded_kv_cache, use_cache=True)
            all_logits = outputs.logits  # [B*BEAM_ACT, n_digit, codebook_size]
            
            # é‡å¡‘å¹¶è®¡ç®—log probs
            all_logits = all_logits.view(batch_size, BEAM_ACT, n_digit, codebook_size)
            all_log_probs = F.log_softmax(all_logits, dim=-1)
            
            # æ‰¾åˆ°æ¯ä¸ªbeaméœ€è¦å¡«å……çš„æœ€åä¸€ä¸ªä½ç½®
            last_mask_pos = torch.argmax(mask_positions.float(), dim=-1)  # [B, BEAM_ACT]
            
            # ä¸ºæ¯ä¸ªbeamé€‰æ‹©å¯¹åº”ä½ç½®çš„æœ€ä½³token
            batch_idx = torch.arange(batch_size, device=device).unsqueeze(1).expand(-1, BEAM_ACT)
            beam_idx = torch.arange(BEAM_ACT, device=device).unsqueeze(0).expand(batch_size, -1)
            
            final_logits = all_log_probs[batch_idx, beam_idx, last_mask_pos]  # [B, BEAM_ACT, codebook_size]
            best_token_logprobs, best_tokens = torch.max(final_logits, dim=-1)  # [B, BEAM_ACT]
            
            # æ›´æ–°æœ€åçš„token
            active_beam_ids.scatter_(2, last_mask_pos.unsqueeze(-1), best_tokens.unsqueeze(-1))
            final_beam_logp = active_beam_logp + best_token_logprobs
        
        # ğŸš€ çµæ´»çš„å»é‡ç­–ç•¥
        dedup_strategy = "simple"  # é»˜è®¤ä½¿ç”¨ simple å»é‡
        if hasattr(model, 'config') and 'dedup_strategy' in model.config:
            dedup_strategy = model.config['dedup_strategy']
        
        if dedup_strategy == "none":
            # ç­–ç•¥1: ä¸å»é‡ï¼Œç›´æ¥é€‰æ‹©top-K
            top_logprobs, top_indices = torch.topk(final_beam_logp, k=min(TOP_K_FINAL, BEAM_ACT), dim=-1)
            batch_range = torch.arange(batch_size, device=device).unsqueeze(1)
            final_sequences = active_beam_ids[batch_range, top_indices]  # [B, TOP_K_FINAL, n_digit]
            
            if batch_size == 1:
                print(f"[VECTORIZED_BEAM] âœ… Generated {final_sequences.shape[1]} sequences (no deduplication)")
                
        elif dedup_strategy == "simple":
            # ç­–ç•¥2: ç®€å•å»é‡ + åˆæ³•æ€§æ£€æŸ¥ï¼ˆæ”¹è¿›çš„æ–¹æ³•ï¼‰
            # â‘  éœ€è¦ tokenizer ä¼ è¿›æ¥
            assert tokenizer is not None, "tokenizer is required for legality check"
            
            final_sequences = []
            for b in range(batch_size):
                batch_sequences = active_beam_ids[b]  # [BEAM_ACT, n_digit]
                batch_logprobs = final_beam_logp[b]   # [BEAM_ACT]
                
                # æŒ‰æ¦‚ç‡æ’åºï¼Œç„¶åç®€å•å»é‡ + åˆæ³•æ€§æ£€æŸ¥
                sorted_indices = torch.argsort(batch_logprobs, descending=True)
                unique_sequences = []
                
                for idx in sorted_indices:
                    seq = batch_sequences[idx]
                    # --------- æ–°å¢ï¼šåˆæ³•æ€§æ£€æŸ¥ ----------
                    is_legal = tokenizer.codebooks_to_item_id(seq.tolist()) is not None
                    if not is_legal:
                        continue  # ç›´æ¥è·³è¿‡éæ³•åºåˆ—
                    # ------------------------------------
                    is_duplicate = any(torch.equal(seq, existing) for existing in unique_sequences)
                    if not is_duplicate:
                        unique_sequences.append(seq)
                        if len(unique_sequences) >= TOP_K_FINAL:
                            break
                            
                # å¡«å……ä¸è¶³çš„éƒ¨åˆ†ï¼ˆç¡®ä¿å¡«å……çš„åºåˆ—ä¹Ÿæ˜¯åˆæ³•çš„ï¼‰
                while len(unique_sequences) < TOP_K_FINAL:
                    if unique_sequences:
                        # å¦‚æœæœ‰åˆæ³•åºåˆ—ï¼Œé‡å¤æœ€åä¸€ä¸ª
                        unique_sequences.append(unique_sequences[-1])
                    else:
                        # å¦‚æœæ²¡æœ‰åˆæ³•åºåˆ—ï¼Œæ‰¾ä¸€ä¸ªåˆæ³•çš„å¡«å……
                        for idx in range(BEAM_ACT):
                            seq = batch_sequences[idx]
                            if tokenizer.codebooks_to_item_id(seq.tolist()) is not None:
                                unique_sequences.append(seq)
                                break
                        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°åˆæ³•åºåˆ—ï¼Œç”¨ç¬¬ä¸€ä¸ªï¼ˆè™½ç„¶ä¸åˆæ³•ï¼Œä½†æ€»æ¯”å´©æºƒå¥½ï¼‰
                        if not unique_sequences:
                            unique_sequences.append(batch_sequences[0])
                
                batch_final = torch.stack(unique_sequences[:TOP_K_FINAL])
                final_sequences.append(batch_final)
            
            final_sequences = torch.stack(final_sequences)
            if batch_size == 1:
                print(f"[VECTORIZED_BEAM] âœ… Generated {final_sequences.shape[1]} unique sequences (simple deduplication + legality check)")
                
        else:  # weighted
            # ç­–ç•¥3: æ¦‚ç‡åŠ æƒå»é‡ + åˆæ³•æ€§æ£€æŸ¥ï¼ˆæ”¹è¿›çš„æ–¹æ³•ï¼‰
            # â‘  éœ€è¦ tokenizer ä¼ è¿›æ¥
            assert tokenizer is not None, "tokenizer is required for legality check"
            
            final_sequences = []
            for b in range(batch_size):
                batch_sequences = active_beam_ids[b]  # [BEAM_ACT, n_digit]
                batch_logprobs = final_beam_logp[b]   # [BEAM_ACT]
                
                # æ„å»ºåºåˆ—åˆ°æ¦‚ç‡çš„æ˜ å°„ï¼Œç´¯åŠ é‡å¤åºåˆ—çš„æ¦‚ç‡ï¼ˆåªè€ƒè™‘åˆæ³•åºåˆ—ï¼‰
                seq_to_logprob = {}
                for i in range(BEAM_ACT):
                    seq_tuple = tuple(batch_sequences[i].cpu().tolist())
                    # --------- æ–°å¢ï¼šåˆæ³•æ€§æ£€æŸ¥ ----------
                    is_legal = tokenizer.codebooks_to_item_id(list(seq_tuple)) is not None
                    if not is_legal:
                        continue  # ç›´æ¥è·³è¿‡éæ³•åºåˆ—
                    # ------------------------------------
                    if seq_tuple in seq_to_logprob:
                        # é‡å¤åºåˆ—ï¼šä½¿ç”¨log-sum-expç´¯åŠ æ¦‚ç‡ï¼ˆæ›´ç¨³å®šï¼‰
                        seq_to_logprob[seq_tuple] = torch.logaddexp(
                            seq_to_logprob[seq_tuple], 
                            batch_logprobs[i]
                        )
                    else:
                        seq_to_logprob[seq_tuple] = batch_logprobs[i]
                
                # æŒ‰ç´¯åŠ åçš„æ¦‚ç‡æ’åº
                sorted_items = sorted(seq_to_logprob.items(), 
                                    key=lambda x: x[1].item(), reverse=True)
                
                # é€‰æ‹©å‰TOP_K_FINALä¸ªä¸é‡å¤åºåˆ—ï¼ˆå·²ç»æŒ‰åŠ æƒæ¦‚ç‡æ’åºï¼‰
                unique_sequences = []
                for seq_tuple, _ in sorted_items[:TOP_K_FINAL]:
                    seq_tensor = torch.tensor(seq_tuple, device=device, dtype=torch.long)
                    unique_sequences.append(seq_tensor)
                
                # å¡«å……ä¸è¶³çš„éƒ¨åˆ†ï¼ˆç¡®ä¿å¡«å……çš„åºåˆ—ä¹Ÿæ˜¯åˆæ³•çš„ï¼‰
                while len(unique_sequences) < TOP_K_FINAL:
                    if unique_sequences:
                        # å¦‚æœæœ‰åˆæ³•åºåˆ—ï¼Œé‡å¤æœ€åä¸€ä¸ª
                        unique_sequences.append(unique_sequences[-1])
                    else:
                        # å¦‚æœæ²¡æœ‰åˆæ³•åºåˆ—ï¼Œæ‰¾ä¸€ä¸ªåˆæ³•çš„å¡«å……
                        for idx in range(BEAM_ACT):
                            seq = batch_sequences[idx]
                            if tokenizer.codebooks_to_item_id(seq.tolist()) is not None:
                                unique_sequences.append(seq)
                                break
                        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°åˆæ³•åºåˆ—ï¼Œç”¨ç¬¬ä¸€ä¸ªï¼ˆè™½ç„¶ä¸åˆæ³•ï¼Œä½†æ€»æ¯”å´©æºƒå¥½ï¼‰
                        if not unique_sequences:
                            unique_sequences.append(batch_sequences[0])
                
                batch_final = torch.stack(unique_sequences[:TOP_K_FINAL])
                final_sequences.append(batch_final)
            
            final_sequences = torch.stack(final_sequences)
            if batch_size == 1:
                print(f"[VECTORIZED_BEAM] âœ… Generated {final_sequences.shape[1]} unique sequences (probability-weighted deduplication + legality check)")
    
    # ------- è®¡ç®—å½“å‰batchçš„ç»Ÿè®¡ä¿¡æ¯ -------
    if tokenizer is not None:  # ä¸å†é™åˆ¶ batch_size==1
        # ä¿®å¤åˆæ³•ç‡è®¡ç®—ï¼šä½¿ç”¨åºåˆ—æ•°ä½œä¸ºåˆ†æ¯ï¼Œè€Œä¸æ˜¯tokenæ•°
        total_seqs = final_sequences.numel() // n_digit
        legal_final = sum(tokenizer.codebooks_to_item_id(seq.tolist()) is not None
                          for seq in final_sequences.view(-1, n_digit))
        final_legal_ratio = legal_final / total_seqs

        # ä¿®å¤é‡å¤ç‡è®¡ç®—ï¼šä½¿ç”¨æ­£ç¡®çš„å…¬å¼
        unique_seqs = len({tuple(seq.tolist()) for seq in final_sequences.view(-1, n_digit)})
        duplicate_ratio = 1 - unique_seqs / total_seqs

        # è¿”å›ç»Ÿè®¡ä¿¡æ¯ä¾›evaluatorä½¿ç”¨ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰“å°
        return final_sequences, final_legal_ratio, duplicate_ratio
    # --------------------------------
    
    return final_sequences


def fast_beam_search_for_eval(model, encoder_hidden, beam_size=10, max_len=4, tokenizer=None, mode="confidence", rand_cfg=None):
    """
    ä¸“é—¨ç”¨äºéªŒè¯çš„å¿«é€Ÿå‘é‡åŒ–beam search
    é‡‡ç”¨ä¸TensorFlowä¸€è‡´çš„ç­–ç•¥ï¼šå‰3æ­¥å›ºå®š512beamï¼Œæœ€åå–top-K
    
    Args:
        model: DIFF_GRMæ¨¡å‹
        encoder_hidden: Encoderè¾“å‡º [batch_size, seq_len, hidden_dim]
        beam_size: æœ€ç»ˆbeamå¤§å°ï¼ˆä¼šè¢«TOP_K_FINALè¦†ç›–ï¼‰
        max_len: æœ€å¤§ç”Ÿæˆé•¿åº¦
        tokenizer: Tokenizer
        mode: "confidence" æˆ– "random"
        rand_cfg: éšæœºé‡‡æ ·é…ç½®å­—å…¸
    
    Returns:
        torch.Tensor: ç”Ÿæˆçš„tokenåºåˆ— [batch_size, TOP_K_FINAL, max_len]
    """
    # ç›´æ¥è°ƒç”¨å‘é‡åŒ–çš„iterative_mask_decode
    result = iterative_mask_decode(
        model=model,
        encoder_hidden=encoder_hidden,
        n_return_sequences=beam_size,
        tokenizer=tokenizer,
        mode=mode,
        rand_cfg=rand_cfg or {}
    )
    
    # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯å…ƒç»„ï¼ˆåºåˆ—+ç»Ÿè®¡ä¿¡æ¯ï¼‰æˆ–åªæ˜¯åºåˆ—
    if isinstance(result, tuple):
        return result[0]  # åªè¿”å›åºåˆ—éƒ¨åˆ†
    else:
        return result


 