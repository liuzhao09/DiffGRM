# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import os
import math
import json
import pickle
import numpy as np
import torch
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

from genrec.dataset import AbstractDataset
from genrec.tokenizer import AbstractTokenizer


class AR_GRMTokenizer(AbstractTokenizer):
    """
    AR_GRM Tokenizer for autoregressive generative recommendation

    Special tokens:
    - PAD=0, BOS=1, EOS=2, SID_OFFSET=3

    SID Configuration:
    - n_digit: configurable (e.g., 4, 8, 12), codebook_size=256
    - vocab_size = 3 + n_digit * codebook_size
    """
    def __init__(self, config: dict, dataset: AbstractDataset):
        # å…œåº•ï¼Œé¿å… KeyError
        config.setdefault('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        config.setdefault('num_proc', 1)

        self.n_codebook_bits = self._get_codebook_bits(config['codebook_size'])
        
        # ç»Ÿä¸€é‡åŒ–å™¨ï¼šæ”¯æŒ sid_quantizerï¼ˆå…¼å®¹ quantizerï¼‰
        sid_q = config.get('sid_quantizer', None)
        if sid_q is None:
            q_old = config.get('quantizer', 'pq').lower()
            sid_q = {'pq': 'opq_pq', 'rq': 'rq_kmeans'}.get(q_old, 'opq_pq')
        self.sid_quantizer = sid_q
        assert self.sid_quantizer in ('opq_pq', 'rq_kmeans', 'none'), \
            f"sid_quantizer must be one of ['opq_pq','rq_kmeans','none'], got {self.sid_quantizer}"
        
        # ä¸ DIFF_GRM å¯¹é½çš„ index_factory
        if self.sid_quantizer == 'opq_pq':
            use_opq = not config.get('disable_opq', False)
            if use_opq:
                self.index_factory = f'OPQ{config["n_digit"]},IVF1,PQ{config["n_digit"]}x{self.n_codebook_bits}'
            else:
                self.index_factory = f'IVF1,PQ{config["n_digit"]}x{self.n_codebook_bits}'
        elif self.sid_quantizer == 'rq_kmeans':
            self.index_factory = f'RQKMEANS{config["n_digit"]}x{self.n_codebook_bits}'
        else:  # 'none'
            self.index_factory = f'RAND{config["n_digit"]}x{self.n_codebook_bits}'

        # å…ˆåˆå§‹åŒ–çˆ¶ç±»ï¼Œä¿è¯ self.config / self.logger ç­‰å­—æ®µå¯ç”¨
        super(AR_GRMTokenizer, self).__init__(config, dataset)
        
        # ç°åœ¨å†å†™æ—¥å¿—
        self.log(f'[TOKENIZER] Index factory: {self.index_factory}')
        self.dataset = dataset  # æ·»åŠ datasetå¼•ç”¨
        self.item2id = dataset.item2id
        self.id2item = dataset.id_mapping['id2item']
        
        # Special tokens - ç®€åŒ–token IDåˆ†é…
        self.pad_token = 0
        self.bos_token = 1
        self.eos_token = 2
        self.mask_token = -1  # MASK tokenç”¨äºæ¨ç†ï¼Œä¸åœ¨vocabä¸­
        self.sid_offset = 3  # SID tokenä»3å¼€å§‹
        
        self.item2tokens = self._init_tokenizer(dataset)
        
        # Create reverse mapping for inference (å¦‚æœè¿˜æ²¡æœ‰åˆ›å»ºçš„è¯)
        if not hasattr(self, 'tokens2item'):
            self.tokens2item = self._create_reverse_mapping()
        
        # Set collate functions
        # ç›´æ¥ä½¿ç”¨æœ¬ç›®å½•ä¸‹ collate
        from .collate import collate_fn_train, collate_fn_val, collate_fn_test
        self.collate_fn = {
            'train': collate_fn_train,
            'val': collate_fn_val,
            'test': collate_fn_test
        }

    @property
    def n_digit(self):
        return self.config['n_digit']

    @property
    def codebook_size(self):
        return self.config['codebook_size']

    @property
    def max_token_seq_len(self) -> int:
        return 1 + self.n_digit  # [BOS] + n_digit SID tokens

    @property
    def vocab_size(self) -> int:
        return 3 + self.n_digit * self.codebook_size  # PAD(0) + BOS(1) + EOS(2) + SID tokens

    def _get_codebook_bits(self, n_codebook):
        x = math.log2(n_codebook)
        assert x.is_integer() and x >= 0, "Invalid value for n_codebook"
        return int(x)

    def _encode_sent_emb(self, dataset: AbstractDataset, output_path: str):
        """ç¼–ç å¥å­åµŒå…¥ï¼šæ”¯æŒä»»æ„ Hugging Face SentenceTransformer æ¨¡å‹ idï¼Œå¹¶åšå‘é‡å½’ä¸€åŒ–"""
        assert self.config['metadata'] == 'sentence', \
            'AR_GRMTokenizer only supports sentence metadata.'

        meta_sentences = []
        for i in range(1, dataset.n_items):
            meta_sentences.append(dataset.item2meta[dataset.id_mapping['id2item'][i]])

        # æ¥å—ä»»æ„HFæ¨¡å‹idï¼ˆå¦‚ Alibaba-NLP/gte-large-en-v1.5 æˆ– BAAI/bge-large-en-v1.5ï¼‰
        model_id = self.config['sent_emb_model']
        sent_emb_model = SentenceTransformer(model_id, trust_remote_code=True).to(self.config['device'])

        # ç›´æ¥encodeï¼ˆGTE/BGEæ— éœ€å‰ç¼€ï¼‰ï¼Œå¹¶è¿›è¡ŒL2å½’ä¸€åŒ–
        sent_embs = sent_emb_model.encode(
            meta_sentences,
            convert_to_numpy=True,
            batch_size=self.config['sent_emb_batch_size'],
            show_progress_bar=True,
            device=self.config['device'],
            normalize_embeddings=True,
        )

        # æŒ‰æ¨¡å‹basenameåˆ†åˆ«è½ç›˜ï¼Œé¿å…ä¸åŒæ¨¡å‹å†²çª
        sent_embs.tofile(output_path)
        return sent_embs

    def _get_items_for_training(self, dataset: AbstractDataset) -> np.ndarray:
        """è·å–è®­ç»ƒç”¨çš„å•†å“"""
        items_for_training = set()
        
        # é¦–å…ˆè§¦å‘æ•°æ®é›†åˆ†å‰²ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆ†å‰²ï¼‰
        split_data = dataset.split()
        
        # ä»è®­ç»ƒé›†ä¸­æ”¶é›†æ‰€æœ‰items
        if 'train' in split_data:
            train_dataset = split_data['train']
            # train_datasetæ˜¯Hugging Face Datasetå¯¹è±¡
            if hasattr(train_dataset, 'column_names') and 'item_seq' in train_dataset.column_names:
                # éå†æ‰€æœ‰item_seq
                for item_seq in train_dataset['item_seq']:
                    if isinstance(item_seq, (list, tuple)):
                        items_for_training.update(item_seq)
                    else:
                        items_for_training.add(item_seq)
        
        # ä¿®å¤ï¼šç¡®ä¿maskå¤§å°ä¸sent_embsåŒ¹é…
        # sent_embsåªåŒ…å«item_idä»1åˆ°n_items-1çš„å•†å“
        n_sent_embs = dataset.n_items - 1  # ä¸_encode_sent_embä¸­çš„range(1, dataset.n_items)åŒ¹é…
        self.log(f'[TOKENIZER] Items for training: {len(items_for_training)} of {n_sent_embs}')
        self.log(f'[TOKENIZER] Training items sample: {list(items_for_training)[:10]}')
        
        mask = np.zeros(n_sent_embs, dtype=bool)
        for item in items_for_training:
            item_id = dataset.item2id[item]
            if 1 <= item_id < dataset.n_items:  # ç¡®ä¿item_idåœ¨æœ‰æ•ˆèŒƒå›´å†…
                mask[item_id - 1] = True  # è½¬æ¢ä¸º0-basedç´¢å¼•
        
        self.log(f'[TOKENIZER] Mask shape: {mask.shape}, True count: {np.sum(mask)}')
        return mask

    def _generate_semantic_id_opq(self, sent_embs, sem_ids_path, train_mask):
        """ä½¿ç”¨OPQ/PQç”Ÿæˆè¯­ä¹‰IDï¼ˆå…¼å®¹ disable_opqï¼‰ï¼Œå¹¶ç”¨ invlists çš„ ids å¯¹é½ã€‚"""
        import faiss
        
        # è°ƒè¯•ä¿¡æ¯
        self.log(f'[TOKENIZER] sent_embs shape: {sent_embs.shape}')
        self.log(f'[TOKENIZER] train_mask shape: {train_mask.shape}')
        self.log(f'[TOKENIZER] train_mask True count: {np.sum(train_mask)}')

        # æ„å»ºç´¢å¼•
        if self.config['opq_use_gpu']:
            res = faiss.StandardGpuResources()
            res.setTempMemory(1024 * 1024 * 512)
            co = faiss.GpuClonerOptions()
            co.useFloat16 = self.n_digit >= 56
        faiss.omp_set_num_threads(self.config['faiss_omp_num_threads'])
        index = faiss.index_factory(
            sent_embs.shape[1],
            self.index_factory,
            faiss.METRIC_INNER_PRODUCT
        )
        self.log(f'[TOKENIZER] Training index...')
        if self.config['opq_use_gpu']:
            index = faiss.index_cpu_to_gpu(res, self.config['opq_gpu_id'], index, co)
        index.train(sent_embs[train_mask])
        index.add(sent_embs)
        if self.config['opq_use_gpu']:
            index = faiss.index_gpu_to_cpu(index)

        # å…¼å®¹ IndexPreTransform ä¸é PreTransform
        if isinstance(index, faiss.IndexPreTransform):
            ivf_index = faiss.downcast_index(index.index)
        else:
            ivf_index = faiss.downcast_index(index)

        invlists = faiss.extract_index_ivf(ivf_index).invlists
        ls = invlists.list_size(0)
        # å– codes ä¸ idsï¼Œå¹¶ä¿æŒåŒåºå¯¹é½
        codes_ptr = invlists.get_codes(0)
        ids_ptr = invlists.get_ids(0)
        pq_codes_u8 = faiss.rev_swig_ptr(codes_ptr, ls * invlists.code_size)
        ids = faiss.rev_swig_ptr(ids_ptr, ls).copy()
        pq_codes_u8 = pq_codes_u8.reshape(-1, invlists.code_size)

        # è§£æ PQ Code
        faiss_sem_ids = []
        n_bytes = invlists.code_size
        for u8code in pq_codes_u8:
            bs = faiss.BitstringReader(faiss.swig_ptr(u8code), n_bytes)
            code = []
            for _ in range(self.n_digit):
                code.append(bs.read(self.n_codebook_bits))
            faiss_sem_ids.append(code)

        # ç”¨ ids å¯¹é½ item é¡ºåº
        item2sem_ids = {}
        for pos, iid0 in enumerate(ids):
            item = self.id2item[int(iid0) + 1]
            item2sem_ids[item] = tuple(int(v) for v in faiss_sem_ids[pos])

        self.log(f'[TOKENIZER] Saving semantic IDs to {sem_ids_path}...')
        os.makedirs(os.path.dirname(sem_ids_path), exist_ok=True)
        with open(sem_ids_path, 'w') as f:
            json.dump(item2sem_ids, f)

    def _generate_semantic_id_rq_kmeans(self, sent_embs, sem_ids_path, train_mask):
        """ä½¿ç”¨ Residual Quantizationï¼ˆKMeansï¼‰ç”Ÿæˆè¯­ä¹‰IDï¼ˆä¸ DIFF_GRM å¯¹é½ï¼‰"""
        import faiss
        d = sent_embs.shape[1]
        K = self.codebook_size
        niter = int(self.config.get('rq_kmeans_niters', 20))
        seed = int(self.config.get('rq_kmeans_seed', 1234))
        
        # åˆå§‹åŒ–æ®‹å·®ä¸ºåŸå§‹å‘é‡
        residuals = sent_embs.copy().astype(np.float32, copy=False)
        codes_all = np.zeros((sent_embs.shape[0], self.n_digit), dtype=np.int64)
        
        for stage in range(self.n_digit):
            kmeans = faiss.Kmeans(d=d, k=K, niter=niter, verbose=False, seed=seed + stage)
            kmeans.train(residuals[train_mask])
            # In current Faiss Python, Kmeans.centroids is already a numpy array
            centroids = np.asarray(kmeans.centroids, dtype=np.float32)
            if centroids.ndim == 1:
                centroids = centroids.reshape(K, d)
            elif centroids.shape == (d, K):
                centroids = centroids.T
            assert centroids.shape == (K, d), f"centroids shape {centroids.shape} != {(K, d)}"
            
            # ä¸ºå…¨éƒ¨æ ·æœ¬åˆ†é…æœ€è¿‘è´¨å¿ƒ
            index = faiss.IndexFlatL2(d)
            index.add(centroids)
            D, I = index.search(residuals, 1)  # I: [N, 1]
            codes_all[:, stage] = I[:, 0].astype(np.int64)
            
            # æ›´æ–°æ®‹å·®
            residuals = residuals - centroids[I[:, 0]]
        
        # è½¬æˆ dict
        item2sem_ids = {}
        for i in range(codes_all.shape[0]):
            item = self.id2item[i + 1]
            item2sem_ids[item] = tuple(int(v) for v in codes_all[i].tolist())
        os.makedirs(os.path.dirname(sem_ids_path), exist_ok=True)
        with open(sem_ids_path, 'w') as f:
            json.dump(item2sem_ids, f)

    def _generate_semantic_id_random(self, sem_ids_path, n_items, seed=12345):
        """ä¸ºæ¯ä¸ªå•†å“éšæœºç”Ÿæˆ n_digit ä¸ª codebook IDï¼ˆå‡åŒ€[0, K-1]ï¼‰"""
        rng = np.random.default_rng(seed)
        item2sem_ids = {}
        for i in range(1, n_items):
            item = self.id2item[i]
            codes = rng.integers(low=0, high=self.codebook_size, size=self.n_digit, endpoint=False, dtype=np.int64)
            item2sem_ids[item] = tuple(int(c) for c in codes.tolist())
        os.makedirs(os.path.dirname(sem_ids_path), exist_ok=True)
        with open(sem_ids_path, 'w') as f:
            json.dump(item2sem_ids, f)

    def _sem_ids_to_tokens(self, item2sem_ids: dict) -> dict:
        """å°†è¯­ä¹‰IDè½¬æ¢ä¸ºtoken"""
        for item in item2sem_ids:
            tokens = list(item2sem_ids[item])
            # ä¿®å¤ï¼šé‡æ–°å¼•å…¥offsetï¼Œé¿å…ä¸PAD/BOSå†²çª
            # æ¯ä¸ªdigitçš„codebook IDåŠ ä¸Šå¯¹åº”çš„offset
            tokens = [t + self.sid_offset + d * self.codebook_size 
                     for d, t in enumerate(tokens)]
            item2sem_ids[item] = tuple(tokens)
        return item2sem_ids

    def _init_tokenizer(self, dataset: AbstractDataset):
        """åˆå§‹åŒ–tokenizer"""
        # æ„å»ºè·¯å¾„ï¼ˆä¸ DIFF ä¸€è‡´ï¼‰
        dataset_name = dataset.__class__.__name__
        if hasattr(dataset, 'category') and dataset.category:
            cache_dir = os.path.join(dataset.cache_dir, 'processed')
        else:
            cache_dir = os.path.join('data', dataset_name, 'processed')
        os.makedirs(cache_dir, exist_ok=True)

        # é‡åŒ–å™¨æ ‡ç­¾ï¼ˆå« seed/itersï¼‰
        model_basename = os.path.basename(self.config["sent_emb_model"])
        quant_tag = self.index_factory
        if self.sid_quantizer == 'rq_kmeans':
            quant_tag += f'_seed{self.config.get("rq_kmeans_seed",1234)}_it{self.config.get("rq_kmeans_niters",20)}'
        elif self.sid_quantizer == 'none':
            quant_tag += f'_seed{self.config.get("sid_random_seed",12345)}'

        sem_ids_path = os.path.join(
            cache_dir,
            f'{model_basename}_pca{self.config["sent_emb_pca"]}_{quant_tag}.sem_ids'
        )

        # ä¸¤ä»½åµŒå…¥æ–‡ä»¶ï¼šraw / pca
        raw_path = os.path.join(cache_dir, f'{model_basename}_raw_d{self.config["sent_emb_dim"]}.sent_emb')
        pca_path = os.path.join(cache_dir, f'{model_basename}_pca{self.config["sent_emb_pca"]}.sent_emb')

        force_regenerate = self.config.get('force_regenerate_opq', False) or self.config.get('force_regenerate_codes', False)

        # æŒ‰é‡åŒ–å™¨é€‰æ‹©å‘é‡ç®¡çº¿
        sent_embs = None
        if self.sid_quantizer == 'opq_pq':
            if self.config['sent_emb_pca'] > 0 and os.path.exists(pca_path):
                self.log(f'[TOKENIZER] Loading PCA-ed sentence embeddings from {pca_path}...')
                sent_embs = np.fromfile(pca_path, dtype=np.float32).reshape(-1, self.config['sent_emb_pca'])
            elif os.path.exists(raw_path):
                self.log(f'[TOKENIZER] Loading RAW sentence embeddings from {raw_path}...')
                raw_embs = np.fromfile(raw_path, dtype=np.float32).reshape(-1, self.config['sent_emb_dim'])
                if self.config['sent_emb_pca'] > 0:
                    self.log(f'[TOKENIZER] Applying PCA to sentence embeddings...')
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=self.config['sent_emb_pca'], whiten=True)
                    training_item_mask = self._get_items_for_training(dataset)
                    pca.fit(raw_embs[training_item_mask])
                    sent_embs = pca.transform(raw_embs).astype(np.float32, copy=False)
                    if self.config.get('normalize_after_pca', True):
                        norms = np.linalg.norm(sent_embs, axis=1, keepdims=True) + 1e-12
                        sent_embs = sent_embs / norms
                    sent_embs.tofile(pca_path)
                else:
                    sent_embs = raw_embs
            else:
                self.log(f'[TOKENIZER] Encoding sentence embeddings...')
                raw_embs = self._encode_sent_emb(dataset, raw_path)
                if self.config['sent_emb_pca'] > 0:
                    self.log(f'[TOKENIZER] Applying PCA to sentence embeddings...')
                    from sklearn.decomposition import PCA
                    pca = PCA(n_components=self.config['sent_emb_pca'], whiten=True)
                    training_item_mask = self._get_items_for_training(dataset)
                    pca.fit(raw_embs[training_item_mask])
                    sent_embs = pca.transform(raw_embs).astype(np.float32, copy=False)
                    if self.config.get('normalize_after_pca', True):
                        norms = np.linalg.norm(sent_embs, axis=1, keepdims=True) + 1e-12
                        sent_embs = sent_embs / norms
                    sent_embs.tofile(pca_path)
                else:
                    sent_embs = raw_embs
            self.log(f'[TOKENIZER] Sentence embeddings shape: {sent_embs.shape}')

        elif self.sid_quantizer == 'rq_kmeans':
            # RQ-KMeansï¼šåªç”¨ RAWï¼Œä¸åš PCA
            if os.path.exists(raw_path):
                self.log(f'[TOKENIZER] Loading RAW sentence embeddings from {raw_path}...')
                sent_embs = np.fromfile(raw_path, dtype=np.float32).reshape(-1, self.config['sent_emb_dim'])
            else:
                self.log(f'[TOKENIZER] Encoding sentence embeddings (RAW, no PCA for RQ-KMeans)...')
                sent_embs = self._encode_sent_emb(dataset, raw_path)
            self.log(f'[TOKENIZER] Sentence embeddings shape (RAW): {sent_embs.shape}')

        # ç”Ÿæˆæˆ–åŠ è½½é‡åŒ–ç»“æœ
        if force_regenerate or not os.path.exists(sem_ids_path):
            if force_regenerate:
                self.log(f'[TOKENIZER] Force regenerating quantization results ({self.sid_quantizer})...')
            else:
                self.log(f'[TOKENIZER] Quantization results not found, generating ({self.sid_quantizer})...')
            training_item_mask = self._get_items_for_training(dataset)
            if self.sid_quantizer == 'opq_pq':
                self._generate_semantic_id_opq(sent_embs, sem_ids_path, training_item_mask)
            elif self.sid_quantizer == 'rq_kmeans':
                self._generate_semantic_id_rq_kmeans(sent_embs, sem_ids_path, training_item_mask)
            else:  # 'none'
                self._generate_semantic_id_random(
                    sem_ids_path, n_items=self.dataset.n_items,
                    seed=int(self.config.get('sid_random_seed', 12345))
                )
        else:
            self.log(f'[TOKENIZER] Using existing quantization results from {sem_ids_path}')

        # åŠ è½½ sem_ids â†’ tokens
        self.log(f'[TOKENIZER] Loading semantic IDs from {sem_ids_path}...')
        item2sem_ids = json.load(open(sem_ids_path, 'r'))
        item2tokens = self._sem_ids_to_tokens(item2sem_ids)

        # åŒæ­¥æ˜ å°„æ–‡ä»¶åï¼ˆå« seed/itersï¼‰
        map_tag = f'{model_basename}_pca{self.config["sent_emb_pca"]}_{quant_tag}_{self.n_digit}d'
        fwd_path = os.path.join(cache_dir, f'item_id2tokens_{map_tag}.npy')
        inv_path = os.path.join(cache_dir, f'tokens2item_{map_tag}.pkl')

        if force_regenerate:
            fwd_exists = inv_exists = False
            self.log(f'[TOKENIZER] Force regenerate enabled, ignoring existing mapping files')
        else:
            fwd_exists = os.path.exists(fwd_path)
            inv_exists = os.path.exists(inv_path)

        if fwd_exists and inv_exists:
            self.log(f'[TOKENIZER] Loading existing mappings for tag: {map_tag} from {fwd_path}')
            item_id2tokens = np.load(fwd_path)
            item2tokens = {}
            for iid, toks in enumerate(item_id2tokens):
                if iid == 0:
                    continue
                item2tokens[self.id2item[iid]] = tuple(toks.tolist())
            with open(inv_path, 'rb') as f:
                self.tokens2item = pickle.load(f)
            self.log(f'[TOKENIZER] Successfully loaded {len(item2tokens)} item mappings')
        else:
            self.item2tokens = item2tokens
            self.tokens2item = self._create_reverse_mapping()
            self._save_mappings()

        if not hasattr(self, 'item2tokens'):
            self.item2tokens = item2tokens
        return item2tokens

    def _create_reverse_mapping(self):
        """åˆ›å»ºåå‘æ˜ å°„ç”¨äºæ¨ç†"""
        tokens2item = {}
        for item, tokens in self.item2tokens.items():
            item_id = self.dataset.item2id[item]
            tokens2item[tuple(tokens)] = item_id
        return tokens2item

    def _save_mappings(self):
        """ä¿å­˜æ˜ å°„æ–‡ä»¶"""
        # æ„å»ºè·¯å¾„ï¼ˆä¸ DIFF ä¸€è‡´ï¼‰
        dataset_name = self.dataset.__class__.__name__
        if hasattr(self.dataset, 'category') and self.dataset.category:
            cache_dir = os.path.join(self.dataset.cache_dir, 'processed')
        else:
            cache_dir = os.path.join('data', dataset_name, 'processed')
        os.makedirs(cache_dir, exist_ok=True)
        
        # æ–‡ä»¶ååŒ…å«ï¼šæ¨¡å‹+PCA+é‡åŒ–å™¨æ ‡ç­¾(+ç§å­/iters)+n_digitï¼Œå®Œå…¨é¿å…ä¸åŒé…ç½®å†²çª
        model_basename = os.path.basename(self.config["sent_emb_model"]) 
        quant_tag = self.index_factory
        if self.sid_quantizer == 'rq_kmeans':
            quant_tag += f'_seed{self.config.get("rq_kmeans_seed",1234)}_it{self.config.get("rq_kmeans_niters",20)}'
        elif self.sid_quantizer == 'none':
            quant_tag += f'_seed{self.config.get("sid_random_seed",12345)}'
        map_tag = f'{model_basename}_pca{self.config["sent_emb_pca"]}_{quant_tag}_{self.n_digit}d'
        
        # ä¿å­˜æ­£æ’ç´¢å¼•ï¼šitem_id â†’ SID-tokens
        item_id2tokens = np.zeros((self.dataset.n_items, self.n_digit), dtype=np.int64)
        for item, tokens in self.item2tokens.items():
            item_id = self.dataset.item2id[item]
            item_id2tokens[item_id] = np.array(tokens)
        
        np.save(os.path.join(cache_dir, f'item_id2tokens_{map_tag}.npy'), item_id2tokens)
        
        # ä¿å­˜å€’æ’ç´¢å¼•ï¼šSID-tokens â†’ item_id
        with open(os.path.join(cache_dir, f'tokens2item_{map_tag}.pkl'), 'wb') as f:
            pickle.dump(self.tokens2item, f)
        
        self.log(f'[TOKENIZER] Saved mappings with tag: {map_tag} to {cache_dir}')
        self.log(f'[TOKENIZER] Files: item_id2tokens_{map_tag}.npy, tokens2item_{map_tag}.pkl')

    def encode_history(self, item_seq, max_len=None):
        """ç¼–ç ç”¨æˆ·å†å²åºåˆ—"""
        if max_len is None:
            max_len = self.config.get('max_history_len', 50)
        if len(item_seq) > max_len:
            item_seq = item_seq[-max_len:]
        
        history_sid = []
        for item in item_seq:
            if item in self.item2tokens:
                history_sid.append(list(self.item2tokens[item]))
            else:
                # æœªçŸ¥å•†å“ç”¨PADå¡«å……
                history_sid.append([self.pad_token] * self.n_digit)
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(history_sid) < max_len:
            history_sid.append([self.pad_token] * self.n_digit)
        
        return history_sid  # è¿”å›listï¼Œè®©datasets.mapè‡ªåŠ¨å¼ é‡åŒ–

    def encode_decoder_input(self, target_item):
        """ç¼–ç decoderè¾“å…¥ - ä¸RPG_EDä¿æŒä¸€è‡´"""
        if target_item in self.item2tokens:
            tokens = list(self.item2tokens[target_item])  # 4ä¸ªtoken IDï¼ˆå¸¦offsetï¼‰
            
            # å°†token IDè½¬æ¢ä¸ºcodebook ID
            codebook_tokens = []
            for digit, token_id in enumerate(tokens):
                codebook_id = token_id - (self.sid_offset + digit * self.codebook_size)
                codebook_tokens.append(codebook_id)
            
            # decoderè¾“å…¥å’Œæ ‡ç­¾éƒ½æ˜¯codebook IDs
            decoder_input = codebook_tokens  # [cb0, cb1, cb2, cb3]
            decoder_labels = codebook_tokens  # [cb0, cb1, cb2, cb3]
        else:
            # ğŸš€ ä¿®å¤ï¼šæœªçŸ¥å•†å“ä½¿ç”¨-100ä½œä¸ºignore_indexï¼Œé¿å…ä¸åˆæ³•codebook 0å†²çª
            decoder_input = [self.pad_token] * self.n_digit  # é•¿åº¦n_digit
            decoder_labels = [-100] * self.n_digit  # ä½¿ç”¨-100ä½œä¸ºignore_index
        
        return decoder_input, decoder_labels

    def decode_tokens_to_item(self, tokens):
        """å°†tokenåºåˆ—è§£ç ä¸ºå•†å“ID"""
        if len(tokens) != self.n_digit:
            return None
        
        token_tuple = tuple(tokens)
        return self.tokens2item.get(token_tuple)

    def codebooks_to_item_id(self, cb_ids):
        """
        å°†codebook IDåºåˆ—è½¬æ¢ä¸ºitem_idï¼Œæ£€æŸ¥åˆæ³•æ€§
        
        Args:
            cb_ids: List[int] é•¿åº¦ n_digit, åŸå§‹ codebook ID (0-255)
            
        Returns:
            item_id(int) æˆ– Noneï¼ˆå¦‚æœéæ³•ï¼‰
        """
        if len(cb_ids) != self.n_digit:
            return None
        
        # å°†codebook IDè½¬æ¢ä¸ºtoken ID
        token_ids = [
            cb_ids[d] + self.sid_offset + d * self.codebook_size
            for d in range(self.n_digit)
        ]
        
        # æŸ¥æ‰¾å¯¹åº”çš„item_id
        return self.tokens2item.get(tuple(token_ids))

    def tokenize_function(self, example: dict, split: str) -> dict:
        """tokenizeå‡½æ•° - ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜"""
        item_seq = example['item_seq']  # Python list
        target_item = item_seq[-1]  # åŸå§‹å­—ç¬¦ä¸²
        
        # ä¿®å¤ï¼šæ‰€æœ‰splitéƒ½åº”è¯¥ç”¨item_seq[:-1]ä½œä¸ºå†å²ï¼Œé¿å…æ•°æ®æ³„éœ²
        history_sid = self.encode_history(item_seq[:-1])
        
        if split == 'train':
            # è®­ç»ƒæ—¶ç¼–ç decoderè¾“å…¥
            decoder_input, decoder_labels = self.encode_decoder_input(target_item)
            return {
                'history_sid': history_sid,  # ç›´æ¥list
                'decoder_input_ids': decoder_input,  # ç›´æ¥list
                'decoder_labels': decoder_labels  # ç›´æ¥list
            }
        else:
            # éªŒè¯/æµ‹è¯•æ—¶ç”ŸæˆçœŸæ ‡ç­¾
            _, decoder_labels = self.encode_decoder_input(target_item)
            return {
                'history_sid': history_sid,  # ç›´æ¥list
                'labels': decoder_labels  # æ–°å¢ï¼šçœŸæ ‡ç­¾åºåˆ—
            }

    def tokenize(self, datasets: dict) -> dict:
        """tokenizeæ•°æ®é›†"""
        tokenized_datasets = {}
        for split in datasets:
            tokenized_datasets[split] = datasets[split].map(
                lambda t: self.tokenize_function(t, split),
                batched=False,  # å…³é—­æ‰¹å¤„ç†ï¼Œé¿å…æ•°æ®ç»“æ„æ··ä¹±
                remove_columns=datasets[split].column_names,
                num_proc=self.config['num_proc'],
                desc=f'Tokenizing {split} set: '
            )

        for split in datasets:
            tokenized_datasets[split].set_format(type='torch')

        return tokenized_datasets 